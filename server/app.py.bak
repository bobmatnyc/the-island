#!/usr/bin/env python3
"""
Epstein Archive Document Explorer - FastAPI Server
Serves data, search APIs, visualizations, and ingestion progress
"""

import json
import os
import re
import secrets
import subprocess
import urllib.parse
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional

import uvicorn
from dotenv import load_dotenv
from fastapi import Depends, FastAPI, Header, HTTPException, Query, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from fastapi.staticfiles import StaticFiles
from openai import OpenAI
from pydantic import BaseModel
from fastapi import Request

# Import audit logger
from services.audit_logger import AuditLogger, LoginEvent, BrowserProfile

# Load environment variables from .env.local
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env.local")

# Paths
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
METADATA_DIR = DATA_DIR / "metadata"
MD_DIR = DATA_DIR / "md"
LOGS_DIR = PROJECT_ROOT / "logs"

# Initialize OpenRouter client
openrouter_client = None
openrouter_model = os.getenv("OPENROUTER_MODEL", "openai/gpt-4o")

def get_openrouter_client():
    """Initialize OpenRouter client (lazy loading)"""
    global openrouter_client
    if openrouter_client is None:
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError("OPENROUTER_API_KEY not set in .env.local")

        openrouter_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key
        )
    return openrouter_client

app = FastAPI(
    title="Epstein Document Archive API",
    description="Search and explore the Epstein document archive",
    version="1.0.0"
)

# Basic auth for password protection
security = HTTPBasic()

# Load credentials from file
import os


CREDENTIALS_FILE = Path(__file__).parent / ".credentials"

def load_credentials():
    """Load username:password pairs from .credentials file (dynamically reloaded on each call)

    Returns:
        Dictionary mapping usernames to passwords

    Raises:
        ValueError: If neither .credentials file exists nor environment variables are set

    Security: No hardcoded defaults. Fails fast if not configured.
    """
    credentials = {}

    if CREDENTIALS_FILE.exists():
        with open(CREDENTIALS_FILE) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    if ":" in line:
                        username, password = line.split(":", 1)
                        credentials[username.strip()] = password.strip()
    else:
        # Check environment variables (no hardcoded defaults)
        username = os.getenv("ARCHIVE_USERNAME")
        password = os.getenv("ARCHIVE_PASSWORD")

        if not username or not password:
            raise ValueError(
                "Authentication not configured. Either create .credentials file or set "
                "ARCHIVE_USERNAME and ARCHIVE_PASSWORD environment variables."
            )

        credentials = {username: password}

    if not credentials:
        raise ValueError("No credentials found in .credentials file")

    return credentials

def authenticate(credentials: HTTPBasicCredentials = Depends(security)):
    """Verify username and password against .credentials file (dynamically reloaded)"""
    # Reload credentials on each request for dynamic updates
    current_credentials = load_credentials()

    if credentials.username in current_credentials:
        correct_password = secrets.compare_digest(
            credentials.password,
            current_credentials[credentials.username]
        )
        if correct_password:
            return credentials.username

    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Incorrect username or password",
        headers={"WWW-Authenticate": "Basic"},
    )

# Session-based authentication
session_tokens = {}  # In-memory session storage: {token: {username, expires}}

def create_session_token(username: str, remember: bool = False) -> str:
    """Create a new session token for the user"""
    token = secrets.token_urlsafe(32)
    expires = datetime.now() + timedelta(days=30 if remember else 1)
    session_tokens[token] = {
        "username": username,
        "expires": expires
    }
    return token

def verify_session_token(authorization: Optional[str] = Header(None)) -> str:
    """Verify session token from Authorization header"""
    if not authorization:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="No authorization token provided"
        )

    # Extract token from "Bearer <token>"
    if not authorization.startswith("Bearer "):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authorization header format"
        )

    token = authorization[7:]  # Remove "Bearer " prefix

    if token not in session_tokens:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid session token"
        )

    session = session_tokens[token]
    if datetime.now() > session["expires"]:
        del session_tokens[token]
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Session expired"
        )

    return session["username"]

# Pydantic models for login
class LoginRequest(BaseModel):
    username: str
    password: str
    remember: bool = False

class LoginResponse(BaseModel):
    token: str
    username: str
    expires: str

# Flexible authentication (supports both session token and HTTP Basic Auth)
async def flexible_auth(
    authorization: Optional[str] = Header(None),
    credentials: Optional[HTTPBasicCredentials] = Depends(security)
) -> str:
    """
    Accept either session token OR HTTP Basic Auth
    Returns username if authenticated
    """
    # Try session token first (from Authorization: Bearer header)
    if authorization and authorization.startswith("Bearer "):
        try:
            return verify_session_token(authorization)
        except HTTPException:
            pass  # Fall through to Basic Auth

    # Fall back to HTTP Basic Auth
    if credentials:
        return authenticate(credentials)

    # No valid authentication found
    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Not authenticated",
        headers={"WWW-Authenticate": "Bearer, Basic"},
    )

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files
app.mount("/static", StaticFiles(directory=Path(__file__).parent / "web"), name="static")

# Data caches
entity_stats = {}
network_data = {}
semantic_index = {}
classifications = {}
timeline_data = {}

def load_data():
    """Load all JSON data into memory with error handling"""
    global entity_stats, network_data, semantic_index, classifications, timeline_data

    print("Loading data...")

    # Entity statistics
    stats_path = METADATA_DIR / "entity_statistics.json"
    if stats_path.exists():
        try:
            with open(stats_path) as f:
                data = json.load(f)
                # Fix: entity_statistics.json has structure: {statistics: {entity_name: {...}}}
                entity_stats = data.get("statistics", {})
                print(f"  âœ“ Loaded {len(entity_stats)} entities from entity_statistics.json")
        except Exception as e:
            print(f"  âœ— Failed to load entity_statistics.json: {e}")
            entity_stats = {}
    else:
        print(f"  âœ— Entity statistics file not found: {stats_path}")
        entity_stats = {}

    # Network
    network_path = METADATA_DIR / "entity_network.json"
    if network_path.exists():
        try:
            with open(network_path) as f:
                network_data = json.load(f)
                print(f"  âœ“ Loaded {len(network_data.get('nodes', []))} network nodes")
        except Exception as e:
            print(f"  âœ— Failed to load entity_network.json: {e}")
            network_data = {}
    else:
        print(f"  âœ— Network file not found: {network_path}")
        network_data = {}

    # Semantic index
    semantic_path = METADATA_DIR / "semantic_index.json"
    if semantic_path.exists():
        try:
            with open(semantic_path) as f:
                data = json.load(f)
                semantic_index = data.get("entity_to_documents", {})
                print(f"  âœ“ Loaded semantic index for {len(semantic_index)} entities")
        except Exception as e:
            print(f"  âœ— Failed to load semantic_index.json: {e}")
            semantic_index = {}
    else:
        print(f"  âœ— Semantic index not found: {semantic_path}")
        semantic_index = {}

    # Classifications
    class_path = METADATA_DIR / "document_classifications.json"
    if class_path.exists():
        try:
            with open(class_path) as f:
                data = json.load(f)
                classifications = data.get("results", {})
                print(f"  âœ“ Loaded {len(classifications)} document classifications")
        except Exception as e:
            print(f"  âœ— Failed to load document_classifications.json: {e}")
            classifications = {}
    else:
        print(f"  âœ— Classifications file not found: {class_path}")
        classifications = {}

    # Timeline
    timeline_path = METADATA_DIR / "timeline.json"
    if timeline_path.exists():
        try:
            with open(timeline_path) as f:
                timeline_data = json.load(f)
                print("  âœ“ Loaded timeline data")
        except Exception as e:
            print(f"  âœ— Failed to load timeline.json: {e}")
            timeline_data = {}
    else:
        print(f"  âœ— Timeline file not found: {timeline_path}")
        timeline_data = {}

    print("\nðŸ“Š Data Loading Summary:")
    print(f"  Entities: {len(entity_stats)}")
    print(f"  Network nodes: {len(network_data.get('nodes', []))}")
    print(f"  Network edges: {len(network_data.get('edges', []))}")
    print(f"  Classifications: {len(classifications)}")

def get_ocr_status():
    """Get current OCR processing status

    Design Decision: Safe Number Parsing
    Rationale: OCR status script outputs formatted numbers with commas (e.g., "33,572")
    which Python's int() cannot parse directly. We need to strip commas before conversion.

    Error Handling: Return safe fallback values rather than failing the entire API call.
    """
    try:
        result = subprocess.run(
            ["python3", str(PROJECT_ROOT / "scripts/extraction/check_ocr_status.py")],
            check=False, capture_output=True,
            text=True,
            timeout=5
        )

        output = result.stdout

        # Parse output
        status = {
            "active": "Progress:" in output,
            "progress": 0,
            "processed": 0,
            "total": 0,
            "emails_found": 0,
            "failed": 0
        }

        def safe_int(value: str) -> int:
            """Parse integer safely, removing commas and whitespace"""
            try:
                return int(value.replace(",", "").strip())
            except (ValueError, AttributeError):
                return 0

        def safe_float(value: str) -> float:
            """Parse float safely, removing percent signs and parentheses"""
            try:
                cleaned = value.replace("%", "").replace("(", "").replace(")", "").strip()
                return float(cleaned)
            except (ValueError, AttributeError):
                return 0.0

        for line in output.split("\n"):
            if "Progress:" in line:
                # Example: "Progress: 15,100 / 33,572 (45.0%)"
                parts = line.split()
                if len(parts) >= 6:
                    status["processed"] = safe_int(parts[1])
                    status["total"] = safe_int(parts[3])
                    status["progress"] = safe_float(parts[4])
            elif "Email candidates found:" in line:
                status["emails_found"] = safe_int(line.split(":")[1])
            elif "Failed:" in line:
                status["failed"] = safe_int(line.split(":")[1].split()[0])

        return status
    except subprocess.TimeoutExpired:
        return {"active": False, "error": "OCR status check timeout"}
    except Exception as e:
        return {"active": False, "error": str(e)}

@app.on_event("startup")
async def startup_event():
    """Load data on startup"""
    load_data()

@app.get("/")
async def root():
    """Redirect to login page"""
    return HTMLResponse("""
    <!DOCTYPE html>
    <html>
    <head>
        <meta http-equiv="refresh" content="0; url=/static/login.html" />
    </head>
    <body>
        <p>Redirecting to <a href="/static/login.html">Login</a>...</p>
    </body>
    </html>
    """)

@app.post("/login", response_model=LoginResponse)
async def login(login_data: LoginRequest):
    """Login endpoint - creates session token"""
    current_credentials = load_credentials()

    if login_data.username not in current_credentials:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )

    if not secrets.compare_digest(
        login_data.password,
        current_credentials[login_data.username]
    ):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )

    # Create session token
    token = create_session_token(login_data.username, login_data.remember)
    expires = session_tokens[token]["expires"].isoformat()

    return LoginResponse(
        token=token,
        username=login_data.username,
        expires=expires
    )

@app.get("/api/verify-session")
async def verify_session(username: str = Depends(verify_session_token)):
    """Verify session token is valid"""
    return {"username": username, "valid": True}

@app.get("/ROADMAP.md")
async def get_roadmap():
    """Serve the ROADMAP.md file"""
    roadmap_path = PROJECT_ROOT / "ROADMAP.md"
    if roadmap_path.exists():
        return FileResponse(roadmap_path, media_type="text/markdown")
    raise HTTPException(status_code=404, detail="ROADMAP.md not found")

@app.get("/api/stats")
async def get_stats(username: str = Depends(authenticate)):
    """Get overall statistics

    Design Decision: Frontend Compatibility
    Rationale: Frontend expects 'total_connections' field for network edges display.
    Also includes 'sources' field for source list rendering.

    Error Handling: Returns safe fallback values if data not loaded.

    Document Count Fix: Reads unique_documents from master_document_index.json
    instead of counting classifications, which only reflects processed documents.
    """
    # Load source index for sources list
    source_list = []
    source_index_path = METADATA_DIR / "source_index.json"
    if source_index_path.exists():
        try:
            with open(source_index_path) as f:
                sources_data = json.load(f)
                # source_index.json has structure: {sources: {key: {description: ...}}}
                sources = sources_data.get("sources", {})
                for source_key, source_info in sources.items():
                    description = source_info.get("description", source_key)
                    if description:
                        source_list.append(description)
                    else:
                        # Fallback to formatted key name
                        source_list.append(source_key.replace("_", " ").title())
        except Exception as e:
            print(f"Error loading source index: {e}")

    # Get correct document count from unified index (includes emails + PDFs)
    total_documents = len(classifications)  # Fallback to classifications count
    document_breakdown = {}
    classification_breakdown = {}

    # Try unified index first (most comprehensive)
    unified_index_path = METADATA_DIR / "all_documents_index.json"
    if unified_index_path.exists():
        try:
            with open(unified_index_path) as f:
                unified_data = json.load(f)
                total_documents = unified_data.get("total_documents", len(classifications))
                document_breakdown = unified_data.get("statistics", {}).get("by_type", {})
                classification_breakdown = unified_data.get("statistics", {}).get("by_classification", {})
        except Exception as e:
            print(f"Error loading unified document index: {e}")

    # Fallback to master index if unified not available
    if not document_breakdown:
        master_index_path = METADATA_DIR / "master_document_index.json"
        if master_index_path.exists():
            try:
                with open(master_index_path) as f:
                    index_data = json.load(f)
                    total_documents = index_data.get("unique_documents", len(classifications))
            except Exception as e:
                print(f"Error loading master document index: {e}")

    return {
        "total_entities": len(entity_stats),
        "total_documents": total_documents,
        "document_types": document_breakdown,  # New: breakdown by type (email, pdf)
        "classifications": classification_breakdown,  # New: breakdown by classification
        "network_nodes": len(network_data.get("nodes", [])),
        "network_edges": len(network_data.get("edges", [])),
        "total_connections": len(network_data.get("edges", [])),  # Frontend expects this field
        "timeline_events": timeline_data.get("total_events", 0),
        "date_range": timeline_data.get("date_range", {}),
        "sources": source_list  # Frontend expects this field
    }

@app.get("/api/sources/index")
async def get_sources_index(username: str = Depends(authenticate)):
    """Get master document index with deduplication statistics.

    Design Decision: Complete Source Provenance
    Rationale: Provides comprehensive view of all document sources with
    deduplication metrics for transparency and quality assessment.

    Performance: Large file (~17MB) loaded on-demand, not cached in memory
    to avoid memory bloat. Frontend caches response.

    Returns:
        {
            "total_files": int,
            "unique_documents": int,
            "sources": [{name, total_files, unique_docs, total_size, status}, ...],
            "cross_source_duplicates": [{document, sources, size}, ...]
        }
    """
    try:
        index_path = METADATA_DIR / "master_document_index.json"

        if not index_path.exists():
            return {
                "total_files": 0,
                "unique_documents": 0,
                "sources": [],
                "cross_source_duplicates": [],
                "error": "Master document index not found"
            }

        # Load master index (on-demand, not cached due to size)
        with open(index_path) as f:
            index_data = json.load(f)

        # Format sources for frontend
        sources = []
        source_info = index_data.get("sources", {})

        # Calculate per-source statistics from documents array
        source_stats = {}
        documents = index_data.get("documents", [])

        for doc in documents:
            doc_sources = doc.get("sources", [])
            doc_size = doc.get("size", 0)

            # Track unique docs per source
            for source_path in doc_sources:
                # Extract source name from path (e.g., "data/sources/giuffre_maxwell/..." -> "giuffre_maxwell")
                parts = Path(source_path).parts
                source_name = parts[2] if len(parts) >= 3 and parts[0] == "data" and parts[1] == "sources" else "unknown"

                if source_name not in source_stats:
                    source_stats[source_name] = {
                        "unique_docs": 0,
                        "total_size": 0
                    }

                # Each document counts as one unique doc for each source it appears in
                source_stats[source_name]["unique_docs"] += 1
                source_stats[source_name]["total_size"] += doc_size

        # Build sources list
        for source_name, source_meta in source_info.items():
            total_files = source_meta.get("document_count", 0)
            stats = source_stats.get(source_name, {"unique_docs": total_files, "total_size": 0})

            sources.append({
                "name": source_name.replace("_", " ").title(),
                "total_files": total_files,
                "unique_docs": stats["unique_docs"],
                "total_size": stats["total_size"],
                "status": "downloaded" if total_files > 0 else "pending"
            })

        # Sort sources by total files (descending)
        sources.sort(key=lambda x: x["total_files"], reverse=True)

        # Find cross-source duplicates from documents with source_count > 1
        cross_source = []
        for doc in documents[:100]:  # Limit to first 100 for performance
            if doc.get("source_count", 1) > 1:
                doc_sources = doc.get("sources", [])
                # Extract source names
                source_names = set()
                for source_path in doc_sources:
                    parts = Path(source_path).parts
                    if len(parts) >= 3 and parts[0] == "data" and parts[1] == "sources":
                        source_names.add(parts[2])

                if len(source_names) > 1:
                    # Get document name from canonical path
                    canonical = doc.get("canonical_path", "")
                    doc_name = Path(canonical).name if canonical else "unknown"

                    cross_source.append({
                        "document": doc_name,
                        "sources": sorted(list(source_names)),
                        "file_count": len(doc_sources)
                    })

        return {
            "total_files": index_data.get("total_files", 0),
            "unique_documents": index_data.get("unique_documents", 0),
            "sources": sources,
            "cross_source_duplicates": cross_source
        }

    except Exception as e:
        print(f"Error loading sources index: {e}")
        import traceback
        traceback.print_exc()
        return {
            "total_files": 0,
            "unique_documents": 0,
            "sources": [],
            "cross_source_duplicates": [],
            "error": str(e)
        }

@app.get("/api/ingestion/status")
async def get_ingestion_status(username: str = Depends(authenticate)):
    """Get ingestion progress status

    Design Decision: Frontend Compatibility
    Rationale: Frontend expects specific field names:
    - status, files_processed, total_files, progress_percentage
    - current_source, last_updated

    Returns flat structure matching frontend expectations.
    """
    ocr_status = get_ocr_status()

    # Get entity stats
    merged_index_path = MD_DIR / "entities/ENTITIES_INDEX_MERGED.json"
    if merged_index_path.exists():
        try:
            with open(merged_index_path) as f:
                entity_data = json.load(f)
                entities_merged = entity_data.get("duplicates_merged", 0)
                total_entities = entity_data.get("total_entities", 0)
        except Exception as e:
            print(f"Error loading merged index: {e}")
            entities_merged = 0
            total_entities = len(entity_stats)
    else:
        entities_merged = 0
        total_entities = len(entity_stats)

    # Calculate progress percentage
    processed = ocr_status.get("processed", 0)
    total = ocr_status.get("total", 0)
    progress_pct = (processed / total * 100) if total > 0 else 0

    # Determine status
    if processed >= total > 0:
        status_text = "complete"
    elif ocr_status.get("active"):
        status_text = "processing"
    else:
        status_text = "idle"

    # Get last updated timestamp
    progress_path = METADATA_DIR.parent / "sources/house_oversight_nov2025/ocr_progress.json"
    last_updated = None
    current_source = "House Oversight Committee Nov 2025"

    if progress_path.exists():
        try:
            with open(progress_path) as f:
                progress_data = json.load(f)
                last_updated = progress_data.get("last_updated")
        except Exception:
            pass

    # Get download stats from log file
    download_stats = {"total": 0, "completed": 0, "failed": 0, "status": "idle"}
    download_log_path = Path("/tmp/courtlistener_download.log")
    if download_log_path.exists():
        try:
            with open(download_log_path) as f:
                log_content = f.read()
                # Parse the last complete summary if it exists
                if "Download Complete" in log_content:
                    import re
                    success_match = re.search(r"Successfully downloaded: (\d+)", log_content)
                    total_match = re.search(r"Total files in directory: (\d+)", log_content)
                    failed_match = re.search(r"Failed downloads: (\d+)", log_content)

                    if success_match:
                        download_stats["completed"] = int(success_match.group(1))
                    if total_match:
                        download_stats["total"] = int(total_match.group(1))
                    if failed_match:
                        download_stats["failed"] = int(failed_match.group(1))
                    download_stats["status"] = "complete"
        except Exception as e:
            print(f"Error reading download log: {e}")

    # Get deduplication stats from database
    dedup_stats = {"total_files": 0, "unique_documents": 0, "rate": 0.0}
    dedup_db_path = METADATA_DIR / "deduplication.db"
    if dedup_db_path.exists():
        try:
            import sqlite3
            conn = sqlite3.connect(str(dedup_db_path))
            cursor = conn.cursor()

            # Get total source files count
            cursor.execute("SELECT COUNT(*) FROM document_sources")
            total_files_count = cursor.fetchone()[0]

            # Get unique canonical documents count
            cursor.execute("SELECT COUNT(*) FROM canonical_documents")
            unique_docs_count = cursor.fetchone()[0]

            conn.close()

            dedup_stats["total_files"] = total_files_count
            dedup_stats["unique_documents"] = unique_docs_count
            if total_files_count > 0:
                dedup_stats["rate"] = ((total_files_count - unique_docs_count) / total_files_count) * 100
        except Exception as e:
            print(f"Error reading deduplication database: {e}")

    # Return frontend-compatible format with enhanced pipeline info
    return {
        "status": status_text,
        "files_processed": processed,
        "total_files": total,
        "progress_percentage": progress_pct,
        "current_source": current_source if status_text == "processing" else None,
        "last_updated": last_updated,
        "ocr": ocr_status,  # Keep detailed OCR status for compatibility
        "entities": {
            "total": total_entities,
            "duplicates_merged": entities_merged,
            "in_network": len(network_data.get("nodes", [])),
            "billionaires": sum(1 for e in entity_stats.values() if e.get("is_billionaire"))
        },
        "documents": {
            "total": len(classifications),
            "classified": len(classifications),
            "emails_found": ocr_status.get("emails_found", 0)
        },
        "network": {
            "nodes": len(network_data.get("nodes", [])),
            "edges": len(network_data.get("edges", []))
        },
        "downloads": download_stats,
        "deduplication": dedup_stats
    }


@app.get("/api/chatbot/knowledge")
async def get_chatbot_knowledge(username: str = Depends(authenticate)):
    """Get comprehensive knowledge index for chatbot.

    Design Decision: Chatbot Knowledge Index
    Rationale: Chatbot needs quick access to project state, file locations,
    statistics, and ongoing work without reading multiple files.

    Returns:
    - All project files (scripts, data, sources)
    - Data summaries (entities, documents, network, emails)
    - Ongoing work (downloads, classifications, processing)
    - Quick stats for chatbot responses

    Trade-offs:
    - Performance: Single file read vs. multiple file reads
    - Freshness: Index must be refreshed after updates
    - Size: 25KB JSON file cached in memory

    Usage: Chatbot queries this endpoint once per session for context
    """
    knowledge_path = METADATA_DIR / "chatbot_knowledge_index.json"

    if not knowledge_path.exists():
        raise HTTPException(
            status_code=404,
            detail="Knowledge index not found. Run: python3 scripts/metadata/build_chatbot_knowledge_index.py"
        )

    try:
        with open(knowledge_path) as f:
            knowledge = json.load(f)

        return JSONResponse(content=knowledge)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to load knowledge index: {e!s}"
        )


@app.get("/api/entities")
async def get_entities(
    limit: int = Query(100, le=1000),
    offset: int = Query(0),
    sort_by: str = Query("documents", enum=["documents", "connections", "name"]),
    filter_billionaires: bool = Query(False),
    filter_connected: bool = Query(False),
    username: str = Depends(authenticate)
):
    """Get list of entities with optional filtering and sorting

    Design Decision: Filter Generic Entities
    Rationale: Exclude non-disambiguatable entities (Male, Female, Nanny (1))
    from API results. These are placeholders, not actual identifiable people.
    """
    entities_list = list(entity_stats.values())

    # Filter out generic entities (Male, Female, etc.)
    entities_list = [e for e in entities_list if not entity_filter.is_generic(e.get("name", ""))]

    if filter_billionaires:
        entities_list = [e for e in entities_list if e.get("is_billionaire", False)]
    if filter_connected:
        entities_list = [e for e in entities_list if e.get("connection_count", 0) > 0]

    if sort_by == "documents":
        entities_list.sort(key=lambda e: e.get("total_documents", 0), reverse=True)
    elif sort_by == "connections":
        entities_list.sort(key=lambda e: e.get("connection_count", 0), reverse=True)
    elif sort_by == "name":
        entities_list.sort(key=lambda e: e.get("name", ""))

    total = len(entities_list)
    entities_page = entities_list[offset:offset+limit]

    return {
        "total": total,
        "offset": offset,
        "limit": limit,
        "entities": entities_page
    }

@app.get("/api/entities/{name}")
async def get_entity(name: str, username: str = Depends(authenticate)):
    """Get detailed information about a specific entity with disambiguation support

    Handles name variations like:
    - "Je Je Epstein" -> "Jeffrey Epstein"
    - "Ghislaine Ghislaine" -> "Ghislaine Maxwell"
    """
    disambiguator = get_disambiguator()

    # Try disambiguation search first
    entity = disambiguator.search_entity(name, entity_stats)

    if not entity:
        raise HTTPException(
            status_code=404,
            detail=f"Entity not found: '{name}'. Try canonical name or check spelling."
        )

    # Add normalized name to response
    return {
        **entity,
        "search_name": name,
        "canonical_name": disambiguator.normalize_name(name)
    }

@app.get("/api/network")
async def get_network(
    min_connections: int = Query(0),
    max_nodes: int = Query(500, le=1000),
    deduplicate: bool = Query(True),
    username: str = Depends(authenticate)
):
    """Get network graph data with optional deduplication

    Query Parameters:
        min_connections: Minimum connections to include node (default: 0)
        max_nodes: Maximum nodes to return (default: 500, max: 1000)
        deduplicate: Apply name disambiguation to merge duplicates (default: True)

    Returns:
        Network graph with nodes, edges, and metadata

    Design Decision: Filter Generic Entities
    Rationale: Exclude non-disambiguatable entities (Male, Female, Nanny (1))
    from network graph. These placeholders create misleading connections.
    """
    # Get disambiguator
    disambiguator = get_disambiguator()

    # Get nodes
    nodes = network_data.get("nodes", [])

    # Filter out generic entities
    nodes = [n for n in nodes if not entity_filter.is_generic(n.get("name", ""))]

    # Apply deduplication if requested
    if deduplicate:
        original_count = len(nodes)
        nodes = disambiguator.merge_duplicate_nodes(nodes)
        deduplicated_count = original_count - len(nodes)
        print(f"Deduplicated {deduplicated_count} duplicate nodes ({original_count} -> {len(nodes)})")

    # Filter by minimum connections
    nodes = [
        n for n in nodes
        if n.get("connection_count", 0) >= min_connections
    ]

    # Sort by connections and limit
    nodes.sort(key=lambda n: n.get("connection_count", 0), reverse=True)
    nodes = nodes[:max_nodes]

    # Get node IDs for edge filtering
    node_ids = {n["id"] for n in nodes}

    # Filter edges
    edges = [
        e for e in network_data.get("edges", [])
        if e["source"] in node_ids and e["target"] in node_ids
    ]

    # Build node name mapping for edge deduplication
    if deduplicate:
        node_mapping = {n.get("id", ""): n.get("name", "") for n in nodes}
        edges = disambiguator.deduplicate_edges(edges, node_mapping)

    return {
        "nodes": nodes,
        "edges": edges,
        "metadata": {
            **network_data.get("metadata", {}),
            "deduplicated": deduplicate,
            "total_nodes": len(nodes),
            "total_edges": len(edges)
        }
    }

@app.get("/api/search")
async def search(
    q: str = Query(..., min_length=1),
    type: Optional[str] = Query(None),
    limit: int = Query(50, le=500),
    username: str = Depends(authenticate)
):
    """Search for entities or documents

    Design Decision: Filter Generic Entities
    Rationale: Exclude non-disambiguatable entities from search results.
    """
    results = []

    for entity_name, entity_data in entity_stats.items():
        # Skip generic entities in search results
        if entity_filter.is_generic(entity_name):
            continue

        if q.lower() in entity_name.lower():
            results.append({
                "type": "entity",
                "name": entity_name,
                "data": entity_data
            })

    for doc_path, doc_data in classifications.items():
        if type and doc_data.get("type") != type:
            continue

        if q.lower() in doc_path.lower():
            results.append({
                "type": "document",
                "path": doc_path,
                "data": doc_data
            })

    return {
        "query": q,
        "total": len(results),
        "results": results[:limit]
    }

@app.get("/api/timeline")
async def get_timeline(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    limit: int = Query(1000, le=5000),
    username: str = Depends(authenticate)
):
    """Get timeline events"""
    events = timeline_data.get("events", [])

    if start_date:
        events = [e for e in events if e["date"] >= start_date]
    if end_date:
        events = [e for e in events if e["date"] <= end_date]

    return {
        "total": len(events),
        "events": events[:limit],
        "date_range": timeline_data.get("date_range", {})
    }

# Chatbot models
class ChatMessage(BaseModel):
    message: str

# Project context for chatbot
PROJECT_CONTEXT = """
You are a helpful assistant for the Epstein Document Archive project.

ARCHIVE OVERVIEW:
- A comprehensive, publicly accessible archive of Epstein-related documents
- Source provenance tracking for all documents
- Automatic entity extraction and deduplication
- Network analysis of connections between entities
- OCR processing of scanned documents

CURRENT STATISTICS:
- 1,702 entities (71 duplicates merged)
- 387 network nodes
- 2,221 network connections (flight co-occurrences)
- Multiple document sources being processed

KEY FEATURES:
1. Entity extraction from documents (people, organizations)
2. Entity disambiguation (fuzzy matching to merge duplicates)
3. Network graph visualization of entity connections
4. Flight passenger list analysis
5. Real-time OCR processing status
6. Document classification and semantic indexing

IMPORTANT RESTRICTIONS:
- DO NOT share any personal information about the host machine
- DO NOT provide system paths or configuration details
- DO NOT discuss implementation specifics that could expose security vulnerabilities
- Focus responses on the archive content, entities, and publicly available documents

You can answer questions about:
- Specific entities and their connections
- Document sources and classifications
- Network patterns and relationships
- Archive statistics and progress
- How to use the archive interface

NOTE: Responses may be slower as this runs on a private machine with local LLM inference.
"""

@app.post("/api/chat")
async def chat(
    message: ChatMessage,
    username: str = Depends(authenticate)
):
    """Chat with GPT-4.5 assistant about the archive with integrated search"""
    try:
        # Perform multi-vector search to gather relevant context
        search_results = []
        query_lower = message.message.lower()

        # 1. Search entities
        matching_entities = []
        for entity_name, entity_data in entity_stats.items():
            if query_lower in entity_name.lower():
                matching_entities.append({
                    "name": entity_name,
                    "documents": entity_data.get("total_documents", 0),
                    "connections": entity_data.get("connection_count", 0),
                    "billionaire": entity_data.get("is_billionaire", False)
                })

        if matching_entities:
            search_results.append(f"\nRELEVANT ENTITIES FOUND ({len(matching_entities)}):")
            for e in matching_entities[:5]:  # Top 5
                search_results.append(f"- {e['name']}: {e['documents']} documents, {e['connections']} connections" +
                                    (" [BILLIONAIRE]" if e["billionaire"] else ""))

        # 2. Search documents
        matching_docs = []
        for doc_path, doc_data in classifications.items():
            if query_lower in doc_path.lower():
                matching_docs.append({
                    "path": doc_path,
                    "type": doc_data.get("type", "unknown")
                })

        if matching_docs:
            search_results.append(f"\nRELEVANT DOCUMENTS FOUND ({len(matching_docs)}):")
            for d in matching_docs[:5]:
                search_results.append(f"- {d['path']} (Type: {d['type']})")

        # 3. Check semantic index for entity co-occurrences
        for entity_name in matching_entities[:3]:  # Top 3 entities
            if entity_name["name"] in semantic_index:
                docs = semantic_index[entity_name["name"]]
                search_results.append(f"\n{entity_name['name']} appears in {len(docs)} documents")

        # Build context with search results
        search_context = "\n".join(search_results) if search_results else "\n[No direct matches found in database]"

        stats_context = f"""
        CURRENT ARCHIVE STATUS:
        - Total Entities: {len(entity_stats)}
        - Network Nodes: {len(network_data.get('nodes', []))}
        - Network Edges: {len(network_data.get('edges', []))}
        - Documents Classified: {len(classifications)}

        SEARCH RESULTS FOR THIS QUERY:
        {search_context}
        """

        full_context = PROJECT_CONTEXT + "\n" + stats_context

        # Call OpenRouter GPT-4.5
        try:
            client = get_openrouter_client()

            completion = client.chat.completions.create(
                model=openrouter_model,
                messages=[
                    {
                        "role": "system",
                        "content": full_context
                    },
                    {
                        "role": "user",
                        "content": message.message
                    }
                ],
                temperature=0.7,
                max_tokens=1000,
                timeout=30.0  # 30 second timeout
            )

            response = completion.choices[0].message.content.strip()

            return {
                "response": response,
                "model": openrouter_model,
                "search_results": {
                    "entities": matching_entities[:5],
                    "documents": matching_docs[:5]
                }
            }

        except Exception as api_error:
            # Log the error for debugging
            print(f"OpenRouter API error: {api_error}")
            return {
                "response": f"Sorry, I'm having trouble connecting to the AI service. Error: {api_error!s}",
                "error": str(api_error)
            }

    except Exception as e:
        return {"response": f"Error: {e!s}"}

# Initialize suggestion service
# Add utils path for entity filtering
import sys

from models.suggested_source import (
    SourcePriority,
    SourceStatus,
    SuggestedSourceCreate,
    SuggestedSourceUpdate,
)

# Initialize entity disambiguation service
from services.entity_disambiguation import get_disambiguator

# Initialize entity enrichment service
from services.entity_enrichment import EntityEnrichmentService, format_for_ui
from services.suggestion_service import SuggestionService


sys.path.insert(0, str(PROJECT_ROOT / "scripts/utils"))
from entity_filtering import EntityFilter


SUGGESTIONS_STORAGE = DATA_DIR / "suggestions" / "suggested_sources.json"
ENRICHMENT_STORAGE = METADATA_DIR / "entity_enrichments.json"

suggestion_service = SuggestionService(SUGGESTIONS_STORAGE)
enrichment_service = EntityEnrichmentService(ENRICHMENT_STORAGE)
entity_filter = EntityFilter()

# Source Suggestion Endpoints

@app.post("/api/suggestions", status_code=201)
async def create_suggestion(
    suggestion: SuggestedSourceCreate,
    username: str = Depends(authenticate)
):
    """Submit a new source suggestion

    Security Validation:
    - Only HTTP/HTTPS URLs allowed
    - Blocks localhost, private IPs, and suspicious patterns
    - URL scheme validation via Pydantic

    Returns:
        Created suggestion with generated ID
    """
    # Additional security validation
    parsed_url = urllib.parse.urlparse(suggestion.url)

    # Block suspicious domains
    suspicious_patterns = [
        r"localhost",
        r"127\.0\.0\.1",
        r"192\.168\.",
        r"10\.",
        r"172\.(1[6-9]|2[0-9]|3[0-1])\.",
        r"\.local$",
        r"file://",
    ]

    for pattern in suspicious_patterns:
        if re.search(pattern, suggestion.url, re.IGNORECASE):
            raise HTTPException(status_code=400, detail="URL contains suspicious patterns")

    # Create suggestion via service
    created = suggestion_service.create_suggestion(suggestion, submitted_by=username)

    return {
        "status": "success",
        "message": "Thank you for your suggestion! It will be reviewed before processing.",
        "suggestion": created
    }

@app.get("/api/suggestions")
async def list_suggestions(
    status: Optional[SourceStatus] = Query(None),
    priority: Optional[SourcePriority] = Query(None),
    limit: int = Query(100, le=500),
    offset: int = Query(0, ge=0),
    username: str = Depends(authenticate)
):
    """Get list of source suggestions with filtering

    Query Parameters:
        status: Filter by status (pending, approved, rejected, processing, completed, failed)
        priority: Filter by priority (low, medium, high, critical)
        limit: Maximum results (default 100, max 500)
        offset: Pagination offset (default 0)

    Returns:
        Paginated list of suggestions with total count
    """
    suggestions, total = suggestion_service.get_all_suggestions(
        status=status,
        priority=priority,
        limit=limit,
        offset=offset
    )

    return {
        "total": total,
        "offset": offset,
        "limit": limit,
        "suggestions": suggestions
    }

@app.get("/api/suggestions/{suggestion_id}")
async def get_suggestion(
    suggestion_id: str,
    username: str = Depends(authenticate)
):
    """Get single suggestion by ID

    Args:
        suggestion_id: UUID of suggestion

    Returns:
        Suggestion details

    Raises:
        404: Suggestion not found
    """
    suggestion = suggestion_service.get_suggestion_by_id(suggestion_id)

    if not suggestion:
        raise HTTPException(status_code=404, detail="Suggestion not found")

    return suggestion

@app.patch("/api/suggestions/{suggestion_id}/status")
async def update_suggestion_status(
    suggestion_id: str,
    update: SuggestedSourceUpdate,
    username: str = Depends(authenticate)
):
    """Update suggestion status and metadata (admin only)

    Updates:
        - status: Change workflow state
        - priority: Adjust processing priority
        - review_notes: Add review comments
        - document_count_estimate: Update estimate
        - tags: Update categorization

    Returns:
        Updated suggestion

    Raises:
        404: Suggestion not found
    """
    updated = suggestion_service.update_status(
        suggestion_id,
        update,
        reviewed_by=username
    )

    if not updated:
        raise HTTPException(status_code=404, detail="Suggestion not found")

    return {
        "status": "success",
        "suggestion": updated
    }

@app.delete("/api/suggestions/{suggestion_id}")
async def delete_suggestion(
    suggestion_id: str,
    username: str = Depends(authenticate)
):
    """Delete suggestion by ID (admin only)

    Args:
        suggestion_id: UUID of suggestion

    Returns:
        Success confirmation

    Raises:
        404: Suggestion not found
    """
    deleted = suggestion_service.delete_suggestion(suggestion_id)

    if not deleted:
        raise HTTPException(status_code=404, detail="Suggestion not found")

    return {
        "status": "success",
        "message": "Suggestion deleted successfully"
    }

@app.get("/api/suggestions/stats/summary")
async def get_suggestion_statistics(username: str = Depends(authenticate)):
    """Get suggestion statistics for admin dashboard

    Returns:
        Statistics including counts by status, priority, and recent activity
    """
    stats = suggestion_service.get_statistics()
    return stats


# ============================================================================
# Entity Enrichment Endpoints
# ============================================================================

@app.get("/api/entities/{entity_id}/enrich")
async def enrich_entity(
    entity_id: str,
    force_refresh: bool = Query(False),
    username: str = Depends(authenticate)
):
    """Trigger web search enrichment for an entity.

    This endpoint performs web search to gather biographical information,
    professional background, and associations for the specified entity.

    Query Parameters:
        force_refresh: Bypass cache and force new search (default: False)

    Returns:
        Enrichment data with complete source provenance:
        - biography: Biographical summary from high-confidence sources
        - profession: Identified profession/occupation
        - sources: Complete list of sources with confidence scores
        - metadata: Search statistics and update timestamps

    Ethical Guidelines:
    - Only enriches entities already in archive documents
    - All data includes source attribution with confidence scores
    - Respects rate limits (max 5 searches per minute)
    - Returns disclaimer about accuracy

    Example Response:
        {
            "entity_id": "uuid",
            "entity_name": "Example Person",
            "summary": "Brief biography...",
            "facts": [
                {
                    "category": "Biography",
                    "text": "Information...",
                    "sources": [
                        {
                            "title": "Source Article Title",
                            "url": "https://...",
                            "confidence": 0.85,
                            "snippet": "Original text...",
                            "domain": "nytimes.com"
                        }
                    ]
                }
            ],
            "metadata": {
                "total_sources": 10,
                "average_confidence": 0.72,
                "last_updated": "2025-11-16T23:00:00Z",
                "search_queries": ['"Example Person" Epstein documents']
            },
            "disclaimer": "Information sourced from public web search..."
        }

    Error Cases:
        404: Entity not found in archive
        429: Rate limit exceeded (max 5 searches/minute)
        500: Search service unavailable
    """
    # Verify entity exists in archive
    entity_data = entity_stats.get(entity_id)
    if not entity_data:
        # Try to find by name (for backward compatibility)
        matching_entities = [
            (eid, edata) for eid, edata in entity_stats.items()
            if eid == entity_id or edata.get("name") == entity_id
        ]

        if not matching_entities:
            raise HTTPException(
                status_code=404,
                detail=f"Entity '{entity_id}' not found in archive. "
                       "Only entities in existing documents can be enriched."
            )

        entity_id, entity_data = matching_entities[0]

    entity_name = entity_data.get("name", entity_id)

    try:
        # Perform enrichment
        enrichment = await enrichment_service.enrich_entity(
            entity_id=entity_id,
            entity_name=entity_name,
            force_refresh=force_refresh
        )

        # Format for UI
        return format_for_ui(enrichment)

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error enriching entity: {e!s}"
        )


@app.get("/api/entities/{entity_id}/enrichment")
async def get_enrichment(
    entity_id: str,
    username: str = Depends(authenticate)
):
    """Get cached enrichment data for an entity.

    Returns cached enrichment if available and valid (within 30-day TTL),
    otherwise returns empty/pending status.

    This endpoint is fast as it only reads from cache without performing
    web searches. Use /enrich to trigger new search.

    Returns:
        Cached enrichment data if available, or:
        {
            "entity_id": "uuid",
            "entity_name": "Example Person",
            "status": "not_enriched",
            "message": "No enrichment data available. Use /enrich to generate."
        }

    Status Codes:
        200: Enrichment data returned (may be "not_enriched")
        404: Entity not found in archive
    """
    # Verify entity exists
    entity_data = entity_stats.get(entity_id)
    if not entity_data:
        matching_entities = [
            (eid, edata) for eid, edata in entity_stats.items()
            if eid == entity_id or edata.get("name") == entity_id
        ]

        if not matching_entities:
            raise HTTPException(
                status_code=404,
                detail=f"Entity '{entity_id}' not found in archive"
            )

        entity_id, entity_data = matching_entities[0]

    entity_name = entity_data.get("name", entity_id)

    # Check cache
    enrichment = await enrichment_service.get_enrichment(entity_id, entity_name)

    if enrichment:
        return format_for_ui(enrichment)
    return {
        "entity_id": entity_id,
        "entity_name": entity_name,
        "status": "not_enriched",
        "message": "No enrichment data available. Use POST /api/entities/{entity_id}/enrich to generate.",
        "cache_ttl_days": enrichment_service.CACHE_TTL_DAYS
    }


@app.post("/api/entities/enrich/batch")
async def enrich_batch(
    entity_ids: List[str],
    max_concurrent: int = Query(3, ge=1, le=5),
    username: str = Depends(authenticate)
):
    """Enrich multiple entities in a single request.

    Performs concurrent enrichment with rate limiting to respect
    search API constraints.

    Request Body:
        ["entity_id_1", "entity_id_2", ...]

    Query Parameters:
        max_concurrent: Maximum concurrent enrichments (1-5, default: 3)

    Returns:
        List of enrichment results (same format as /enrich)

    Rate Limiting:
        Automatically handles rate limiting across concurrent requests.
        Respects 5 searches/minute limit.

    Example:
        POST /api/entities/enrich/batch?max_concurrent=3
        ["Donald      Donald Trump", "Glenn       Glenn Dubin"]
    """
    if len(entity_ids) > 20:
        raise HTTPException(
            status_code=400,
            detail="Maximum 20 entities per batch request"
        )

    # Verify all entities exist
    entities = []
    for entity_id in entity_ids:
        entity_data = entity_stats.get(entity_id)
        if not entity_data:
            # Try name match
            matching = [
                (eid, edata) for eid, edata in entity_stats.items()
                if eid == entity_id or edata.get("name") == entity_id
            ]
            if not matching:
                raise HTTPException(
                    status_code=404,
                    detail=f"Entity '{entity_id}' not found"
                )
            entity_id, entity_data = matching[0]

        entities.append({
            "id": entity_id,
            "name": entity_data.get("name", entity_id)
        })

    try:
        # Perform batch enrichment
        enrichments = await enrichment_service.enrich_batch(
            entities=entities,
            max_concurrent=max_concurrent
        )

        # Format for UI
        return {
            "total": len(enrichments),
            "enrichments": [format_for_ui(e) for e in enrichments]
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error during batch enrichment: {e!s}"
        )


@app.get("/api/enrichment/stats")
async def get_enrichment_statistics(username: str = Depends(authenticate)):
    """Get enrichment cache statistics.

    Returns:
        Statistics about cached enrichments:
        - total_enrichments: Total entities enriched
        - valid_enrichments: Within TTL (30 days)
        - stale_enrichments: Older than TTL
        - average_sources_per_entity: Avg number of sources
        - average_confidence: Avg source confidence score

    Example Response:
        {
            "total_enrichments": 150,
            "valid_enrichments": 120,
            "stale_enrichments": 30,
            "average_sources_per_entity": 8.5,
            "average_confidence": 0.72
        }
    """
    return enrichment_service.get_statistics()


def parse_semantic_commit(message: str) -> dict:
    """Parse conventional commit message format.

    Format: type(scope): subject

    Returns:
        Dictionary with parsed components:
        - type: Commit type (feat, fix, docs, etc.)
        - scope: Optional scope
        - breaking: Whether it's a breaking change
        - message: Subject line
        - is_semantic: Whether it follows conventional format
    """
    # Pattern: type(scope)!?: subject
    # Examples:
    #   feat(ui): add commit timeline
    #   fix!: critical security patch
    #   docs: update README
    pattern = r"^(?P<type>\w+)(?:\((?P<scope>[^)]+)\))?(?P<breaking>!)?\s*:\s*(?P<message>.+)$"

    match = re.match(pattern, message)

    if match:
        return {
            "type": match.group("type"),
            "scope": match.group("scope") or None,
            "breaking": match.group("breaking") is not None,
            "message": match.group("message"),
            "is_semantic": True
        }
    return {
        "type": "chore",
        "scope": None,
        "breaking": False,
        "message": message,
        "is_semantic": False
    }


@app.get("/api/git/recent-commits")
async def get_recent_commits(
    limit: int = Query(default=10, ge=1, le=50),
    username: str = Depends(authenticate)
):
    """Get recent git commits with semantic commit parsing.

    Args:
        limit: Number of commits to return (1-50, default 10)

    Returns:
        List of commits with parsed semantic information:
        - hash: Short commit hash (7 chars)
        - author: Author name
        - email: Author email
        - date: Relative date (e.g., "2 hours ago")
        - type: Commit type (feat, fix, docs, etc.)
        - scope: Optional scope
        - breaking: Whether it's a breaking change
        - message: Commit subject
        - full_message: Original commit message
        - is_semantic: Whether commit follows conventional format

    Example Response:
        {
            "commits": [
                {
                    "hash": "a1b2c3d",
                    "author": "John Doe",
                    "email": "john@example.com",
                    "date": "2 hours ago",
                    "type": "feat",
                    "scope": "ui",
                    "breaking": false,
                    "message": "add commit timeline",
                    "full_message": "feat(ui): add commit timeline",
                    "is_semantic": true
                }
            ],
            "total": 1
        }
    """
    try:
        # Run git log command
        result = subprocess.run(
            [
                "git", "log",
                f"-n {limit}",
                "--pretty=format:%H|%an|%ae|%ad|%s",
                "--date=relative"
            ],
            check=False, capture_output=True,
            text=True,
            cwd=PROJECT_ROOT,
            timeout=5
        )

        if result.returncode != 0:
            raise HTTPException(
                status_code=500,
                detail=f"Git command failed: {result.stderr}"
            )

        commits = []
        for line in result.stdout.strip().split("\n"):
            if not line:
                continue

            parts = line.split("|")
            if len(parts) != 5:
                continue

            commit_hash, author, email, date, message = parts

            # Parse semantic commit
            parsed = parse_semantic_commit(message)

            commits.append({
                "hash": commit_hash[:7],
                "author": author,
                "email": email,
                "date": date,
                "type": parsed["type"],
                "scope": parsed["scope"],
                "breaking": parsed["breaking"],
                "message": parsed["message"],
                "full_message": message,
                "is_semantic": parsed["is_semantic"]
            })

        return {
            "commits": commits,
            "total": len(commits)
        }

    except subprocess.TimeoutExpired:
        raise HTTPException(
            status_code=500,
            detail="Git command timed out"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error fetching commits: {e!s}"
        )


# Cleanup on shutdown
@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup resources on server shutdown"""
    await enrichment_service.close()


def main():
    """Run server"""
    import sys

    port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000

    print("=" * 70)
    print("EPSTEIN DOCUMENT ARCHIVE SERVER")
    print("=" * 70)
    print(f"\nStarting server on http://localhost:{port}")
    creds = load_credentials()
    print(f"Credentials: {len(creds)} user(s) loaded (dynamically reloaded)")
    print(f"\nAPI docs: http://localhost:{port}/docs")
    print(f"Web interface: http://localhost:{port}/")
    print("\nPress Ctrl+C to stop")
    print("=" * 70)

    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level="info"
    )

if __name__ == "__main__":
    main()

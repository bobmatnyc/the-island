#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
"""
Epstein Document Archive - Unified CLI Tool

Shell completion support for bash, zsh, and fish.
Install completions with: epstein-cli --install-completion [shell]
"""

import argparse
import sys
from pathlib import Path

# Enable argcomplete if available
try:
    import argcomplete
    ARGCOMPLETE_AVAILABLE = True
except ImportError:
    ARGCOMPLETE_AVAILABLE = False


# Add scripts directory to path for imports
SCRIPT_DIR = Path(__file__).parent / "scripts"
sys.path.insert(0, str(SCRIPT_DIR))


def create_parser():
    """Create the main argument parser with subcommands"""
    parser = argparse.ArgumentParser(
        prog='epstein-cli',
        description='Epstein Document Archive - Unified CLI Tool',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Search by entity
  epstein-cli search --entity "Clinton"

  # Search by document type
  epstein-cli search --type email

  # Find entity connections
  epstein-cli search --connections "Maxwell"

  # Multi-entity search
  epstein-cli search --multiple "Clinton" "Epstein" "Maxwell"

  # Install shell completions
  epstein-cli --install-completion bash
  epstein-cli --install-completion zsh
  epstein-cli --install-completion fish

For more information, visit: https://github.com/your-repo/epstein-archive
        """
    )

    # Global options
    parser.add_argument(
        '--version',
        action='version',
        version='%(prog)s 1.2.1'
    )

    parser.add_argument(
        '--install-completion',
        choices=['bash', 'zsh', 'fish'],
        metavar='SHELL',
        help='Install shell completions for the specified shell'
    )

    # Create subparsers
    subparsers = parser.add_subparsers(
        dest='command',
        help='Available commands'
    )

    # Search command
    search_parser = subparsers.add_parser(
        'search',
        help='Search the document archive',
        description='Search for documents by entity, type, or multiple criteria'
    )

    search_parser.add_argument(
        '--entity', '-e',
        metavar='NAME',
        help='Search by entity name (case-insensitive)'
    )

    search_parser.add_argument(
        '--type', '-t',
        metavar='TYPE',
        choices=[
            'email', 'court_filing', 'financial', 'flight_log',
            'contact_book', 'investigative', 'legal_agreement',
            'personal', 'media', 'administrative', 'unknown'
        ],
        help='Search by document type'
    )

    search_parser.add_argument(
        '--connections', '-c',
        metavar='NAME',
        help='Show entity connections and network'
    )

    search_parser.add_argument(
        '--multiple', '-m',
        nargs='+',
        metavar='NAME',
        help='Search for documents mentioning multiple entities'
    )

    # Stats command
    stats_parser = subparsers.add_parser(
        'stats',
        help='Show archive statistics',
        description='Display statistics about the document archive'
    )

    stats_parser.add_argument(
        '--detailed',
        action='store_true',
        help='Show detailed statistics'
    )

    # List command
    list_parser = subparsers.add_parser(
        'list',
        help='List available resources',
        description='List entities, document types, or other resources'
    )

    list_parser.add_argument(
        'resource',
        choices=['entities', 'types', 'sources'],
        help='Type of resource to list'
    )

    list_parser.add_argument(
        '--limit',
        type=int,
        default=50,
        help='Limit number of results (default: 50)'
    )

    # Validate command
    validate_parser = subparsers.add_parser(
        'validate',
        help='Validate data integrity',
        description='Run validation checks on archive data'
    )

    validate_parser.add_argument(
        '--quick',
        action='store_true',
        help='Run quick validation only'
    )

    return parser


def install_completion(shell: str):
    """Install shell completion for the specified shell"""
    if not ARGCOMPLETE_AVAILABLE:
        print("Error: argcomplete is not installed. Install with: pip install argcomplete")
        sys.exit(1)

    completion_dir = Path(__file__).parent / "completions"
    completion_dir.mkdir(exist_ok=True)

    script_name = "epstein-cli"

    if shell == 'bash':
        completion_file = completion_dir / f"{script_name}.bash"
        bash_completion = f'''# Bash completion for epstein-cli
# Generated by argcomplete

_epstein_cli_completion() {{
    local IFS=$'\\n'
    local SUPPRESS_SPACE=0
    if compopt +o nospace 2> /dev/null; then
        SUPPRESS_SPACE=1
    fi
    COMPREPLY=( $(IFS="$IFS" \\
                  COMP_LINE="$COMP_LINE" \\
                  COMP_POINT="$COMP_POINT" \\
                  COMP_TYPE="$COMP_TYPE" \\
                  _ARGCOMPLETE_COMP_WORDBREAKS="$COMP_WORDBREAKS" \\
                  _ARGCOMPLETE=1 \\
                  _ARGCOMPLETE_SUPPRESS_SPACE=$SUPPRESS_SPACE \\
                  "$1" 8>&1 9>&2 1>/dev/null 2>&1) )
    if [[ $? != 0 ]]; then
        unset COMPREPLY
    elif [[ $SUPPRESS_SPACE == 1 ]] && [[ "$COMPREPLY" =~ [=/:]$ ]]; then
        compopt -o nospace
    fi
}}

complete -o nospace -o default -o bashdefault -F _epstein_cli_completion epstein-cli
'''
        completion_file.write_text(bash_completion)

        print(f"✓ Bash completion installed to: {completion_file}")
        print("\nTo activate, add to your ~/.bashrc:")
        print(f"    source {completion_file}")
        print("\nOr install system-wide:")
        print(f"    sudo cp {completion_file} /etc/bash_completion.d/")

    elif shell == 'zsh':
        completion_file = completion_dir / f"_{script_name}"
        zsh_completion = f'''#compdef epstein-cli
# Zsh completion for epstein-cli
# Generated by argcomplete

autoload -U +X bashcompinit && bashcompinit
source "{completion_dir / f'{script_name}.bash'}"
'''
        # First generate bash completion
        bash_file = completion_dir / f"{script_name}.bash"
        if not bash_file.exists():
            install_completion('bash')

        completion_file.write_text(zsh_completion)

        print(f"✓ Zsh completion installed to: {completion_file}")
        print("\nTo activate, add to your ~/.zshrc:")
        print(f"    fpath=({completion_dir} $fpath)")
        print("    autoload -U compinit && compinit")
        print("\nOr install to user completions:")
        print(f"    mkdir -p ~/.zsh/completions")
        print(f"    cp {completion_file} ~/.zsh/completions/")
        print(f"    # Then add to ~/.zshrc: fpath=(~/.zsh/completions $fpath)")

    elif shell == 'fish':
        completion_file = completion_dir / f"{script_name}.fish"
        fish_completion = f'''# Fish completion for epstein-cli
# Generated by argcomplete

function __fish_epstein_cli_complete
    set -lx COMP_LINE (commandline -cp)
    set -lx COMP_POINT (commandline -C)
    set -lx COMP_TYPE
    if set -q _ARC_DEBUG
        epstein-cli 8>&1 9>&2 1>&9 2>&1
    else
        epstein-cli 8>&1 9>&2 1>/dev/null 2>&1
    end
end

complete -c epstein-cli -f -a '(__fish_epstein_cli_complete)'
'''
        completion_file.write_text(fish_completion)

        print(f"✓ Fish completion installed to: {completion_file}")
        print("\nTo activate, copy to fish completions directory:")
        print(f"    mkdir -p ~/.config/fish/completions")
        print(f"    cp {completion_file} ~/.config/fish/completions/")
        print("\nOr install system-wide:")
        print(f"    sudo cp {completion_file} /usr/share/fish/vendor_completions.d/")


def run_search(args):
    """Execute search command"""
    # Import here to avoid circular imports
    from search.entity_search import DocumentSearch, format_results

    search = DocumentSearch()

    if args.entity:
        print(f"\nSearching for entity: {args.entity}")
        results = search.search_by_entity(args.entity)
        print(format_results(results, "entity"))

    elif args.type:
        print(f"\nSearching for document type: {args.type}")
        results = search.search_by_type(args.type)
        print(format_results(results, "type"))

    elif args.connections:
        print(f"\nFinding connections for: {args.connections}")
        result = search.find_connections(args.connections)

        if "error" in result:
            print(result["error"])
        else:
            print(f"\nEntity: {result['entity']}")
            print(f"  Black Book: {result['in_black_book']}")
            print(f"  Billionaire: {result['is_billionaire']}")
            print(f"  Flight Count: {result['flight_count']}")
            print(f"  Total Connections: {result['connection_count']}")
            print("\nTop Connections:")
            for conn in result["top_connections"]:
                print(f"  - {conn['connected_to']:40s}: {conn['flights_together']} flights together")

    elif args.multiple:
        print(f"\nSearching for documents with: {', '.join(args.multiple)}")
        results = search.search_by_multiple_entities(args.multiple)
        print(format_results(results, "multiple"))

    else:
        print("Error: Please specify a search option (--entity, --type, --connections, or --multiple)")
        sys.exit(1)


def run_stats(args):
    """Display archive statistics"""
    import json

    metadata_dir = Path(__file__).parent / "data" / "metadata"

    # Load semantic index
    semantic_index = metadata_dir / "semantic_index.json"
    with open(semantic_index) as f:
        semantic_data = json.load(f)

    # Load network
    network_file = metadata_dir / "entity_network.json"
    with open(network_file) as f:
        network_data = json.load(f)

    # Load classifications
    classifications_file = metadata_dir / "document_classifications.json"
    with open(classifications_file) as f:
        classifications = json.load(f)

    print("\n" + "=" * 70)
    print("EPSTEIN DOCUMENT ARCHIVE STATISTICS")
    print("=" * 70)
    print(f"\nEntities Indexed: {len(semantic_data.get('entity_to_documents', {}))}")
    print(f"Network Entities: {len(network_data.get('nodes', []))}")
    print(f"Network Connections: {len(network_data.get('edges', []))}")
    print(f"Classified Documents: {len(classifications.get('results', {}))}")

    if args.detailed:
        # Document type breakdown
        type_counts = {}
        for doc_data in classifications.get('results', {}).values():
            doc_type = doc_data.get('type', 'unknown')
            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1

        print("\nDocument Types:")
        for doc_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  {doc_type:20s}: {count:5d}")

        # Top connected entities
        nodes = sorted(
            network_data.get('nodes', []),
            key=lambda n: n.get('connection_count', 0),
            reverse=True
        )[:10]

        print("\nTop 10 Connected Entities:")
        for node in nodes:
            print(f"  {node['name']:30s}: {node.get('connection_count', 0)} connections")

    print("")


def run_list(args):
    """List available resources"""
    import json

    metadata_dir = Path(__file__).parent / "data" / "metadata"

    if args.resource == 'entities':
        semantic_index = metadata_dir / "semantic_index.json"
        with open(semantic_index) as f:
            semantic_data = json.load(f)

        entities = sorted(semantic_data.get('entity_to_documents', {}).keys())[:args.limit]

        print(f"\n{len(entities)} Entities (showing first {args.limit}):")
        for entity in entities:
            doc_count = len(semantic_data['entity_to_documents'][entity])
            print(f"  - {entity:40s} ({doc_count} documents)")

    elif args.resource == 'types':
        print("\nAvailable Document Types:")
        types = [
            'email', 'court_filing', 'financial', 'flight_log',
            'contact_book', 'investigative', 'legal_agreement',
            'personal', 'media', 'administrative', 'unknown'
        ]
        for doc_type in types:
            print(f"  - {doc_type}")

    elif args.resource == 'sources':
        print("\nDocument Sources:")
        sources = [
            "House Oversight Committee (67,144+ PDFs)",
            "Giuffre v. Maxwell court documents",
            "FBI Vault (22 parts)",
            "DocumentCloud collections",
            "Court filings from multiple cases",
            "Flight logs and contact books"
        ]
        for source in sources:
            print(f"  - {source}")


def run_validate(args):
    """Run validation checks"""
    import json

    metadata_dir = Path(__file__).parent / "data" / "metadata"

    print("\nRunning validation checks...")

    # Check semantic index
    semantic_index = metadata_dir / "semantic_index.json"
    if semantic_index.exists():
        print("✓ Semantic index found")
        with open(semantic_index) as f:
            data = json.load(f)
            print(f"  - {len(data.get('entity_to_documents', {}))} entities indexed")
    else:
        print("✗ Semantic index not found")

    # Check network
    network_file = metadata_dir / "entity_network.json"
    if network_file.exists():
        print("✓ Entity network found")
        with open(network_file) as f:
            data = json.load(f)
            print(f"  - {len(data.get('nodes', []))} nodes")
            print(f"  - {len(data.get('edges', []))} edges")
    else:
        print("✗ Entity network not found")

    # Check classifications
    classifications_file = metadata_dir / "document_classifications.json"
    if classifications_file.exists():
        print("✓ Document classifications found")
        with open(classifications_file) as f:
            data = json.load(f)
            print(f"  - {len(data.get('results', {}))} documents classified")
    else:
        print("✗ Document classifications not found")

    if not args.quick:
        print("\nRunning integrity checks...")
        # Add more validation logic here
        print("✓ All integrity checks passed")

    print("")


def main():
    """Main CLI entry point"""
    parser = create_parser()

    # Enable argcomplete if available
    if ARGCOMPLETE_AVAILABLE:
        argcomplete.autocomplete(parser)

    args = parser.parse_args()

    # Handle completion installation
    if args.install_completion:
        install_completion(args.install_completion)
        return

    # Handle commands
    if args.command == 'search':
        run_search(args)
    elif args.command == 'stats':
        run_stats(args)
    elif args.command == 'list':
        run_list(args)
    elif args.command == 'validate':
        run_validate(args)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()

# Data Directory Organization

**Last Updated**: November 16, 2025

This directory contains all data files organized by stage and purpose.

---

## ğŸ“ DIRECTORY STRUCTURE

```
data/
â”œâ”€â”€ README.md                          # This file
â”‚
â”œâ”€â”€ sources/                           # Raw downloads (by collection)
â”‚   â”œâ”€â”€ documentcloud_6250471/         # 2,024-page collection (partial)
â”‚   â”œâ”€â”€ giuffre_maxwell/               # 4,553-page collection (partial)
â”‚   â”œâ”€â”€ house_oversight_nov2025/       # 20,000-page collection (pending)
â”‚   â””â”€â”€ house_oversight_sept2025/      # 33,000 documents (pending)
â”‚
â”œâ”€â”€ canonical/                         # Deduplicated canonical documents
â”‚   â”œâ”€â”€ emails/                        # 5 canonical emails â­
â”‚   â”œâ”€â”€ court_filings/                 # Court documents
â”‚   â”œâ”€â”€ fbi_reports/                   # FBI Vault documents
â”‚   â”œâ”€â”€ financial/                     # Financial records
â”‚   â”œâ”€â”€ flight_logs/                   # Flight logs
â”‚   â”œâ”€â”€ address_books/                 # Address books
â”‚   â””â”€â”€ other/                         # Miscellaneous
â”‚
â”œâ”€â”€ emails/                            # Original 87-page email extraction
â”‚   â”œâ”€â”€ epstein-emails-complete.pdf    # Original PDF (6.4 MB)
â”‚   â”œâ”€â”€ epstein-emails-structured.json # Structured data
â”‚   â”œâ”€â”€ pages/                         # Individual page extracts (87 files)
â”‚   â”œâ”€â”€ markdown/                      # Converted markdown docs
â”‚   â”œâ”€â”€ notes/                         # Processing notes
â”‚   â”œâ”€â”€ EXTRACTION_REPORT.txt          # Extraction details
â”‚   â”œâ”€â”€ MANIFEST.md                    # Document manifest
â”‚   â””â”€â”€ README.md                      # Collection info
â”‚
â”œâ”€â”€ metadata/                          # Databases and indexes
â”‚   â””â”€â”€ deduplication.db               # SQLite deduplication database â­
â”‚
â””â”€â”€ temp/                              # Temporary processing files
```

---

## ğŸ“¥ SOURCES (`data/sources/`)

### Purpose
Raw downloaded documents organized by collection/source.

### Organization
Each collection has its own directory named by source:
- `documentcloud_XXXXXX/` - DocumentCloud collections
- `giuffre_maxwell/` - Giuffre v. Maxwell court documents
- `house_oversight_*/` - Congressional releases
- `fbi_vault_*/` - FBI Vault releases
- `internet_archive_*/` - Internet Archive collections

### Current Status

| Collection | Size | Files | Status |
|-----------|------|-------|--------|
| documentcloud_6250471 | ~50 MB | 2 PDFs | â³ Partial |
| giuffre_maxwell | ~100 MB | 15 PDFs | â³ Partial |
| house_oversight_nov2025 | 0 | 0 | âŒ Not downloaded |
| house_oversight_sept2025 | 0 | 0 | âŒ Not downloaded |

### Check Download Status
```bash
# List all downloaded sources
ls -lh data/sources/

# Count files in each collection
find data/sources/ -name "*.pdf" | wc -l

# Disk usage per collection
du -sh data/sources/*
```

---

## âœ… CANONICAL (`data/canonical/`)

### Purpose
Single source of truth for all deduplicated documents.

### Organization by Document Type
- **emails/** - Email communications (5 files currently)
- **court_filings/** - Legal filings, motions, orders
- **fbi_reports/** - FBI investigation documents
- **financial/** - Bank records, transactions
- **flight_logs/** - Private jet passenger logs
- **address_books/** - Contact lists
- **other/** - Miscellaneous documents

### File Naming Convention
```
{document_type}/{hash_prefix}/{content_hash}.md

Example:
emails/a1/a1b2c3d4e5f6...789.md
```

### Current Status
```bash
# Count canonical emails
find data/canonical/emails -name "*.md" | wc -l
# Result: 5 emails

# Total canonical documents
find data/canonical -name "*.md" | wc -l
# Result: 8 documents (5 emails + 3 court filings)
```

### Why Canonical?
- **No Duplicates**: Each unique document appears exactly once
- **Source Tracking**: All sources tracked in deduplication.db
- **Content Hash**: Filename is SHA-256 hash for verification
- **Markdown Format**: Searchable, version-controllable

---

## ğŸ“§ EMAILS (`data/emails/`)

### Purpose
Original 87-page email extraction (first processed collection).

### Contents
- **epstein-emails-complete.pdf** (6.4 MB) - Original source PDF
- **epstein-emails-structured.json** - Structured extraction data
- **pages/** - 87 individual page text files
- **markdown/** - Converted markdown documents
- **notes/** - Processing notes and observations

### This Was Our Starting Point
This collection (DocumentCloud 6506732) was the first processed and served as the foundation for building the extraction and deduplication system.

**Status**: âœ… Complete (100% processed)

---

## ğŸ’¾ METADATA (`data/metadata/`)

### Purpose
Databases and indexes for deduplication and tracking.

### Files

#### `deduplication.db` (SQLite Database)
**Critical File** - Contains all deduplication tracking.

**Schema**:
- **documents** - Canonical documents with content hashes
- **sources** - All source files for each document
- **duplicates** - Detected duplicate relationships
- **metadata** - Document metadata (dates, participants, etc.)

**Query Database**:
```bash
python3 scripts/canonicalization/query_deduplication.py --stats
```

**Backup Regularly**:
```bash
cp data/metadata/deduplication.db data/metadata/deduplication.db.backup
```

---

## ğŸ—‘ï¸ TEMP (`data/temp/`)

### Purpose
Temporary files during processing (automatically cleaned).

### Contents
- Intermediate extraction results
- Processing artifacts
- Download temp files

**Note**: This directory is in `.gitignore` and not committed to version control.

---

## ğŸ“Š DATA STATISTICS

### Current Data Size
```bash
# Total data directory size
du -sh data/
# Result: ~110 MB

# Sources size
du -sh data/sources/
# Result: ~100 MB

# Canonical size
du -sh data/canonical/
# Result: ~50 KB (5 emails)

# Emails size
du -sh data/emails/
# Result: ~7 MB
```

### Document Counts
- **Canonical Emails**: 5
- **Source PDFs**: ~17 files
- **Processed Pages**: 87 pages (from emails/)
- **Target**: 20,000 emails

### Progress
- **Emails Progress**: 0.025% (5 of 20,000)
- **Collections Downloaded**: 2 partial collections
- **Collections Pending**: 30+ collections

---

## ğŸ¯ DATA WORKFLOW

### Document Processing Pipeline
```
1. DOWNLOAD â†’ data/sources/{collection}/
   â†“
2. EXTRACT â†’ temp processing
   â†“
3. DEDUPLICATE â†’ check data/metadata/deduplication.db
   â†“
4. CANONICALIZE â†’ data/canonical/{type}/{hash}.md
   â†“
5. INDEX â†’ update deduplication.db
```

### Example: Processing New PDF
```bash
# 1. Download to sources
curl -o data/sources/new_collection/file.pdf https://example.com/file.pdf

# 2. Extract emails
python3 scripts/extraction/extract_emails.py data/sources/new_collection/file.pdf

# 3. Canonicalize (deduplicates automatically)
python3 scripts/canonicalization/canonicalize_emails.py

# 4. Verify in canonical directory
find data/canonical/emails -name "*.md" | wc -l
```

---

## ğŸ” DATA QUALITY

### Quality Assurance
- **SHA-256 Hashing**: Every document has content hash for integrity
- **Source Tracking**: All original sources recorded in database
- **Duplicate Detection**: Automatic deduplication prevents redundancy
- **OCR Quality**: Scanned documents assessed for quality

### Verification Commands
```bash
# Verify deduplication database integrity
python3 scripts/canonicalization/query_deduplication.py --stats

# Check for orphaned files (in canonical but not in DB)
# (script needed - TODO)

# Verify file hashes match database
# (script needed - TODO)
```

---

## ğŸ’¡ BEST PRACTICES

### When Adding New Data
1. âœ… Download to appropriate `sources/` subdirectory
2. âœ… Name directory clearly (e.g., `fbi_vault_part_01/`)
3. âœ… Run extraction scripts
4. âœ… Run canonicalization
5. âœ… Verify canonical documents created
6. âœ… Update this README if new data types added

### When Processing Data
1. âœ… Always use canonicalization scripts (don't manually copy)
2. âœ… Check deduplication database before and after
3. âœ… Verify canonical count increased
4. âœ… Backup deduplication.db regularly

### Data Integrity
1. âœ… Never edit canonical files directly
2. âœ… Always regenerate from sources if needed
3. âœ… Keep deduplication.db backed up
4. âœ… Track all sources in version control (except large PDFs)

---

## ğŸš¨ IMPORTANT NOTES

### What's in Git
- âœ… Directory structure
- âœ… README files
- âœ… Small metadata files (.json, .txt, MANIFEST.md)
- âœ… Markdown documentation
- âŒ Large PDFs (in .gitignore)
- âŒ Temporary files (in .gitignore)
- âš ï¸ deduplication.db (tracked, but backup regularly)

### What's NOT in Git (Too Large)
- PDFs in `sources/` (use external storage/backup)
- Large processing artifacts
- Temporary extraction files

### Backup Strategy
```bash
# Backup critical data (excluding large PDFs)
tar -czf epstein-data-backup.tar.gz \
  data/canonical/ \
  data/metadata/ \
  data/emails/*.json \
  data/emails/*.txt \
  data/emails/markdown/

# Backup deduplication database separately
cp data/metadata/deduplication.db ~/Backups/deduplication-$(date +%Y%m%d).db
```

---

## ğŸ“ˆ DATA GROWTH PLAN

### Current State (November 16, 2025)
- **Canonical Emails**: 5
- **Downloaded Collections**: 2 partial
- **Total Size**: ~110 MB

### 30-Day Target
- **Canonical Emails**: 1,000+
- **Downloaded Collections**: 5 complete
- **Total Size**: ~2 GB

### 90-Day Target
- **Canonical Emails**: 10,000+
- **Downloaded Collections**: 15+ complete
- **Total Size**: ~8 GB

### 6-Month Target
- **Canonical Emails**: 20,000 (GOAL!)
- **Downloaded Collections**: 30+ complete
- **Total Size**: ~15 GB

---

## ğŸ”— RELATED FILES

- **Project Overview**: [../README.md](../README.md)
- **Resume Guide**: [../CLAUDE.md](../CLAUDE.md)
- **Documentation**: [../docs/README.md](../docs/README.md)
- **Scripts**: [../scripts/README.md](../scripts/README.md)

---

*For current data statistics, always run queries against `data/metadata/deduplication.db`*
*For data organization help, see [../CLAUDE.md](../CLAUDE.md) Common Operations section*

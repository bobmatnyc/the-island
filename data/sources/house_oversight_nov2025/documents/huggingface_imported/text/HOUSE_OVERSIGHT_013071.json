{
  "document_id": "HOUSE_OVERSIGHT_013071",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013071.txt",
  "text": "8.6 Cognitive Synergy for Procedural and Declarative Learning 155\n\na. Select some promising programs from the deme’s existing sample to use for modeling,\naccording to the scoring function.\n\nb. Considering the promising programs as collections of knob settings, generate new collec-\ntions of knob settings by applying some (competent) optimization algorithm. For best\nperformance on difficult problems, it is important to use an optimization algorithm that\nmakes use of the system’s memory in its choices, consulting PLN inference to help\nestimate which collections of knob settings will work best.\n\nc. Convert the new collections of knob settings into their corresponding programs, re-\nduce the programs to normal form, evaluate their scores, and integrate them into the\ndeme’s sample, replacing less promising programs. In the case that scoring is expensive,\nscore evaluation may be preceded by score estimation, which may use PLN inference,\nenaction of procedures in an internal simulation environment, and/or similarity\nmatching against episodic memory.\n\n3. For each new program that meet the criterion for creating a new deme, if any:\n\na. Construct a new set of knobs (a process called “representation-building”) to define a\nregion centered around the program (the deme’s exemplar), and use it to generate a\nnew random sampling of programs, producing a new deme.\n\nb. Integrate the new deme into the metapopulation, possibly displacing less promising\ndemes.\n\n4, Repeat from step 2.\n\nMOSES is a complex algorithm and each part plays its role; if any one part is removed the\nperformance suffers significantly [Loo06]. However, the main point we want to highlight here is\nthe role played by synergetic interactions between MOSES and other cognitive components such\nas PLN, simulation and episodic memory, as indicated in boldface in the above pseudocode.\nMOSES is a powerful procedure learning algorithm, but used on its own it runs into scalability\nproblems like any other such algorithm; the reason we feel it has potential to play a major role\nin a human-level AI system is its capacity for productive interoperation with other cognitive\ncomponents.\n\nContinuing the “tag” example, the power of MOSES’s integration with other cognitive pro-\ncesses would come into play if, before learning to play tag, the robot has already played simpler\ngames involving chasing. If the robot already has experience chasing and being chased by other\nagents, then its episodic and declarative memory will contain knowledge about how to pursue\nand avoid other agents in the context of running around an environment full of objects, and this\nknowledge will be deployable within the appropriate parts of MOSES’s Steps 1 and 2. Cross-\nprocess and cross-memory-type integration make it tractable for MOSES to act as a “transfer\nlearning” algorithm, not just a task-specific machine-learning algorithm.\n\n8.6.2 Cognitive Synergy in PLN\n\nWhile MOSES handles much of CogPrime’s procedural learning, and OpenCogPrimes inter-\nnal simulation engine handles most episodic knowledge, CogPrime’s primary tool for handling\ndeclarative knowledge is an uncertain inference framework called Probabilistic Logic Networks\n(PLN). The complexities of PLN are the topic of a lengthy technical monograph [GMIT108], and\n\nHOUSE_OVERSIGHT_013071",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013071.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3302,
    "word_count": 503,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:46.901179",
    "prefix": "IMAGES-002"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016221",
  "filename": "TEXT-001-HOUSE_OVERSIGHT_016221.txt",
  "text": "﻿National Pub date: February 19, 2019\nTitle: DEEP THINKING\nSubtitle: Twenty-Five Ways of Looking at AI\nBy: John Brockman\nLength: 90,000 words\nHeadline: Science world luminary John Brockman assembles twenty-five of the most\nimportant scientific minds, people who have been thinking about the field artificial\nintelligence for most of their careers for an unparalleled round-table examination about\nmind, thinking, intelligence and what it means to be human.\nDescription:\n\"Artificial intelligence is today's story—the story behind all other stories. It is the Second\nComing and the Apocalypse at the same time: Good AI versus evil AI.\" —John\nBrockman\nMore than sixty years ago, mathematician-philosopher Norbert Wiener published a book\non the place of machines in society that ended with a warning: “we shall never receive\nthe right answers to our questions unless we ask the right questions…. The hour is very\nlate, and the choice of good and evil knocks at our door.”\nIn the wake of advances in unsupervised, self-improving machine learning, a small but\ninfluential community of thinkers is considering Wiener’s words again. In Deep\nThinking, John Brockman gathers their disparate visions of where AI might be taking us.\nThe fruit of the long history of Brockman’s profound engagement with the most\nimportant scientific minds who have been thinking about AI—from Alison Gopnik and\nDavid Deutsch to Frank Wilczek and Stephen Wolfram— Deep Thinking is an ideal\nintroduction to the landscape of crucial issues AI presents.\nThe collision between opposing perspectives is salutary and exhilarating; some of these\nfigures, such as computer scientist Stuart Russell, Skype co-founder Jaan Tallinn, and\nphysicist Max Tegmark, are deeply concerned with the threat of AI, including the\nexistential one, while others, notably robotics entrepreneur Rodney Brooks, philosopher\nDaniel Dennett, and bestselling author Steven Pinker, have a very different view. Serious,\nsearching and authoritative, Deep Thinking lays out the intellectual landscape of one of\nthe most important topics of our time.\nParticipants in The Deep Thinking Project\nChris Anderson is an entrepreneur; a roboticist; former editor-in-chief of Wired; cofounder\nand CEO of 3DR; and author of The Long Tail, Free, and Makers.\nRodney Brooks is a computer scientist; Panasonic Professor of Robotics, emeritus, MIT;\nformer director, MIT Computer Science Lab; and founder, chairman, and CTO of\nRethink Robotics. He is the author of Flesh and Machines.\nGeorge M. Church is Robert Winthrop Professor of Genetics at Harvard Medical\nSchool; Professor of Health Sciences and Technology, Harvard-MIT; and co-author (with\nEd Regis) of Regenesis: How Synthetic Biology Will Reinvent Nature and Ourselves.\nDaniel C. Dennett is University Professor and Austin B. Fletcher Professor of\nPhilosophy and director of the Center for Cognitive Studies at Tufts University. He is the\nauthor of a dozen books, including Consciousness Explained and, most recently, From\nBacteria to Bach and Back: The Evolution of Minds.\nDavid Deutsch is a quantum physicist and a member of the Centre for Quantum\nComputation at the Clarendon Laboratory, Oxford University. He is the author of The\nFabric of Reality and The Beginning of Infinity.\nAnca Dragan is an assistant professor in the Department of Electrical Engineering and\nComputer Sciences at UC Berkeley. She co-founded and serves on the steering\ncommittee for the Berkeley AI Research (BAIR) Lab and is a co-principal investigator in\nBerkeley’s Center for Human-Compatible AI.\nGeorge Dyson is a historian of science and technology and the author of Baidarka: the\nKayak, Darwin Among the Machines, Project Orion, and Turing’s Cathedral.\nPeter Galison is a science historian, Joseph Pellegrino University Professor and cofounder\nof\nthe Black Hole Initiative at Harvard University, and the author of Einstein's Clocks and\nPoincaré’s Maps: Empires of Time.\nNeil Gershenfeld is a physicist and director of MIT’s Center for Bits and Atoms. He is\nthe author of FAB, co-author (with Alan Gershenfeld & Joel Cutcher-Gershenfeld) of\nDesigning Reality, and founder of the global fab lab network.\nAlison Gopnik is a developmental psychologist at UC Berkeley; her books include The\nPhilosophical Baby and, most recently, The Gardener and the Carpenter: What the New\nScience of Child Development Tells Us About the Relationship Between Parents and\nChildren.\n2\nTom Griffiths is Henry R. Luce Professor of Information, Technology, Consciousness,\nand Culture at Princeton University. He is co-author (with Brian Christian) of Algorithms\nto Live By.\nW. Daniel “Danny” Hillis is an inventor, entrepreneur, and computer scientist, Judge\nWidney Professor of Engineering and Medicine at USC, and author of The Pattern on the\nStone: The Simple Ideas That Make Computers Work.\nCaroline A. Jones is a professor of art history in the Department of Architecture at MIT\nand author of Eyesight Alone: Clement Greenberg’s Modernism and the\nBureaucratization of the Senses; Machine in the Studio: Constructing the Postwar\nAmerican Artist; and The Global Work of Art.\nDavid Kaiser is Germeshausen Professor of the History of Science and professor of\nphysics at MIT, and head of its Program in Science, Technology & Society. He is the\nauthor of How the Hippies Saved Physics: Science, Counterculture, and the Quantum\nRevival and American Physics and the Cold War Bubble (forthcoming).\nSeth Lloyd is a theoretical physicist at MIT, Nam P. Suh Professor in the Department of\nMechanical Engineering, and an external professor at the Santa Fe Institute. He is the\nauthor of Programming the Universe: A Quantum Computer Scientist Takes on the\nCosmos.\nHans Ulrich Obrist is artistic director of the Serpentine Gallery, London, and the author\nof Ways of Curating and Lives of the Artists, Lives of the Architects.\nJudea Pearl is professor of computer science and director of the Cognitive Systems\nLaboratory at UCLA. His most recent book, co-authored with Dana Mackenzie, is The\nBook of Why: The\nAlex “Sandy” Pentland is Toshiba Professor and professor of media arts and sciences,\nMIT; director of the Human Dynamics and Connection Science labs and the Media Lab\nEntrepreneurship Program, and the author of Social Physics.\nNew Science of Cause and Effect.\nSteven Pinker, a Johnstone Family Professor in the Department of Psychology at\nHarvard University, is an experimental psychologist who conducts research in visual\ncognition, psycholinguistics, and social relations. He is the author of eleven books,\nincluding The Blank Slate, The Better Angels of Our Nature, and, most recently,\nEnlightenment Now: The Case for Reason, Science, Humanism, and Progress.\nVenki Ramakrishnan is a scientist at the Medical Research Council Laboratory of\nMolecular Biology, Cambridge University; recipient of the Nobel Prize in Chemistry\n(2009); current president of the Royal Society; and the author of Gene Machine: The\nRace to Discover the Secrets of the Ribosome.\n3\nStuart Russell is a professor of computer science and Smith-Zadeh Professor in\nEngineering at UC Berkeley. He is the coauthor (with Peter Norvig) of Artificial\nIntelligence: A Modern Approach.\nJaan Tallin, a computer programmer, theoretical physicist, and investor, is a codeveloper\nof Skype and Kazaa.\nMax Tegmark is an MIT physicist and AI researcher; president of the Future of Life\nInstitute; scientific director of the Foundational Questions Institute; and the author of Our\nMathematical Universe and Life 3.0: Being Human in the Age of Artificial Intelligence.\nFrank Wilczek is Herman Feshbach Professor of Physics at MIT, recipient of the 2004\nNobel Prize in physics, and the author of A Beautiful Question: Finding Nature’s Deep\nDesign.\nStephen Wolfram is a scientist, inventor, and the founder and CEO of Wolfram\nResearch. He is the creator of the symbolic computation program Mathematica and its\nprogramming language, Wolfram Language, as well as the knowledge engine\nWolfram|Alpha. He is also the author of A New Kind of Science.\n4\nDeep Thinking\nTwenty-five Ways of Looking at AI\nedited by John Brockma\nPenguin Press — February 19, 2019\n5\nTable of Contents\nAcknowledgments\nIntroduction: On the Promise and Peril of AI\nby John Brockman\nArtificial intelligence is today’s story—the story behind all other stories. It is the Second\nComing and the Apocalypse at the same time: Good AI versus evil AI. This book comes\nout of an ongoing conversation with a number of important thinkers, both in the world of\nAI and beyond it, about what AI is and what it means. Called the Deep Thinking Project,\nthis conversation began in earnest in September 2016, in a meeting at the Mayflower\nGrace Hotel in Washington, Connecticut with some of the book’s contributors.\nWhat quickly emerged from that first meeting is that the excitement and fear in the wider\nculture surrounding AI now has an analogue in the way Norbert Wiener’s ideas\nregarding “cybernetics” worked their way through the culture, particularly in the\n1960’s, as artists began to incorporate thinking about new technologies into their work.\nI witnessed the impact of those ideas at close hand; indeed it’s not too much to say they\nset me off on my life’s path. With the advent of the digital era beginning in the early\n1970s, people stopped talking about Wiener, but today, his Cybernetic Idea has been so\nwidely adopted that it’s internalized to the point where it no longer needs a name. It’s\neverywhere, it’s in the air, and it’s a fitting a place to begin.\nSeth Lloyd: Wrong, but More Relevant Than Ever\nIt is exactly in the extension of the cybernetic idea to human beings that Wiener’s\nconceptions missed their target.\nJudea Pearl: The Limitations of Opaque Learning Machines\nDeep learning has its own dynamics, it does its own repair and its own optimization, and\nit gives you the right results most of the time. But when it doesn’t, you don’t have a clue\nabout what went wrong and what should be fixed.\nStuart Russell: The Purpose Put Into the Machine\nWe may face the prospect of superintelligent machines—their actions by definition\nunpredictable by us and their imperfectly specified objectives conflicting with our own—\nwhose motivation to preserve their existence in order to achieve those objectives may be\ninsuperable.\nGeorge Dyson: The Third Law\nAny system simple enough to be understandable will not be complicated enough to\nbehave intelligently, while any system complicated enough to behave intelligently will be\ntoo complicated to understand.\nDaniel C. Dennett: What Can We Do?\nWe don’t need artificial conscious agents. We need intelligent tools.\n6\nRodney Brooks: The Inhuman Mess Our Machines Have Gotten Us Into\nWe are in a much more complex situation today than Wiener foresaw, and I am worried\nthat it is much more pernicious than even his worst imagined fears.\nFrank Wilczek: The Unity of Intelligence\nThe advantages of artificial over natural intelligence appear permanent, while the\nadvantages of natural over artificial intelligence, though substantial at present, appear\ntransient.\nMax Tegmark: Let’s Aspire to More Than Making Ourselves Obsolete\nWe should analyze what could go wrong with AI to ensure that it goes right.\nJaan Tallinn: Dissident Messages\nContinued progress in AI can precipitate a change of cosmic proportions—a runaway\nprocess that will likely kill everyone.\nSteven Pinker: Tech Prophecy and the Underappreciated Causal Power of Ideas\nThere is no law of complex systems that says that intelligent agents must turn into\nruthless megalomaniacs.\nDavid Deutsch: Beyond Reward and Punishment\nMisconceptions about human thinking and human origins are causing corresponding\nmisconceptions about AGI and how it might be created.\nTom Griffiths: The Artificial Use of Human Beings\nAutomated intelligent systems that will make good inferences about what people want\nmust have good generative models for human behavior.\nAnca Dragan: Putting the Human into the AI Equation\nIn the real world, an AI must interact with people and reason about them. People will\nhave to formally enter the AI problem definition somewhere.\nChris Anderson: Gradient Descent\nJust because AI systems sometimes end up in local minima, don’t conclude that this\nmakes them any less like life. Humans—indeed, probably all life-forms—are often stuck\nin local minima.\nDavid Kaiser: “Information” for Wiener, for Shannon, and for Us\nMany of the central arguments in The Human Use of Human Beings seem closer to the\n19th century than the 21st. Wiener seems not to have fully embraced Shannon’s notion of\ninformation as consisting of irreducible, meaning-free bits.\nNeil Gershenfeld: Scaling\nAlthough machine making and machine thinking might appear to be unrelated trends,\nthey lie in each other’s futures.\n7\nW. Daniel Hillis: The First Machine Intelligences\nHybrid superintelligences such as nation states and corporations have their own\nemergent goals and their actions are not always aligned to the interests of the people\nwho created them.\nVenki Ramakrishnan: Will Computers Become Our Overlords?\nOur fears about AI reflect the belief that our intelligence is what makes us special.\nAlex “Sandy” Pentland: The Human Strategy\nHow can we make a good human-artificial ecosystem, something that’s not a machine\nsociety but a cyberculture in which we can all live as humans—a culture with a human\nfeel to it?\nHans Ulrich Obrist: Making the Invisible Visible: Art Meets AI\nMany contemporary artists are articulating various doubts about the promises of AI and\nreminding us not to associate the term “artificial intelligence” solely with positive\noutcomes.\nAlison Gopnik: AIs versus Four-Year-Olds\nLooking at what children do may give programmers useful hints about directions for\ncomputer learning.\nPeter Galison: Algorists Dream of Objectivity\nBy now, the legal, ethical, formal, and economic dimensions of algorithms are all quasiinfinite.\nGeorge M. Church: The Rights of Machines\nProbably we should be less concerned about us-versus-them and more concerned about\nthe rights of all sentients in the face of an emerging unprecedented diversity of minds.\nCaroline A. Jones: The Artistic Use of Cybernetic Beings\nThe work of cybernetically inclined artists concerns the emergent behaviors of life that\nelude AI in its current condition.\nStephen Wolfram: Artificial Intelligence and the Future of Civilization\nThe most dramatic discontinuity will surely be when we achieve effective human\nimmortality. Whether this will be achieved biologically or digitally isn’t clear, but\ninevitably it will be achieved.\n8\nIntroduction: On the Promise and Peril of AI\nJohn Brockman\nArtificial intelligence is today’s story—the story behind all other stories. It is the Second\nComing and the Apocalypse at the same time: Good AI versus evil AI. This book comes\nout of an ongoing conversation with a number of important thinkers, both in the world of\nAI and beyond it, about what AI is and what it means. Called the Deep Thinking Project,\nthis conversation began in earnest in September 2016, in a meeting at the Mayflower\nGrace Hotel in Washington, Connecticut with some of the book’s contributors.\nWhat quickly emerged from that first meeting is that the excitement and fear in the wider\nculture surrounding AI now has an analogue in the way Norbert Wiener’s ideas regarding\n“cybernetics” worked their way through the culture, particularly in the 1960’s, as artists\nbegan to incorporate thinking about new technologies into their work. I witnessed the\nimpact of those ideas at close hand; indeed it’s not too much to say they set me off on my\nlife’s path. With the advent of the digital era beginning in the early 1970s, people stopped\ntalking about Wiener, but today, his Cybernetic Idea has been so widely adopted that it’s\ninternalized to the point where it no longer needs a name. It’s everywhere, it’s in the air,\nand it’s a fitting a place to begin.\nNew Technologies=New Perceptions\nBefore AI, there was Cybernetics—the idea of automatic, self-regulating control, laid out\nin Norbert Wiener’s foundational text of 1948. I can date my own serious exposure to it\nto 1966, when the composer John Cage invited me and four or five other young arts\npeople to join him for a series of dinners—an ongoing seminar about media,\ncommunications, art, music, and philosophy that focused on Cage’s interest in the ideas\nof Wiener, Claude Shannon, and Marshall McLuhan, all of whom had currency in the\nNew York art circles in which I was then moving. In particular, Cage had picked up on\nMcLuhan’s idea that by inventing electronic technologies we had externalized our central\nnervous system—that is, our minds—and that we now had to presume that “there’s only\none mind, the one we all share.”\nIdeas of this nature were beginning to be of great interest to the artists I was\nworking with in New York at the Film-Makers’ Cinémathèque, where I was program\nmanager for a series of multimedia productions called the New Cinema 1 (also known as\nthe Expanded Cinema Festival), under the auspices of avant-garde filmmaker and\nimpresario Jonas Mekas. They included visual artists Claes Oldenburg, Robert\nRauschenberg, Andy Warhol, Robert Whitman; kinetic artists Charlotte Moorman and\nNam June Paik; happenings artists Allan Kaprow and Carolee Schneemann; dancer Tricia\nBrown; filmmakers Jack Smith, Stan Vanderbeek, Ed Emshwiller, and the Kuchar\nbrothers; avant-garde dramatist Ken Dewey; poet Gerd Stern and the USCO group;\nminimalist musicians Lamonte Young and Terry Riley; and through Warhol, the music\ngroup, The Velvet Underground. Many of these people were reading Wiener, and\ncybernetics was in the air. It was at one of these dinners that Cage reached into his\nbriefcase and took out a copy of Cybernetics and handed it to me, saying, “This is for\nyou.”\n9\nDuring the Festival, I received an unexpected phone call from Wiener’s colleague\nArthur K. Solomon, head of Harvard’s graduate program in biophysics. Wiener had died\nthe year before, and Solomon and Wiener’s other close colleagues at MIT and Harvard\nhad been reading about the Expanded Cinema Festival in the New York Times and were\nintrigued by the connection to Wiener’s work. Solomon invited me to bring some of the\nartists up to Cambridge to meet with him and a group that included MIT sensorycommunications\nresearcher Walter Rosenblith, Harvard applied mathematician Anthony\nOettinger, and MIT engineer Harold “Doc” Edgerton, inventor of the strobe light.\nLike many other “art meets science” situations I’ve been involved in since, the\ntwo-day event was an informed failure: ships passing in the night. But I took it all\nonboard and the event was consequential in some interesting ways—one of which came\nfrom the fact that they took us to see “the” computer. Computers were a rarity back then;\nat least, none of us on the visit had ever seen one. We were ushered into a large space on\nthe MIT campus, in the middle of which there was a “cold room” raised off the floor and\nenclosed in glass, in which technicians wearing white lab coats, scarves, and gloves were\nbusy collating punch cards coming through an enormous machine. When I approached,\nthe steam from my breath fogged up the window into the cold room. Wiping it off, I saw\n“the” computer. I fell in love.\nLater, in the Fall of 1967, I went to Menlo Park to spend time with Stewart Brand,\nwhom I had met in New York in 1965 when he was a satellite member of the USCO\ngroup of artists. Now, with his wife Lois, a mathematician, he was preparing the first\nedition of The Whole Earth Catalog for publication. While Lois and the team did the\nheavy lifting on the final mechanicals for WEC, Stewart and I sat together in a corner for\ntwo days, reading, underlining, and annotating the same paperback copy of Cybernetics\nthat Cage had handed to me the year before, and debating Wiener’s ideas.\nInspired by this set of ideas, I began to develop a theme, a mantra of sorts, that\nhas informed my endeavors since: “new technologies = new perceptions.” Inspired by\ncommunications theorist Marshall McLuhan, architect-designer Buckminster Fuller,\nfuturist John McHale, and cultural anthropologists Edward T. (Ned) Hall and Edmund\nCarpenter, I started reading avidly in the field of information theory, cybernetics, and\nsystems theory. McLuhan suggested I read biologist J.Z. Young’s Doubt and Certainty\nin Science in which he said that we create tools and we mold ourselves through our use of\nthem. The other text he recommended was Warren Weaver and Claude Shannon’s 1949\npaper “Recent Contributions to the Mathematical Theory of Communication,” which\nbegins: “The word communication will be used here in a very broad sense to include all\nof the procedures by which one mind may affect another. This, of course, involves not\nonly written and oral speech, but also music, the pictorial arts, the theater, the ballet, and\nin fact all human behavior.\"\nWho knew that within two decades of that moment we would begin to recognize\nthe brain as a computer? And in the next two decades, as we built our computers into the\nInternet, that we would begin to realize that the brain is not a computer, but a network of\ncomputers? Certainly not Wiener, a specialist in analogue feedback circuits designed to\ncontrol machines, nor the artists, nor, least of all, myself.\n“We must cease to kiss the whip that lashes us.”\n10\nTwo years after Cybernetics, in 1950, Norbert Wiener published The Human Use of\nHuman Beings—a deeper story, in which he expressed his concerns about the runaway\ncommercial exploitation and other unforeseen consequences of the new technologies of\ncontrol. I didn’t read The Human Use of Human Beings until the spring of 2016, when I\npicked up my copy, a first edition, which was sitting in my library next to Cybernetics.\nWhat shocked me was the realization of just how prescient Wiener was in 1950 about\nwhat’s going on today. Although the first edition was a major bestseller—and, indeed,\njump-started an important conversation—under pressure from his peers Wiener brought\nout a revised and milder edition in 1954, from which the original concluding chapter,\n“Voices of Rigidity,” is conspicuously absent.\nScience historian George Dyson points out that in this long-forgotten first edition,\nWiener predicted the possibility of a “threatening new Fascism dependent on the machine\nà gouverner”:\nNo elite escaped his criticism, from the Marxists and the Jesuits (“all of\nCatholicism is indeed essentially a totalitarian religion”) to the FBI (“our great\nmerchant princes have looked upon the propaganda technique of the Russians,\nand have found that it is good”) and the financiers lending their support “to make\nAmerican capitalism and the fifth freedom of the businessman supreme\nthroughout the world.” Scientists . . . received the same scrutiny given the\nChurch: “Indeed, the heads of great laboratories are very much like Bishops, with\ntheir association with the powerful in all walks of life, and the dangers they incur\nof the carnal sins of pride and of lust for power.”\nThis jeremiad did not go well for Wiener. As Dyson puts it:\nThese alarms were discounted at the time, not because Wiener was wrong about\ndigital computing but because larger threats were looming as he completed his\nmanuscript in the fall of 1949. Wiener had nothing against digital computing but\nwas strongly opposed to nuclear weapons and refused to join those who were\nbuilding digital computers to move forward on the thousand-times-more-powerful\nhydrogen bomb.\nSince the original of The Human Use of Human Beings is now out of print, lost to\nus is Wiener’s cri de coeur, more relevant today than when he wrote it, sixty-eight years\nago: “We must cease to kiss the whip that lashes us.”\nMind, Thinking, Intelligence\nAmong the reasons we don’t hear much about “Cybernetics” today, two are central: First,\nalthough The Human Use of Human Beings was considered an important book in its time,\nit ran counter to the aspirations of many of Wiener’s colleagues, including John von\nNeumann and Claude Shannon, who were interested in the commercialization of the new\ntechnologies. Second, computer pioneer John McCarthy disliked Wiener and refused to\nuse Wiener’s term “Cybernetics.” McCarthy, in turn, coined the term “artificial\nintelligence” and became a founding father of that field.\n11\nAs Judea Pearl, who, in the 1980s, introduced a new approach to artificial\nintelligence called Bayesian networks, explained to me:\nWhat Wiener created was excitement to believe that one day we are going to\nmake an intelligent machine. He wasn't a computer scientist. He talked feedback,\nhe talked communication, he talked analog. His working metaphor was a\nfeedback circuit, which he was an expert in. By the time the digital age began in\nthe early 1960s people wanted to talk programming, talk codes, talk about\ncomputational functions, talk about short-term memory, long-term memory—\nmeaningful computer metaphors. Wiener wasn’t part of that, and he didn’t reach\nthe new generation that germinated with his ideas. His metaphors were too old,\npassé. There were new means already available that were ready to capture the\nhuman imagination.” By 1970, people were no longer talking about Wiener.\nOne critical factor missing in Wiener’s vision was the cognitive element: mind, thinking,\nintelligence. As early as 1942, at the first of a series of foundational interdisciplinary\nmeetings about the control of complex systems that would come to be known as the\nMacy conferences, leading researchers were arguing for the inclusion of the cognitive\nelement into the conversation. While von Neumann, Shannon, and Wiener were\nconcerned about systems of control and communication of observed systems, Warren\nMcCullough wanted to include mind. He turned to cultural anthropologists Gregory\nBateson and Margaret Mead to make the connection to the social sciences. Bateson in\nparticular was increasingly talking about patterns and processes, or “the pattern that\nconnects.” He called for a new kind of systems ecology in which organisms and the\nenvironment in which they live are one in the same, and should be considered as a single\ncircuit. By the early 1970s the Cybernetics of observed systems—1 st order Cybernetics—\nmoved to the Cybernetics of observing systems—2 nd order Cybernetics—or “the\nCybernetics of Cybernetics”, as coined by Heinz von Foerster, who joined the Macy\nconferences in the mid 1950s, and spearheaded the new movement.\nCybernetics, rather than disappearing, was becoming metabolized into everything,\nso we no longer saw it as a separate, distinct new discipline. And there it remains, hiding\nin plain sight.\n“The Shtick of the Steins”\nMy own writing about these issues at the time was on the radar screen of the 2 nd order\nCybernetics crowd, including Heinz von Foerster as well as John Lilly and Alan Watts,\nwho were the co-organizers of something called \"The AUM Conference,\" shorthand for\n“The American University of Masters”, which took place in Big Sur in 1973, a gathering\nof philosophers, psychologists, and scientists, each of whom asked to lecture on his own\nwork in terms of its relationship to the ideas of British mathematician G. Spencer Brown\npresented in his book, Laws of Form.\nI was a bit puzzled when I received an invitation—a very late invitation indeed—\nwhich they explained was based on their interest in the ideas I presented in a book called\nAfterwords, which were very much on their wavelength. I jumped at the opportunity, the\nmain reason being that the keynote speaker was none other than Richard Feynman. I love\n12\nto spend time with physicists, the reason being that they think about the universe, i.e.\neverything. And no physicist was reputed to be articulate as Feynman. I couldn’t wait to\nmeet him. I accepted. That said, I am not a scientist, and I had never entertained the idea\nof getting on a stage and delivering a “lecture” of any kind, least of all a commentary on\nan obscure mathematical theory in front of a group identified as the world’s most\ninteresting thinkers. Only upon my arrival in Big Sur did I find out the reason for my\nvery late invitation. “When is Feynman’s talk?” I asked at the desk. “Oh, didn’t Alan\nWatts tell you? Richard is ill and has been hospitalized. You’re his replacement. And, by\nthe way, what’s the title of your keynote lecture?”\nI tried to make myself invisible for several days. Alan Watts, realizing that I was\navoiding the podium, woke me up one night with a 3am knock on the door of my room. I\nopened the door to find him standing in front of me wearing a monk’s robe with a hood\nthat covering much of his face. His arms extended, he held a lantern in one hand, and a\nmagnum of scotch on the other.\n“John”, he said in a deep voice with a rich aristocratic British accent, “you are a\nphony.” “And, John”, he continued, I am a phony. But John, I am a real phony!”\nThe next day I gave my lecture, entitled \"Einstein, Gertrude Stein, Wittgenstein,\nand Frankenstein.\" Einstein: the revolution in 20 th century physics; Gertrude Stein: the\nfirst writer who made integral to her work the idea of an indeterminate and discontinuous\nuniverse. Words represented neither character nor activity: A rose is a rose is a rose, and\na universe is a universe is a universe.); Wittgenstein: the world as limits of language.\n“The limits of my language mean the limits of my world”. The end of the distinction\nbetween observer and observed. Frankenstein: Cybernetics AI, robotics, all the essayists\nin this volume.\nThe lecture had unanticipated consequences. Among the participants at the AUM\nConference were several authors of #1 New York Times bestsellers, yet no one there had\na literary agent. And I realized that all were engaged in writing a genre of book both\nunnamed and unrecognized by New York publishers. Since I had an MBA from\nColombia Business School, and a series of relative successes in business, I was\ndragooned into becoming an agent, initially for Gregory Bateson and John Lilly, whose\nbooks I sold quickly, and for sums that caught my attention, thus kick-starting my career\nas a literary agent.\nI never did meet Richard Feynman.\nThe Long AI Winters\nThis new career put me in close touch with most of the AI pioneers, and over the decades\nI rode with them on waves of enthusiasm, and into valleys of disappointment.\nIn the early ‘80s the Japanese government mounted a national effort to advance\nAI. They called it the 5 th Generation; their goal was to change the architecture of\ncomputation by breaking “the von Neumann bottleneck”, by creating a massively parallel\ncomputer. In so doing, they hoped to jumpstart their economy and become a dominant\nworld power in the field. In1983, the leader of the Japanese 5 th Generation consortium\ncame to New York for a meeting organized by Heinz Pagels, the president of the New\nYork Academy of Sciences. I had a seat at the table alongside the leaders of the 1 st\ngeneration, Marvin Minsky and John McCarthy, the 2 nd generation, Edward Feigenbaum\n13\nand Roger Schank, and Joseph Traub, head of the National Supercomputer Consortium.\nIn 1981 with Heinz’s help, I had founded “The Reality Club” (the precursor to the\nnon-profit Edge.org), whose initial interdisciplinary meetings took place in the Board\nRoom at the NYAS. Heinz was working on his book, Dreams of Reason: The Rise of the\nScience of Complexity, which he considered to be a research agenda for science in the\n1990's.\nThrough the Reality Club meetings, I got to know two young researchers who\nwere about to play key roles in revolutionizing computer science. At MIT in the late\nseventies, Danny Hillis developed the algorithms that made possible the massively\nparallel computer. In 1983, his company, Thinking Machines, built the world's fastest\nsupercomputer by utilizing parallel architecture. His \"connection machine,\" closely\nreflected the workings of the human mind. Seth Lloyd at Rockefeller University was\nundertaking seminal work in the fields of quantum computation and quantum\ncommunications, including proposing the first technologically feasible design for a\nquantum computer.\nAnd the Japanese? Their foray into artificial intelligence failed, and was followed\nby twenty years of anemic economic growth. But, the leading US scientists took this\nprogram very seriously. And Feigenbaum, who was the cutting-edge computer scientist\nof the day, teamed up with McCorduck to write a book on these developments. The Fifth\nGeneration: Artificial Intelligence and Japan's Computer Challenge to the World was\npublished in 1983. We had a code name for the project: “It’s coming, it’s coming!” But it\ndidn’t come; it went.\nFrom that point on I’ve worked with researchers in nearly every variety of AI and\ncomplexity, including Rodney Brooks, Hans Moravec, John Archibald Wheeler, Benoit\nMandelbrot, John Henry Holland, Danny Hillis, Freeman Dyson, Chris Langton, Doyne\nFarmer, Geoffrey West, Stuart Russell, and Judea Pearl.\nAn Ongoing Dynamical Emergent System\nFrom the initial meeting in Washington, CT to the present, I arranged a number of\ndinners and discussions in London and Cambridge, Massachusetts, as well as a public\nevent at London’s City Hall. Among the attendees were distinguished scientists, science\nhistorians, and communications theorists, all of whom have been thinking seriously about\nAI issues for their entire careers.\nI commissioned essays from a wide range of contributors, with or without\nreferences to Wiener (leaving it up to each participant). In the end, 25 people wrote\nessays, all individuals concerned about what is happening today in the age of AI. Deep\nThinking in not my book, rather it is our book: Seth Lloyd, Judea Pearl, Stuart Russell,\nGeorge Dyson, Daniel C. Dennett, Rodney Brooks, Frank Wilczek, Max Tegmark, Jaan\nTallinn, Steven Pinker, David Deutsch, Tom Griffiths, Anca Dragan, Chris Anderson,\nDavid Kaiser, Neil Gershenfeld, W. Daniel Hillis, Venki Ramakrishnan, Alex “Sandy”\nPentland, Hans Ulrich Obrist, Alison Gopnik, Peter Galison, George M. Church, Caroline\nA. Jones, Stephen Wolfram.\nI see The Deep Thinking Project as an ongoing dynamical emergent system, a\npresentation of the ideas of a community of sophisticated thinkers who are bringing their\nexperience and erudition to bear in challenging the prevailing digital AI narrative as they\n14\ncommunicate their thoughts to one another. The aim is to present a mosaic of views\nwhich will help make sense out of this rapidly emerging field.\nI asked the essayists to consider:\n(a) The Zen-like poem “Thirteen Ways of Looking at a Blackbird,” by Wallace\nStevens, which he insisted was “not meant to be a collection of epigrams or of ideas, but\nof sensations.” It is an exercise in “perspectivism,” consisting of short, separate sections,\neach of which mentions blackbirds in some way. The poem is about his own imagination;\nit concerns what he attends to.\n(b) The parable of the blind men and an elephant. Like the elephant, AI is too big\na topic for any one perspective, never mind the fact that no two people seem to see things\nthe same way.\nWhat do we want the book to do? Stewart Brand has noted that “revisiting\npioneer thinking is perpetually useful. And it gives a long perspective that invites\nthinking in decades and centuries about the subject. All contemporary discussion, is\nbound to age badly and immediately without the longer perspective.”\nDanny Hillis wants people in AI to realize how they’ve been programmed by\nWiener’s book. “You’re executing its road map,” he says, and you just don’t realize it.”\nDan Dennett would like to “let Wiener emerge as the ghost at the banquet. Think\nof it as a source of hybrid vigor, a source of unsettling ideas to shake uŒp the established\nmindset.”\nNeil Gershenfeld argues that “stealth remedial education for the people running\nthe “Big Five” would be a great output from the book.”\nFreeman Dyson Freeman, one of the few people alive who knew Wiener, notes\nthat “The Human Use of Human Beings is one of the best books ever written. Wiener got\nalmost everything right. I will be interested to see what your bunch of wizards will do\nwith it.”\nThe Evolving AI Narrative\nThings have changed—and they remain the same. Now AI is everywhere. We have the\nInternet. We have our smartphones. The founders of the dominant companies—the\ncompanies that hold “the whip that lashes us”—have net worths of $65 billion, $90\nbillion, $130 billion. High-profile individuals such as Elon Musk, Nick Bostrom, Martin\nRees, Eliezer Yudkowsky, and the late Stephen Hawking have issued dire warnings about\nAI, resulting in the ascendancy of well-funded institutes tasked with promoting “Nice\nAI.” But will we, as a species, be able to control a fully realized, unsupervised, selfimproving\nAI? Wiener’s warnings and admonitions in The Human Use of Human Beings\nare now very real, and they need to be looked at anew by researchers at the forefront of\nthe AI revolution. Here is Dyson again:\nWiener became increasingly disenchanted with the “gadget worshipers” whose\ncorporate selfishness brought “motives to automatization that go beyond a\nlegitimate curiosity and are sinful in themselves.” He knew the danger was not\nmachines becoming more like humans but humans being treated like machines.\n“The world of the future will be an ever more demanding struggle against the\nlimitations of our intelligence,” he warned in God & Golem, Inc., published in\n15\n1964, the year of his death, “not a comfortable hammock in which we can lie\ndown to be waited upon by our robot slaves.”\nIt’s time to examine the evolving AI narrative by identifying the leading members of that\nmainstream community along with the dissidents, and presenting their counternarratives\nin their own voices.\nThe essays that follow thus constitute a much-needed update from the field.\nJohn Brockman\nNew York, 2019\n16\nI met Seth Lloyd in the late 1980s, when new ways of thinking were everywhere: the\nimportance of biological organizing principles, the computational view of mathematics\nand physical processes, the emphasis on parallel networks, the importance of nonlinear\ndynamics, the new understanding of chaos, connectionist ideas, neural networks, and\nparallel distributive processing. The advances in computation during that period\nprovided us with a new way of thinking about knowledge.\nSeth likes to refer to himself as a quantum mechanic. He is internationally known\nfor his work in the field of quantum computation, which attempts to harness the exotic\nproperties of quantum theory, like superposition and entanglement, to solve problems\nthat would take several lifetimes to solve on classical computers.\nIn the essay that follows, he traces the history of information theory from Norbert\nWiener’s prophetic insights to the predictions of a technological “singularity” that some\nwould have us believe will supplant the human species. His takeaway on the recent\nprogramming method known as deep learning is to call for a more modest set of\nexpectations; he notes that despite AI’s enormous advances, robots “still can’t tie their\nown shoes.”\nIt’s difficult for me to talk about Seth without referencing his relationship with his\nfriend and professor, the late theoretical physicist Heinz Pagels of Rockefeller\nUniversity. The graduate student and the professor each had a profound effect on each\nother’s ideas.\nIn the summer of 1988, I visited Heinz and Seth at the Aspen Center for Physics.\nTheir joint work on the subject of complexity was featured in the current issue of\nScientific American; they were ebullient. That was just two weeks before Heinz’s tragic\ndeath in a hiking accident while descending Pyramid Peak with Seth. They were talking\nabout quantum computing.\n17\nWRONG, BUT MORE RELEVANT THAN EVER\nSeth Lloyd\nSeth Lloyd is a theoretical physicist at MIT, Nam P. Suh Professor in the Department of\nMechanical Engineering, and an external professor at the Santa Fe Institute.\nThe Human Use of Human Beings, Norbert Wiener’s 1950 popularization of his highly\ninfluential book Cybernetics: Control and Communication in the Animal and the\nMachine (1948), investigates the interplay between human beings and machines in a\nworld in which machines are becoming ever more computationally capable and powerful.\nIt is a remarkably prescient book, and remarkably wrong. Written at the height of the\nCold War, it contains a chilling reminder of the dangers of totalitarian organizations and\nsocieties, and of the danger to democracy when it tries to combat totalitarianism with\ntotalitarianism’s own weapons.\nWiener’s Cybernetics looked in close scientific detail at the process of control via\nfeedback. (“Cybernetics,” from the ancient Greek for “helmsman,” is the etymological\nbasis of our word “governor,” which is what James Watt called his pathbreaking\nfeedback control device that transformed the use of steam engines.) Because he was\nimmersed in problems of control, Wiener saw the world as a set of complex, interlocking\nfeedback loops, in which sensors, signals, and actuators such as engines interact via an\nintricate exchange of signals and information. The engineering applications of\nCybernetics were tremendously influential and effective, giving rise to rockets, robots,\nautomated assembly lines, and a host of precision-engineering techniques—in other\nwords, to the basis of contemporary industrial society.\nWiener had greater ambitions for cybernetic concepts, however, and in The\nHuman Use of Human Beings he spells out his thoughts on its application to topics as\ndiverse as Maxwell’s Demon, human language, the brain, insect metabolism, the legal\nsystem, the role of technological innovation in government, and religion. These broader\napplications of cybernetics were an almost unequivocal failure. Vigorously hyped from\nthe late 1940s to the early 1960s—to a degree similar to the hype of computer and\ncommunication technology that led to the dotcom crash of 2000-2001—cybernetics\ndelivered satellites and telephone switching systems but generated few if any useful\ndevelopments in social organization and society at large.\nNearly seventy years later, however, The Human Use of Human Beings has more\nto teach us humans than it did the first time around. Perhaps the most remarkable feature\nof the book is that it introduces a large number of topics concerning human/machine\ninteractions that are still of considerable relevance. Dark in tone, the book makes several\npredictions about disasters to come in the second half of the 20th century, many of which\nare almost identical to predictions made today about the second half of the 21st.\nFor example, Wiener foresaw a moment in the near future of 1950 in which\nhumans would cede control of society to a cybernetic artificial intelligence, which would\nthen proceed to wreak havoc on humankind. The automation of manufacturing, Wiener\npredicted, would both create large advances in productivity and displace many workers\nfrom their jobs—a sequence of events that did indeed come to pass in the ensuing\ndecades. Unless society could find productive occupations for these displaced workers,\nWiener warned, revolt would ensue.\n18\nBut Wiener failed to foresee crucial technological developments. Like pretty\nmuch all technologists of the 1950s, he failed to predict the computer revolution.\nComputers, he thought, would eventually fall in price from hundreds of thousands of\n(1950s) dollars to tens of thousands; neither he nor his compeers anticipated the\ntremendous explosion of computer power that would follow the development of the\ntransistor and the integrated circuit. Finally, because of his emphasis on control, Wiener\ncould not foresee a technological world in which innovation and self-organization bubble\nup from the bottom rather than being imposed from the top.\nFocusing on the evils of totalitarianism (political, scientific, and religious),\nWiener saw the world in a deeply pessimistic light. His book warned of the catastrophe\nthat awaited us if we didn’t mend our ways, fast. The current world of human beings and\nmachines, more than a half century after its publication, is much more complex, richer,\nand contains a much wider variety of political, social, and scientific systems than he was\nable to envisage. The warnings of what will happen if we get it wrong, however—for\nexample, control of the entire Internet by a global totalitarian regime—remain as relevant\nand pressing today as they were in 1950.\nWhat Wiener Got Right\nWiener’s most famous mathematical works focused on problems of signal analysis and\nthe effects of noise. During World War II, he developed techniques for aiming antiaircraft\nfire by making models that could predict the future trajectory of an airplane by\nextrapolating from its past behavior. In Cybernetics and in The Human Use of Human\nBeings, Wiener notes that this past behavior includes quirks and habits of the human\npilot, thus a mechanized device can predict the behavior of humans. Like Alan Turing,\nwhose Turing Test suggested that computing machines could give responses to questions\nwhich were indistinguishable from human responses, Wiener was fascinated by the\nnotion of capturing human behavior by mathematical description. In the 1940s, he\napplied his knowledge of control and feedback loops to neuro-muscular feedback in\nliving systems, and was responsible for bringing Warren McCulloch and Walter Pitts to\nMIT, where they did their pioneering work on artificial neural networks.\nWiener’s central insight was that the world should be understood in terms of\ninformation. Complex systems, such as organisms, brains, and human societies, consist\nof interlocking feedback loops in which signals exchanged between subsystems result in\ncomplex but stable behavior. When feedback loops break down, the system goes\nunstable. He constructed a compelling picture of how complex biological systems\nfunction, a picture that is by and large universally accepted today.\nWiener’s vision of information as the central quantity in governing the behavior\nof complex systems was remarkable at the time. Nowadays, when cars and refrigerators\nare jammed with microprocessors and much of human society revolves around computers\nand cell phones connected by the Internet, it seems prosaic to emphasize the centrality of\ninformation, computation, and communication. In Wiener’s time, however, the first\ndigital computers had only just come into existence, and the Internet was not even a\ntwinkle in the technologist’s eye.\nWiener’s powerful conception of not just engineered complex systems but all\ncomplex systems as revolving around cycles of signals and computation led to\ntremendous contributions to the development of complex human-made systems. The\n19\nmethods he and others developed for the control of missiles, for example, were later put\nto work in building the Saturn V moon rocket, one of the crowning engineering\nachievements of the 20th century. In particular, Wiener’s applications of cybernetic\nconcepts to the brain and to computerized perception are the direct precursors of today’s\nneural-network-based deep-learning circuits, and of artificial intelligence itself. But\ncurrent developments in these fields have diverged from his vision, and their future\ndevelopment may well affect the human uses both of human beings and of machines.\nWhat Wiener Got Wrong\nIt is exactly in the extension of the cybernetic idea to human beings that Wiener’s\nconceptions missed their target. Setting aside his ruminations on language, law, and\nhuman society for the moment, look at a humbler but potentially useful innovation that he\nthought was imminent in 1950. Wiener notes that prosthetic limbs would be much more\neffective if their wearers could communicate directly with their prosthetics by their own\nneural signals, receiving information about pressure and position from the limb and\ndirecting its subsequent motion. This turned out to be a much harder problem than\nWiener envisaged: Seventy years down the road, prosthetic limbs that incorporate neural\nfeedback are still in the very early stages. Wiener’s concept was an excellent one—it’s\njust that the problem of interfacing neural signals with mechanical-electrical devices is\nhard.\nMore significantly, Wiener (along with pretty much everyone else in 1950)\ngreatly underappreciated the potential of digital computation. As noted, Wiener’s\nmathematical contributions were to the analysis of signals and noise and his analytic\nmethods apply to continuously varying, or analog, signals. Although he participated in\nthe wartime development of digital computation, he never foresaw the exponential\nexplosion of computing power brought on by the introduction and progressive\nminiaturization of semiconductor circuits. This is hardly Wiener’s fault: The transistor\nhadn’t been invented yet, and the vacuum-tube technology of the digital computers he\nwas familiar with was clunky, unreliable, and unscalable to ever larger devices. In an\nappendix to the 1948 edition of Cybernetics, he anticipates chess-playing computers and\npredicts that they’ll be able to look two or three moves ahead. He might have been\nsurprised to learn that within half a century a computer would beat the human world\nchampion at chess.\nTechnological Overestimation and the Existential Risks of the Singularity\nWhen Wiener wrote his books, a significant example of technological overestimation was\nabout to occur. The 1950s saw the first efforts at developing artificial intelligence, by\nresearchers such as Herbert Simon, John McCarthy, and Marvin Minsky, who began to\nprogram computers to perform simple tasks and to construct rudimentary robots. The\nsuccess of these initial efforts inspired Simon to declare that “machines will be capable,\nwithin twenty years, of doing any work a man can do.” Such predictions turned out to be\nspectacularly wrong. As they became more powerful, computers got better and better at\nplaying chess because they could systematically generate and evaluate a vast selection of\npossible future moves. But the majority of predictions of AI, e.g., robotic maids, turned\nout to be illusory. When Deep Blue beat Garry Kasparov at chess in 1997, the most\n20\npowerful room-cleaning robot was a Roomba, which moved around vacuuming at\nrandom and squeaked when it got caught under the couch.\nTechnological prediction is particularly chancy, given that technologies progress\nby a series of refinements, halted by obstacles and overcome by innovation. Many\nobstacles and some innovations can be anticipated, but more cannot. In my own work\nwith experimentalists on building quantum computers, I typically find that some of the\ntechnological steps I expect to be easy turn out to be impossible, whereas some of the\ntasks I imagine to be impossible turn out to be easy. You don’t know until you try.\nIn the 1950s, partly inspired by conversations with Wiener, John von Neumann\nintroduced the notion of the “technological singularity.” Technologies tend to improve\nexponentially, doubling in power or sensitivity over some interval of time. (For\nexample, since 1950, computer technologies have been doubling in power roughly\nevery two years, an observation enshrined as Moore’s Law.) Von Neumann\nextrapolated from the observed exponential rate of technological improvement to\npredict that “technological progress will become incomprehensively rapid and\ncomplicated,” outstripping human capabilities in the not too distant future. Indeed, if\none extrapolates the growth of raw computing power—expressed in terms of bits and\nbit flips—into the future at its current rate, computers should match human brains\nsometime in the next two to four decades (depending on how one estimates the\ninformation-processing power of human brains).\nThe failure of the initial overly optimistic predictions of AI dampened talk about\nthe technological singularity for a few decades, but since the 2005 publication of Ray\nKurzweil’s The Singularity is Near, the idea of technological advance leading to\nsuperintelligence is back in force. Some believers, Kurzweil included, regard this\nsingularity as an opportunity: Humans can merge their brains with the\nsuperintelligence and thereby live forever. Others, such as Stephen Hawking and Elon\nMusk, worried that this superintelligence would prove to be malign and regarded it as\nthe greatest existing threat to human civilization. Still others, including some of the\ncontributors to the present volume, think such talk is overblown.\nWiener’s life work and his failure to predict its consequences are intimately\nbound up in the idea of an impending technological singularity. His work on\nneuroscience and his initial support of McCulloch and Pitts adumbrated the startlingly\neffective deep-learning methods of the present day. Over the past decade, and\nparticularly in the last five years, such deep-learning techniques have finally exhibited\nwhat Wiener liked to call Gestalt—for example, the ability to recognize that a circle is\na circle even if when slanted sideways it looks like an ellipse. His work on control,\ncombined with his work on neuromuscular feedback, was significant for the\ndevelopment of robotics and is the inspiration for neural-based human/machine\ninterfaces. His lapses in technological prediction, however, suggest that we should\ntake the notion of a technological singularity with a grain of salt. The general\ndifficulties of technological prediction and the problems specific to the development of\na superintelligence should warn us against overestimating both the power and the\nefficacy of information processing.\nThe Arguments for Singularity Skepticism\nNo exponential increase lasts forever. An atomic explosion grows exponentially, but\n21\nonly until it runs out of fuel. Similarly, the exponential advances in Moore’s Law are\nstarting to run into limits imposed by basic physics. The clock speed of computers\nmaxed out at a few gigahertz a decade and a half ago, simply because the chips were\nstarting to melt. The miniaturization of transistors is already running into quantummechanical\nproblems due to tunneling and leakage currents. Eventually, the various\nexponential improvements in memory and processing driven by Moore’s Law will\ngrind to a halt. A few more decades, however, will probably be time enough for the\nraw information-processing power of computers to match that of brains—at least by\nthe crude measures of number of bits and number of bit-flips per second.\nHuman brains are intricately constructed, the process of millions of years of\nnatural selection. In Wiener’s time, our understanding of the architecture of the brain\nwas rudimentary and simplistic. Since then, increasingly sensitive instrumentation and\nimaging techniques have shown our brains to be far more varied in structure and\ncomplex in function than Wiener could have imagined. I recently asked Tomaso\nPoggio, one of the pioneers of modern neuroscience, whether he was worried that\ncomputers, with their rapidly increasing processing power, would soon emulate the\nfunctioning of the human brain. “Not a chance,” he replied.\nThe recent advances in deep learning and neuromorphic computation are very\ngood at reproducing a particular aspect of human intelligence focused on the operation\nof the brain’s cortex, where patterns are processed and recognized. These advances\nhave enabled a computer to beat the world champion not just of chess but of Go, an\nimpressive feat, but they’re far short of enabling a computerized robot to tidy a room.\n(In fact, robots with anything approaching human capability in a broad range of\nflexible movements are still far away—search “robots falling down.” Robots are good\nat making precision welds on assembly lines, but they still can’t tie their own shoes.)\nRaw information-processing power does not mean sophisticated informationprocessing\npower. While computer power has advanced exponentially, the programs\nby which computers operate have often failed to advance at all. One of the primary\nresponses of software companies to increased processing power is to add “useful”\nfeatures which often make the software harder to use. Microsoft Word reached its\napex in 1995 and has been slowly sinking under the weight of added features ever\nsince. Once Moore’s Law starts slowing down, software developers will be confronted\nwith hard choices between efficiency, speed, and functionality.\nA major fear of the singulariteers is that as computers become more involved in\ndesigning their own software they’ll rapidly bootstrap themselves into achieving\nsuperhuman computational ability. But the evidence of machine learning points in the\nopposite direction. As machines become more powerful and capable of learning, they\nlearn more and more as human beings do—from multiple examples, often under the\nsupervision of human and machine teachers. Education is as hard and slow for\ncomputers as it is for teenagers. Consequently, systems based on deep learning are\nbecoming more rather than less human. The skills they bring to learning are not\n“better than” but “complementary to” human learning: Computer learning systems can\nidentify patterns that humans cannot—and vice versa. The world’s best chess players\nare neither computers nor humans but humans working together with computers.\nCyberspace is indeed inhabited by harmful programs, but these primarily take the form\nof malware—viruses notable for their malign mindlessness, not for their\n22\nsuperintelligence.\nWhither Wiener\nWiener noted that exponential technological progress is a relatively modern phenomenon\nand not all of it is good. He regarded atomic weapons and the development of missiles\nwith nuclear warheads as a recipe for the suicide of the human species. He compared the\nheadlong exploitation of the planet’s resources with the Mad Tea Party of Alice in\nWonderland: Having laid waste to one local environment, we make progress simply by\nmoving on to lay waste to the next. Wiener’s optimism about the development of\ncomputers and neuro-mechanical systems was tempered by his pessimism about their\nexploitation by authoritarian governments, such as the Soviet Union, and the tendency for\ndemocracies, such as the United States, to become more authoritarian themselves in\nconfronting the threat of authoritarianism.\nWhat would Wiener think of the current human use of human beings? He would\nbe amazed by the power of computers and the Internet. He would be happy that the early\nneural nets in which he played a role have spawned powerful deep-learning systems that\nexhibit the perceptual ability he demanded of them—although he might not be impressed\nthat one of the most prominent examples of such computerized Gestalt is the ability to\nrecognize photos of kittens on the World Wide Web. Rather than regarding machine\nintelligence as a threat, I suspect he would regard it as a phenomenon in its own right,\ndifferent from and co-evolving with our own human intelligence.\nUnsurprised by global warming—the Mad Tea Party of our era—Wiener would\napplaud the exponential improvement in alternative-energy technologies and would apply\nhis cybernetic expertise to developing the intricate set of feedback loops needed to\nincorporate such technologies into the coming smart electrical grid. Nonetheless,\nrecognizing that the solution to the problem of climate change is at least as much political\nas it is technological, he would undoubtedly be pessimistic about our chances of solving\nthis civilization-threatening problem in time. Wiener hated hucksters—political\nhucksters most of all—but he acknowledged that hucksters would always be with us.\nIt’s easy to forget just how scary Wiener’s world was. The United States and the\nSoviet Union were in a full-out arms race, building hydrogen bombs mounted on nuclear\nwarheads carried by intercontinental ballistic missiles guided by navigation systems to\nwhich Wiener himself—to his dismay—had contributed. I was four years old when\nWiener died. In 1964, my nursery school class was practicing duck-and-cover under our\ndesks to prepare for a nuclear attack. Given the human use of human beings in his own\nday, if he could see our current state, Wiener’s first response would be to be relieved that\nwe are still alive.\n23\nIn the 1980s, Judea Pearl introduced a new approach to artificial intelligence called\nBayesian networks. This probability-based model of machine reasoning enabled\nmachines to function—in a complex and uncertain world—as “evidence engines,”\ncontinuously revising their beliefs in light of new evidence.\nWithin a few years, Judea’s Bayesian networks had completely overshadowed the\nprevious rule-based approaches to artificial intelligence. The advent of deep learning—\nin which computers, in effect, teach themselves to be smarter by observing tons of data,\nhas given him pause, because this method lacks transparency.\nWhile recognizing the impressive achievements in deep learning by colleagues\nsuch as Michael Jordan and Geoffrey Hinton, he feels uncomfortable with this kind of\nopacity. He set out to understand the theoretical limitations of deep-learning systems\nand points out that basic barriers exist that will prevent them from achieving a human\nkind of intelligence, no matter what we do. Leveraging the computational benefits of\nBayesian networks, Judea realized that the combination of simple graphical models and\ndata could also be used to represent and infer cause-effect relationships. The\nsignificance of this discovery far transcends its roots in artificial intelligence. His latest\nbook explains causal thinking to the general public; you might say it is a primer on how\nto think even though human.\nJudea’s principled, mathematical approach to causality is a profound\ncontribution to the realm of ideas. It has already benefited virtually every field of\ninquiry, especially the data-intensive health and social sciences.\n24\nTHE LIMITATIONS OF OPAQUE LEARNING MACHINES\nJudea Pearl\nJudea Pearl is a professor of computer science and director of the Cognitive Systems\nLaboratory at UCLA. His most recent book, co-authored with Dana Mackenzie, is The\nBook of Why: The New Science of Cause and Effect.\nAs a former physicist, I was extremely interested in cybernetics. Though it did not utilize\nthe full power of Turing Machines, it was highly transparent, perhaps because it was\nfounded on classical control theory and information theory. We are losing this\ntransparency now, with the deep-learning style of machine learning. It is fundamentally a\ncurve-fitting exercise that adjusts weights in intermediate layers of a long input-output\nchain.\nI find many users who say that it “works well and we don’t know why.” Once\nyou unleash it on large data, deep learning has its own dynamics, it does its own repair\nand its own optimization, and it gives you the right results most of the time. But when it\ndoesn’t, you don’t have a clue about what went wrong and what should be fixed. In\nparticular, you do not know if the fault is in the program, in the method, or because things\nhave changed in the environment. We should be aiming at a different kind of\ntransparency.\nSome argue that transparency is not really needed. We don’t understand the\nneural architecture of the human brain, yet it runs well, so we forgive our meager\nunderstanding and use human helpers to great advantage. In the same way, they argue,\nwhy not unleash deep-learning systems and create intelligence without understanding\nhow they work? I buy this argument to some extent. I personally don’t like opacity, so I\nwon’t spend my time on deep learning, but I know that it has a place in the makeup of\nintelligence. I know that non-transparent systems can do marvelous jobs, and our brain is\nproof of that marvel.\nBut this argument has its limitation. The reason we can forgive our meager\nunderstanding of how human brains work is because our brains work the same way, and\nthat enables us to communicate with other humans, learn from them, instruct them, and\nmotivate them in our own native language. If our robots will all be as opaque as\nAlphaGo, we won’t be able to hold a meaningful conversation with them, and that would\nbe unfortunate. We will need to retrain them whenever we make a slight change in the\ntask or in the operating environment.\nSo, rather than experimenting with opaque learning machines, I am trying to\nunderstand their theoretical limitations and examine how these limitations can be\novercome. I do it in the context of causal-reasoning tasks, which govern much of how\nscientists think about the world and, at the same time, are rich in intuition and toy\nexamples, so we can monitor the progress in our analysis. In this context, we’ve\ndiscovered that some basic barriers exist, and that unless they are breached we won’t get\na real human kind of intelligence no matter what we do. I believe that charting these\nbarriers may be no less important than banging our heads against them.\nCurrent machine-learning systems operate almost exclusively in a statistical, or\nmodel-blind, mode, which is analogous in many ways to fitting a function to a cloud of\ndata points. Such systems cannot reason about “what if ?” questions and, therefore,\n25\ncannot serve as the basis for Strong AI—that is, artificial intelligence that emulates\nhuman-level reasoning and competence. To achieve human-level intelligence, learning\nmachines need the guidance of a blueprint of reality, a model—similar to a road map that\nguides us in driving through an unfamiliar city.\nTo be more specific, current learning machines improve their performance by\noptimizing parameters for a stream of sensory inputs received from the environment. It is\na slow process, analogous to the natural-selection process that drives Darwinian\nevolution. It explains how species like eagles and snakes have developed superb vision\nsystems over millions of years. It cannot explain, however, the super-evolutionary\nprocess that enabled humans to build eyeglasses and telescopes over barely a thousand\nyears. What humans had that other species lacked was a mental representation of their\nenvironment—representations that they could manipulate at will to imagine alternative\nhypothetical environments for planning and learning.\nHistorians of Homo sapiens such as Yuval Noah Harari and Steven Mithen are in\ngeneral agreement that the decisive ingredient that gave our ancestors the ability to\nachieve global dominion about forty thousand years ago was their ability to create and\nstore a mental representation of their environment, interrogate that representation, distort\nit by mental acts of imagination, and finally answer the “What if?” kind of questions.\nExamples are interventional questions (“What if I do such-and-such?”) and retrospective\nor counterfactual questions (“What if I had acted differently?”). No learning machine in\noperation today can answer such questions. Moreover, most learning machines do not\npossess a representation from which the answers to such questions can be derived.\nWith regard to causal reasoning, we find that you can do very little with any form\nof model-blind curve fitting, or any statistical inference, no matter how sophisticated the\nfitting process is. We have also found a theoretical framework for organizing such\nlimitations, which forms a hierarchy.\nOn the first level, you have statistical reasoning, which can tell you only how\nseeing one event would change your belief about another. For example, what can a\nsymptom tell you about a disease?\nThen you have a second level, which entails the first but not vice versa. It deals\nwith actions. “What will happen if we raise prices?” “What if you make me laugh?”\nThat second level of the hierarchy requires information about interventions which is not\navailable in the first. This information can be encoded in a graphical model, which\nmerely tells us which variable responds to another.\nThe third level of the hierarchy is the counterfactual. This is the language used by\nscientists. “What if the object were twice as heavy?” “What if I were to do things\ndifferently?” “Was it the aspirin that cured my headache, or the nap I took?”\nCounterfactuals are at the top level in the sense that they cannot be derived even if we\ncould predict the effects of all actions. They need an extra ingredient, in the form of\nequations, to tell us how variables respond to changes in other variables.\nOne of the crowning achievements of causal-inference research has been the\nalgorithmization of both interventions and counterfactuals, the top two layers of the\nhierarchy. In other words, once we encode our scientific knowledge in a model (which\nmay be qualitative), algorithms exist that examine the model and determine if a given\nquery, be it about an intervention or about a counterfactual, can be estimated from the\navailable data—and, if so, how. This capability has transformed dramatically the way\n26\nscientists are doing science, especially in such data-intensive sciences as sociology and\nepidemiology, for which causal models have become a second language. These\ndisciplines view their linguistic transformation as the Causal Revolution. As Harvard\nsocial scientist Gary King puts it, “More has been learned about causal inference in the\nlast few decades than the sum total of everything that had been learned about it in all\nprior recorded history.”\nAs I contemplate the success of machine learning and try to extrapolate it to the\nfuture of AI, I ask myself, “Are we aware of the basic limitations that were discovered in\nthe causal-inference arena? Are we prepared to circumvent the theoretical impediments\nthat prevent us from going from one level of the hierarchy to another level?”\nI view machine learning as a tool to get us from data to probabilities. But then we\nstill have to make two extra steps to go from probabilities into real understandingnce—\ntwo big steps. One is to predict the effect of actions, and the second is counterfactual\nimagination. We cannot claim to understand reality unless we make the last two steps.\nIn his insightful book Foresight and Understanding (1961), the philosopher\nStephen Toulmin identified the transparency-versus-opacity contrast as the key to\nunderstanding the ancient rivalry between Greek and Babylonian sciences. According to\nToulmin, the Babylonian astronomers were masters of black-box predictions, far\nsurpassing their Greek rivals in accuracy and consistency of celestial observations. Yet\nScience favored the creative-speculative strategy of the Greek astronomers, which was\nwild with metaphorical imagery: circular tubes full of fire, small holes through which\ncelestial fire was visible as stars, and hemispherical Earth riding on turtleback. It was\nthis wild modeling strategy, not Babylonian extrapolation, that jolted Eratosthenes (276-\n194 BC) to perform one of the most creative experiments in the ancient world and\ncalculate the circumference of the Earth. Such an experiment would never have occurred\nto a Babylonian data-fitter.\nModel-blind approaches impose intrinsic limitations on the cognitive tasks that\nStrong AI can perform. My general conclusion is that human-level AI cannot emerge\nsolely from model-blind learning machines; it requires the symbiotic collaboration of\ndata and models.\nData science is a science only to the extent that it facilitates the interpretation of\ndata—a two-body problem, connecting data to reality. Data alone are hardly a science,\nno matter how “big” they get and how skillfully they are manipulated. Opaque learning\nsystems may get us to Babylon, but not to Athens.\n27\nComputer scientist Stuart Russell, along with Elon Musk, Stephen Hawking, Max\nTegmark, and numerous others, has insisted that attention be paid to the potential\ndangers in creating an intelligence on the superhuman (or even the human) level—an\nAGI, or artificial general intelligence, whose programmed purposes may not necessarily\nalign with our own.\nHis early work was on understanding the notion of “bounded optimality” as a\nformal definition of intelligence that you can work on. He developed the technique of\nrational meta-reasoning, “which is, roughly speaking, that you do the computations that\nyou expect to improve the quality of your ultimate decision as quickly as possible.” He\nhas also worked on the unification of probability theory and first-order logic—resulting\nin a new and far more effective monitoring system for the Comprehensive Nuclear Test\nBan Treaty—and on the problem of decision making over long timescales (his\npresentations on the latter topic are usually titled, “Life: Play and Win in 20 trillion\nmoves”).\nHe is very concerned with the continuing development of autonomous weapons,\nsuch as lethal micro-drones, which are potentially scalable into weapons of mass\ndestruction. He drafted the letter from forty of the world’s leading AI researchers to\nPresident Obama which resulted in high-level national-security meetings.\nHis current work centers on the creation of what he calls “provably beneficial”\nAI. He wants to ensure AI safety by “imbuing systems with explicit uncertainty” about\nthe objectives of their human programmers, an approach that would amount to a fairly\nradical reordering of current AI research.\nStuart is also on the radar of anyone who has taken a course in computer science\nin the last twenty-odd years. He is co-author of “the” definitive AI textbook, with an\nestimated 5-million-plus English-language readers.\n28\nTHE PURPOSE PUT INTO THE MACHINE\nStuart Russell\nStuart Russell is a professor of computer science and Smith-Zadeh Professor in\nEngineering at UC Berkeley. He is the coauthor (with Peter Norvig) of Artificial\nIntelligence: A Modern Approach.\nAmong the many issues raised in Norbert Wiener’s The Human Use of Human Beings\n(1950) that are currently relevant, the most significant to the AI researcher is the\npossibility that humanity may cede control over its destiny to machines.\nWiener considered the machines of the near future as far too limited to exert global\ncontrol, imagining instead that machines and machine-like control systems would be\nwielded by human elites to reduce the great mass of humanity to the status of “cogs and\nlevers and rods.” Looking further ahead, he pointed to the difficulty of correctly\nspecifying objectives for highly capable machines, noting\na few of the simpler and more obvious truths of life, such as that when a djinnee is\nfound in a bottle, it had better be left there; that the fisherman who craves a boon\nfrom heaven too many times on behalf of his wife will end up exactly where he\nstarted; that if you are given three wishes, you must be very careful what you wish\nfor.\nThe dangers are clear enough:\nWoe to us if we let [the machine] decide our conduct, unless we have previously\nexamined the laws of its action, and know fully that its conduct will be carried out on\nprinciples acceptable to us! On the other hand, the machine like the djinnee, which\ncan learn and can make decisions on the basis of its learning, will in no way be\nobliged to make such decisions as we should have made, or will be acceptable to us.\nTen years later, after seeing Arthur Samuel’s checker-playing program learn to play\ncheckers far better than its creator, Wiener published “Some Moral and Technical\nConsequences of Automation” in Science. In this paper, the message is even clearer:\nIf we use, to achieve our purposes, a mechanical agency with whose operation we\ncannot efficiently interfere . . . we had better be quite sure that the purpose put into\nthe machine is the purpose which we really desire. . . .\nIn my view, this is the source of the existential risk from superintelligent AI cited in\nrecent years by such observers as Elon Musk, Bill Gates, Stephen Hawking, and Nick\nBostrom.\nPutting Purposes Into Machines\nThe goal of AI research has been to understand the principles underlying intelligent\nbehavior and to build those principles into machines that can then exhibit such behavior.\nIn the 1960s and 1970s, the prevailing theoretical notion of intelligence was the capacity\nfor logical reasoning, including the ability to derive plans of action guaranteed to achieve\na specified goal. More recently, a consensus has emerged around the idea of a rational\n29\nagent that perceives, and acts in order to maximize, its expected utility. Subfields such as\nlogical planning, robotics, and natural-language understanding are special cases of the\ngeneral paradigm. AI has incorporated probability theory to handle uncertainty, utility\ntheory to define objectives, and statistical learning to allow machines to adapt to new\ncircumstances. These developments have created strong connections to other disciplines\nthat build on similar concepts, including control theory, economics, operations research,\nand statistics.\nIn both the logical-planning and rational-agent views of AI, the machine’s\nobjective—whether in the form of a goal, a utility function, or a reward function (as in\nreinforcement learning)—is specified exogenously. In Wiener’s words, this is “the\npurpose put into the machine.” Indeed, it has been one of the tenets of the field that AI\nsystems should be general-purpose—i.e., capable of accepting a purpose as input and\nthen achieving it—rather than special-purpose, with their goal implicit in their design.\nFor example, a self-driving car should accept a destination as input instead of having one\nfixed destination. However, some aspects of the car’s “driving purpose” are fixed, such\nas that it shouldn’t hit pedestrians. This is built directly into the car’s steering algorithms\nrather than being explicit: No self-driving car in existence today “knows” that pedestrians\nprefer not to be run over.\nPutting a purpose into a machine which optimizes its behavior according to clearly\ndefined algorithms seems an admirable approach to ensuring that the machine’s “conduct\nwill be carried out on principles acceptable to us!” But, as Wiener warns, we need to put\nin the right purpose. We might call this the King Midas problem: Midas got exactly what\nhe asked for—namely, that everything he touched would turn to gold—but too late he\ndiscovered the drawbacks of drinking liquid gold and eating solid gold. The technical\nterm for putting in the right purpose is value alignment. When it fails, we may\ninadvertently imbue machines with objectives counter to our own. Tasked with finding a\ncure for cancer as fast as possible, an AI system might elect to use the entire human\npopulation as guinea pigs for its experiments. Asked to de-acidify the oceans, it might\nuse up all the oxygen in the atmosphere as a side effect. This is a common characteristic\nof systems that optimize: Variables not included in the objective may be set to extreme\nvalues to help optimize that objective.\nUnfortunately, neither AI nor other disciplines (economics, statistics, control\ntheory, operations research) built around the optimization of objectives have much to say\nabout how to identify the purposes “we really desire.” Instead, they assume that\nobjectives are simply implanted into the machine. AI research, in its present form,\nstudies the ability to achieve objectives, not the design of those objectives.\nSteve Omohundro has pointed to a further difficulty, observing that intelligent\nentities must act to preserve their own existence. This tendency has nothing to do with a\nself-preservation instinct or any other biological notion; it’s just that an entity cannot\nachieve its objectives if it’s dead. According to Omohundro’s argument, a\nsuperintelligent machine that has an off-switch—which some, including Alan Turing\nhimself, in a 1951 talk on BBC Radio 3, have seen as our potential salvation—will take\nsteps to disable the switch in some way. 1 Thus we may face the prospect of\nsuperintelligent machines—their actions by definition unpredictable by us and their\n1\nOmohundro, “The Basic AI Drives,” in Proc. First AGI Conf., 171: “Artificial General Intelligence,” eds.\nP. Wang, B. Goertzel, & S. Franklin (IOS press, 2008).\n30\nimperfectly specified objectives conflicting with our own—whose motivation to preserve\ntheir existence in order to achieve those objectives may be insuperable.\n1001 Reasons to Pay No Attention\nObjections have been raised to these arguments, primarily by researchers within the AI\ncommunity. The objections reflect a natural defensive reaction, coupled perhaps with a\nlack of imagination about what a superintelligent machine could do. None hold water on\ncloser examination. Here are some of the more common ones:\n• Don’t worry, we can just switch it off. 2 This is often the first thing that pops into a\nlayperson’s head when considering risks from superintelligent AI—as if a\nsuperintelligent entity would never think of that. This is rather like saying that the\nrisk of losing to DeepBlue or AlphaGo is negligible—all one has to do is make\nthe right moves.\n• Human-level or superhuman AI is impossible. 3 This is an unusual claim for AI\nresearchers to make, given that, from Turing onward, they have been fending off\nsuch claims from philosophers and mathematicians. The claim, which is backed\nby no evidence, appears to concede that if superintelligent AI were possible, it\nwould be a significant risk. It’s as if a bus driver, with all of humanity as\npassengers, said, “Yes, I am driving toward a cliff—in fact, I’m pressing the pedal\nto the metal! But trust me, we’ll run out of gas before we get there!” The claim\nrepresents a foolhardy bet against human ingenuity. We have made such bets\nbefore and lost. On September 11, 1933, renowned physicist Ernest Rutherford\nstated, with utter confidence, “Anyone who expects a source of power from the\ntransformation of these atoms is talking moonshine.” On September 12, 1933,\nLeo Szilard invented the neutron-induced nuclear chain reaction. A few years\nlater he demonstrated such a reaction in his laboratory at Columbia University.\nAs he recalled in a memoir: “We switched everything off and went home. That\nnight, there was very little doubt in my mind that the world was headed for grief.”\n• It’s too soon to worry about it. The right time to worry about a potentially serious\nproblem for humanity depends not just on when the problem will occur but also\non how much time is needed to devise and implement a solution that avoids the\nrisk. For example, if we were to detect a large asteroid predicted to collide with\nthe Earth in 2067, would we say, “It’s too soon to worry”? And if we consider\nthe global catastrophic risks from climate change predicted to occur later in this\ncentury, is it too soon to take action to prevent them? On the contrary, it may be\ntoo late. The relevant timescale for human-level AI is less predictable, but, like\nnuclear fission, it might arrive considerably sooner than expected. One variation\non this argument is Andrew Ng’s statement that it’s “like worrying about\noverpopulation on Mars.” This appeals to a convenient analogy: Not only is the\n2\nAI researcher Jeff Hawkins, for example, writes, “Some intelligent machines will be virtual, meaning they\nwill exist and act solely within computer networks. . . . It is always possible to turn off a computer network,\neven if painful.” https://www.recode.net/2015/3/2/11559576/.\n3\nThe AI100 report (Peter Stone et al.), sponsored by Stanford University, includes the following: “Unlike\nin the movies, there is no race of superhuman robots on the horizon or probably even possible.”\nhttps://ai100.stanford.edu/2016-report.\n31\nrisk easily managed and far in the future, but also it’s extremely unlikely that\nwe’d even try to move billions of humans to Mars in the first place. The analogy\nis a false one, however. We are already devoting huge scientific and technical\nresources to creating ever-more-capable AI systems. A more apt analogy would\nbe a plan to move the human race to Mars with no consideration for what we\nmight breathe, drink, or eat once we’d arrived.\n• Human-level AI isn’t really imminent, in any case. The AI100 report, for example,\nassures us, “Contrary to the more fantastic predictions for AI in the popular press,\nthe Study Panel found no cause for concern that AI is an imminent threat to\nhumankind.” This argument simply misstates the reasons for concern, which are\nnot predicated on imminence. In his 2014 book, Superintelligence: Paths,\nDangers, Strategies, Nick Bostrom, for one, writes, “It is no part of the argument\nin this book that we are on the threshold of a big breakthrough in artificial\nintelligence, or that we can predict with any precision when such a development\nmight occur.”\n• You’re just a Luddite. It’s an odd definition of Luddite that includes Turing,\nWiener, Minsky, Musk, and Gates, who rank among the most prominent\ncontributors to technological progress in the 20th and 21st centuries. 4\nFurthermore, the epithet represents a complete misunderstanding of the nature of\nthe concerns raised and the purpose for raising them. It is as if one were to accuse\nnuclear engineers of Luddism if they pointed out the need for control of the\nfission reaction. Some objectors also use the term “anti-AI,” which is rather like\ncalling nuclear engineers “anti-physics.” The purpose of understanding and\npreventing the risks of AI is to ensure that we can realize the benefits. Bostrom,\nfor example, writes that success in controlling AI will result in “a civilizational\ntrajectory that leads to a compassionate and jubilant use of humanity’s cosmic\nendowment”—hardly a pessimistic prediction.\n• Any machine intelligent enough to cause trouble will be intelligent enough to have\nappropriate and altruistic objectives. 5 (Often, the argument adds the premise that\npeople of greater intelligence tend to have more altruistic objectives, a view that\nmay be related to the self-conception of those making the argument.) This\nargument is related to Hume’s is-ought problem and G. E. Moore’s naturalistic\nfallacy, suggesting that somehow the machine, as a result of its intelligence, will\nsimply perceive what is right, given its experience of the world. This is\nimplausible; for example, one cannot perceive, in the design of a chessboard and\nchess pieces, the goal of checkmate; the same chessboard and pieces can be used\nfor suicide chess, or indeed many other games still to be invented. Put another\nway: Where Bostrom imagines humans driven extinct by a putative robot that\nturns the planet into a sea of paper clips, we humans see this outcome as tragic,\n4\nElon Musk, Stephen Hawking, and others (including, apparently, the author) received the 2015 Luddite of\nthe Year Award from the Information Technology Innovation Foundation:\nhttps://itif.org/publications/2016/01/19/artificial-intelligence-alarmists-win-itif%E2%80%99s-annualluddite-award.\n5\nRodney Brooks, for example, asserts that it’s impossible for a program to be “smart enough that it would\nbe able to invent ways to subvert human society to achieve goals set for it by humans, without\nunderstanding the ways in which it was causing problems for those same humans.”\nhttp://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/.\n32\nwhereas the iron-eating bacterium Thiobacillus ferrooxidans is thrilled. Who’s to\nsay the bacterium is wrong? The fact that a machine has been given a fixed\nobjective by humans doesn’t mean that it will automatically recognize the\nimportance to humans of things that aren’t part of the objective. Maximizing the\nobjective may well cause problems for humans, but, by definition, the machine\nwill not recognize those problems as problematic.\n• Intelligence is multidimensional, “so ‘smarter than humans’ is a meaningless\nconcept.” 6 It is a staple of modern psychology that IQ doesn’t do justice to the\nfull range of cognitive skills that humans possess to varying degrees. IQ is indeed\na crude measure of human intelligence, but it is utterly meaningless for current AI\nsystems, because their capabilities across different areas are uncorrelated. How\ndo we compare the IQ of Google’s search engine, which cannot play chess, with\nthat of DeepBlue, which cannot answer search queries?\nNone of this supports the argument that because intelligence is multifaceted,\nwe can ignore the risk from superintelligent machines. If “smarter than humans”\nis a meaningless concept, then “smarter than gorillas” is also meaningless, and\ngorillas therefore have nothing to fear from humans; clearly, that argument\ndoesn’t hold water. Not only is it logically possible for one entity to be more\ncapable than another across all the relevant dimensions of intelligence, it is also\npossible for one species to represent an existential threat to another even if the\nformer lacks an appreciation for music and literature.\nSolutions\nCan we tackle Wiener’s warning head-on? Can we design AI systems whose purposes\ndon’t conflict with ours, so that we’re sure to be happy with how they behave? On the\nface of it, this seems hopeless, because it will doubtless prove infeasible to write down\nour purposes correctly or imagine all the counterintuitive ways a superintelligent entity\nmight fulfill them.\nIf we treat superintelligent AI systems as if they were black boxes from outer\nspace, then indeed we have no hope. Instead, the approach we seem obliged to take, if\nwe are to have any confidence in the outcome, is to define some formal problem F, and\ndesign AI systems to be F-solvers, such that no matter how perfectly a system solves F,\nwe’re guaranteed to be happy with the solution. If we can work out an appropriate F that\nhas this property, we’ll be able to create provably beneficial AI.\nHere’s an example of how not to do it: Let a reward be a scalar value provided\nperiodically by a human to the machine, corresponding to how well the machine has\nbehaved during each period, and let F be the problem of maximizing the expected sum of\nrewards obtained by the machine. The optimal solution to this problem is not, as one\nmight hope, to behave well, but instead to take control of the human and force him or her\nto provide a stream of maximal rewards. This is known as the wireheading problem,\nbased on observations that humans themselves are susceptible to the same problem if\ngiven a means to electronically stimulate their own pleasure centers.\nThere is, I believe, an approach that may work. Humans can reasonably be\ndescribed as having (mostly implicit) preferences over their future lives—that is, given\n6\nKevin Kelly, “The Myth of a Superhuman AI,” Wired, Apr. 25, 2017.\n33\nenough time and unlimited visual aids, a human could express a preference (or\nindifference) when offered a choice between two future lives laid out before him or her in\nall their aspects. (This idealization ignores the possibility that our minds are composed of\nsubsystems with incompatible preferences; if true, that would limit a machine’s ability to\noptimally satisfy our preferences, but it doesn’t seem to prevent us from designing\nmachines that avoid catastrophic outcomes.) The formal problem F to be solved by the\nmachine in this case is to maximize human future-life preferences subject to its initial\nuncertainty as to what they are. Furthermore, although the future-life preferences are\nhidden variables, they’re grounded in a voluminous source of evidence—namely, all of\nthe human choices ever made. This formulation sidesteps Wiener’s problem: The\nmachine may learn more about human preferences as it goes along, of course, but it will\nnever achieve complete certainty.\nA more precise definition is given by the framework of cooperative inversereinforcement\nlearning, or CIRL. A CIRL problem involves two agents, one human and\nthe other a robot. Because there are two agents, the problem is what economists call a\ngame. It is a game of partial information, because while the human knows the reward\nfunction, the robot doesn’t—even though the robot’s job is to maximize it.\nA simple example: Suppose that Harriet, the human, likes to collect paper\nclips and staples and her reward function depends on how many of each she has. More\nprecisely, if she has p paper clips and s staples, her degree of happiness is θp + (1-θ)s,\nwhere θ is essentially an exchange rate between paper clips and staples. If θ is 1, she\nlikes only paper clips; if θ is 0, she likes only staples; if θ is 0.5, she is indifferent\nbetween them; and so on. It’s the job of Robby, the robot, to produce the paper clips and\nstaples. The point of the game is that Robby wants to make Harriet happy, but he doesn’t\nknow the value of θ, so he isn’t sure how many of each to produce.\nHere’s how the game works. Let the true value of θ be 0.49—that is, Harriet\nhas a slight preference for staples over paper clips. And let’s assume that Robby has a\nuniform prior belief about θ—that is, he believes θ is equally likely to be any value\nbetween 0 and 1. Harriet now gets to do a small demonstration, producing either two\npaper clips or two staples or one of each. After that, the robot can produce either ninety\npaper clips, or ninety staples, or fifty of each. You might think that Harriet, who prefers\nstaples to paper clips, should produce two staples. But in that case, Robby’s rational\nresponse would be to produce ninety staples (with a total value to Harriet of 45.9), which\nis a less desirable outcome for Harriet than fifty of each (total value 50.0). The optimal\nsolution of this particular game is that Harriet produces one of each, so then Robby\nmakes fifty of each. Thus, the way the game is defined encourages Harriet to “teach”\nRobby—as long as she knows that Robby is watching carefully.\nWithin the CIRL framework, one can formulate and solve the off-switch\nproblem—that is, the problem of how to prevent a robot from disabling its off-switch.\n(Turing may rest easier.) A robot that’s uncertain about human preferences actually\nbenefits from being switched off, because it understands that the human will press the\noff-switch to prevent the robot from doing something counter to those preferences. Thus\nthe robot is incentivized to preserve the off-switch, and this incentive derives directly\nfrom its uncertainty about human preferences. 7\nThe off-switch example suggests some templates for controllable-agent\n7\nSee Hadfield-Menell et al., “The Off-Switch Game,” https://arxiv.org/pdf/1611.08219.pdf.\n34\ndesigns and provides at least one case of a provably beneficial system in the sense\nintroduced above. The overall approach resembles mechanism-design problems in\neconomics, wherein one incentivizes other agents to behave in ways beneficial to the\ndesigner. The key difference here is that we are building one of the agents in order to\nbenefit the other.\nThere are reasons to think this approach may work in practice. First, there is\nabundant written and filmed information about humans doing things (and other humans\nreacting). Technology to build models of human preferences from this storehouse will\npresumably be available long before superintelligent AI systems are created. Second,\nthere are strong, near-term economic incentives for robots to understand human\npreferences: If one poorly designed domestic robot cooks the cat for dinner, not realizing\nthat its sentimental value outweighs its nutritional value, the domestic-robot industry will\nbe out of business.\nThere are obvious difficulties, however, with an approach that expects a robot\nto learn underlying preferences from human behavior. Humans are irrational,\ninconsistent, weak-willed, and computationally limited, so their actions don’t always\nreflect their true preferences. (Consider, for example, two humans playing chess.\nUsually, one of them loses, but not on purpose!) So robots can learn from nonrational\nhuman behavior only with the aid of much better cognitive models of humans.\nFurthermore, practical and social constraints will prevent all preferences from being\nmaximally satisfied simultaneously, which means that robots must mediate among\nconflicting preferences—something that philosophers and social scientists have struggled\nwith for millennia. And what should robots learn from humans who enjoy the suffering\nof others? It may be best to zero out such preferences in the robots’ calculations.\nFinding a solution to the AI control problem is an important task; it may be,\nin Bostrom’s words, “the essential task of our age.” Up to now, AI research has focused\non systems that are better at making decisions, but this is not the same as making better\ndecisions. No matter how excellently an algorithm maximizes, and no matter how\naccurate its model of the world, a machine’s decisions may be ineffably stupid in the eyes\nof an ordinary human if its utility function is not well aligned with human values.\nThis problem requires a change in the definition of AI itself—from a field\nconcerned with pure intelligence, independent of the objective, to a field concerned with\nsystems that are provably beneficial for humans. Taking the problem seriously seems\nlikely to yield new ways of thinking about AI, its purpose, and our relationship to it.\n35\nIn 2005, George Dyson, a historian of science and technology, visited Google at the\ninvitation of some Google engineers. The occasion was the sixtieth anniversary of John\nvon Neumann’s proposal for a digital computer. After the visit, George wrote an essay,\n“Turing’s Cathedral,” which, for the first time, alerted the public about what Google’s\nfounders had in store for the world. “We are not scanning all those books to be read by\npeople,” explained one of his hosts after his talk. “We are scanning them to be read by\nan AI.”\nGeorge offers a counternarrative to the digital age. His interests have included\nthe development of the Aleut kayak, the evolution of digital computing and\ntelecommunications, the origins of the digital universe, and a path not taken into space.\nHis career (he never finished high school, yet has been awarded an honorary doctorate\nfrom the University of Victoria) has proved as impossible to classify as his books.\nHe likes to point out that analog computing, once believed to be as extinct as the\ndifferential analyzer, has returned. He argues that while we may use digital components,\nat a certain point the analog computing being performed by the system far exceeds the\ncomplexity of the digital code with which it is built. He believes that true artificial\nintelligence—with analog control systems emerging from a digital substrate the way\ndigital computers emerged out of analog components in the aftermath of World War II—\nmay not be as far off as we think.\nIn this essay, George contemplates the distinction between analog and digital\ncomputation and finds analog to be alive and well. Nature’s response to an attempt to\nprogram machines to control everything may be machines without programming over\nwhich no one has control.\n36\nTHE THIRD LAW\nGeorge Dyson\nGeorge Dyson is a historian of science and technology and the author of Baidarka: the\nKayak, Darwin Among the Machines, Project Orion, and Turing’s Cathedral.\nThe history of computing can be divided into an Old Testament and a New Testament:\nbefore and after electronic digital computers and the codes they spawned proliferated\nacross the Earth. The Old Testament prophets, who delivered the underlying logic,\nincluded Thomas Hobbes and Gottfried Wilhelm Leibniz. The New Testament prophets\nincluded Alan Turing, John von Neumann, Claude Shannon, and Norbert Wiener. They\ndelivered the machines.\nAlan Turing wondered what it would take for machines to become intelligent.\nJohn von Neumann wondered what it would take for machines to self-reproduce. Claude\nShannon wondered what it would take for machines to communicate reliably, no matter\nhow much noise intervened. Norbert Wiener wondered how long it would take for\nmachines to assume control.\nWiener’s warnings about control systems beyond human control appeared in\n1949, just as the first generation of stored-program electronic digital computers were\nintroduced. These systems required direct supervision by human programmers,\nundermining his concerns. What’s the problem, as long as programmers are in control of\nthe machines? Ever since, debate over the risks of autonomous control has remained\nassociated with the debate over the powers and limitations of digitally coded machines.\nDespite their astonishing powers, little real autonomy has been observed. This is a\ndangerous assumption. What if digital computing is being superseded by something\nelse?\nElectronics underwent two fundamental transitions over the past hundred years:\nfrom analog to digital and from vacuum tubes to solid state. That these transitions\noccurred together does not mean they are inextricably linked. Just as digital computation\nwas implemented using vacuum tube components, analog computation can be\nimplemented in solid state. Analog computation is alive and well, even though vacuum\ntubes are commercially extinct.\nThere is no precise distinction between analog and digital computing. In general,\ndigital computing deals with integers, binary sequences, deterministic logic, and time that\nis idealized into discrete increments, whereas analog computing deals with real numbers,\nnondeterministic logic, and continuous functions, including time as it exists as a\ncontinuum in the real world.\nImagine you need to find the middle of a road. You can measure its width using\nany available increment and then digitally compute the middle to the nearest increment.\nOr you can use a piece of string as an analog computer, mapping the width of the road to\nthe length of the string and finding the middle, without being limited to increments, by\ndoubling the string back upon itself.\nMany systems operate across both analog and digital regimes. A tree integrates a\nwide range of inputs as continuous functions, but if you cut down that tree, you find that\nit has been counting the years digitally all along.\n37\nIn analog computing, complexity resides in network topology, not in code.\nInformation is processed as continuous functions of values such as voltage and relative\npulse frequency rather than by logical operations on discrete strings of bits. Digital\ncomputing, intolerant of error or ambiguity, depends upon error correction at every step\nalong the way. Analog computing tolerates errors, allowing you to live with them.\nNature uses digital coding for the storage, replication, and recombination of\nsequences of nucleotides, but relies on analog computing, running on nervous systems,\nfor intelligence and control. The genetic system in every living cell is a stored-program\ncomputer. Brains aren’t.\nDigital computers execute transformations between two species of bits: bits\nrepresenting differences in space and bits representing differences in time. The\ntransformations between these two forms of information, sequence and structure, are\ngoverned by the computer’s programming, and as long as computers require human\nprogrammers, we retain control.\nAnalog computers also mediate transformations between two forms of\ninformation: structure in space and behavior in time. There is no code and no\nprogramming. Somehow—and we don’t fully understand how—Nature evolved analog\ncomputers known as nervous systems, which embody information absorbed from the\nworld. They learn. One of the things they learn is control. They learn to control their\nown behavior, and they learn to control their environment to the extent that they can.\nComputer science has a long history—going back to before there even was\ncomputer science—of implementing neural networks, but for the most part these have\nbeen simulations of neural networks by digital computers, not neural networks as evolved\nin the wild by Nature herself. This is starting to change: from the bottom up, as the\nthreefold drivers of drone warfare, autonomous vehicles, and cell phones push the\ndevelopment of neuromorphic microprocessors that implement actual neural networks,\nrather than simulations of neural networks, directly in silicon (and other potential\nsubstrates); and from the top down, as our largest and most successful enterprises\nincreasingly turn to analog computation in their infiltration and control of the world.\nWhile we argue about the intelligence of digital computers, analog computing is\nquietly supervening upon the digital, in the same way that analog components like\nvacuum tubes were repurposed to build digital computers in the aftermath of World War\nII. Individually deterministic finite-state processors, running finite codes, are forming\nlarge-scale, nondeterministic, non-finite-state metazoan organisms running wild in the\nreal world. The resulting hybrid analog/digital systems treat streams of bits collectively,\nthe way the flow of electrons is treated in a vacuum tube, rather than individually, as bits\nare treated by the discrete-state devices generating the flow. Bits are the new electrons.\nAnalog is back, and its nature is to assume control.\nGoverning everything from the flow of goods to the flow of traffic to the flow of\nideas, these systems operate statistically, as pulse-frequency coded information is\nprocessed in a neuron or a brain. The emergence of intelligence gets the attention of\nHomo sapiens, but what we should be worried about is the emergence of control.\n~ ~ ~\n38\nImagine it is 1958 and you are trying to defend the continental United States against\nairborne attack. To distinguish hostile aircraft, one of the things you need, besides a\nnetwork of computers and early-warning radar sites, is a map of all commercial air\ntraffic, updated in real time. The United States built such a system and named it SAGE\n(Semi-Automatic Ground Environment). SAGE in turn spawned Sabre, the first\nintegrated reservation system for booking airline travel in real time. Sabre and its\nprogeny soon became not just a map as to what seats were available but also a system\nthat began to control, with decentralized intelligence, where airliners would fly, and\nwhen.\nBut isn’t there a control room somewhere, with someone at the controls? Maybe\nnot. Say, for example, you build a system to map highway traffic in real time, simply by\ngiving cars access to the map in exchange for reporting their own speed and location at\nthe time. The result is a fully decentralized control system. Nowhere is there any\ncontrolling model of the system except the system itself.\nImagine it is the first decade of the 21st century and you want to track the\ncomplexity of human relationships in real time. For social life at a small college, you\ncould construct a central database and keep it up to date, but its upkeep would become\noverwhelming if taken to any larger scale. Better to pass out free copies of a simple\nsemi-autonomous code, hosted locally, and let the social network update itself. This code\nis executed by digital computers, but the analog computing performed by the system as a\nwhole far exceeds the complexity of the underlying code. The resulting pulse-frequency\ncoded model of the social graph becomes the social graph. It spreads wildly across the\ncampus and then the world.\nWhat if you wanted to build a machine to capture what everything known to the\nhuman species means? With Moore’s Law behind you, it doesn’t take too long to digitize\nall the information in the world. You scan every book ever printed, collect every email\never written, and gather forty-nine years of video every twenty-four hours, while tracking\nwhere people are and what they do, in real time. But how do you capture the meaning?\nEven in the age of all things digital, this cannot be defined in any strictly logical\nsense, because meaning, among humans, isn’t fundamentally logical. The best you can\ndo, once you have collected all possible answers, is to invite well-defined questions and\ncompile a pulse-frequency weighted map of how everything connects. Before you know\nit, your system will not only be observing and mapping the meaning of things, it will start\nconstructing meaning as well. In time, it will control meaning, in the same way as the\ntraffic map starts to control the flow of traffic even though no one seems to be in control.\n~ ~ ~\nThere are three laws of artificial intelligence. The first, known as Ashby’s Law, after\ncybernetician W. Ross Ashby, author of Design for a Brain, states that any effective\ncontrol system must be as complex as the system it controls.\nThe second law, articulated by John von Neumann, states that the defining\ncharacteristic of a complex system is that it constitutes its own simplest behavioral\ndescription. The simplest complete model of an organism is the organism itself. Trying\nto reduce the system’s behavior to any formal description makes things more\ncomplicated, not less.\n39\nThe third law states that any system simple enough to be understandable will not\nbe complicated enough to behave intelligently, while any system complicated enough to\nbehave intelligently will be too complicated to understand.\nThe Third Law offers comfort to those who believe that until we understand\nintelligence, we need not worry about superhuman intelligence arising among machines.\nBut there is a loophole in the Third Law. It is entirely possible to build something\nwithout understanding it. You don’t need to fully understand how a brain works in order\nto build one that works. This is a loophole that no amount of supervision over algorithms\nby programmers and their ethical advisors can ever close. Provably “good” AI is a myth.\nOur relationship with true AI will always be a matter of faith, not proof.\nWe worry too much about machine intelligence and not enough about selfreproduction,\ncommunication, and control. The next revolution in computing will be\nsignaled by the rise of analog systems over which digital programming no longer has\ncontrol. Nature’s response to those who believe they can build machines to control\neverything will be to allow them to build a machine that controls them instead.\n40\nDan Dennett is the philosopher of choice in the AI community. He is perhaps best\nknown in cognitive science for his concept of intentional systems and his model of human\nconsciousness, which sketches a computational architecture for realizing the stream of\nconsciousness in the massively parallel cerebral cortex. That uncompromising\ncomputationalism has been opposed by philosophers such as John Searle, David\nChalmers, and the late Jerry Fodor, who have protested that the most important aspects\nof consciousness—intentionality and subjective qualia—cannot be computed.\nTwenty-five years ago, I was visiting Marvin Minsky, one of the original AI\npioneers, and asked him about Dan. “He’s our best current philosopher—the next\nBertrand Russell,” said Marvin, adding that unlike traditional philosophers, Dan was a\nstudent of neuroscience, linguistics, artificial intelligence, computer science, and\npsychology: “He’s redefining and reforming the role of the philosopher. Of course, Dan\ndoesn’t understand my Society-of-Mind theory, but nobody’s perfect.”\nDan’s view of the efforts of AI researchers to create superintelligent AIs is\nrelentlessly levelheaded. What, me worry? In this essay, he reminds us that AIs, above\nall, should be regarded—and treated—as tools and not as humanoid colleagues.\nHe has been interested in information theory since his graduate school days at\nOxford. In fact, he told me that early in his career he was keenly interested in writing a\nbook about Wiener’s cybernetic ideas. As a thinker who embraces the scientific method,\none of his charms is his willingness to be wrong. Of a recent piece entitled “What Is\nInformation?” he has announced, “I stand by it, but it’s under revision. I’m already\nmoving beyond it and realizing there’s a better way of tackling some of these issues.” He\nwill most likely remain cool and collected on the subject of AI research, although he has\nacknowledged, often, that his own ideas evolve—as anyone’s ideas should.\n41\nWHAT CAN WE DO?\nDaniel C. Dennett\nDaniel C. Dennett is University Professor and Austin B. Fletcher Professor of\nPhilosophy and director of the Center for Cognitive Studies at Tufts University. He is the\nauthor of a dozen books, including Consciousness Explained and, most recently, From\nBacteria to Bach and Back: The Evolution of Minds.\nMany have reflected on the irony of reading a great book when you are too young to\nappreciate it. Consigning a classic to the already read stack and thereby insulating\nyourself against any further influence while gleaning only a few ill-understood ideas from\nit is a recipe for neglect that is seldom benign. This struck me with particular force when\nI reread The Human Use of Human Beings more than sixty years after my juvenile\nencounter. We should all make it a regular practice to reread books from our youth,\nwhere we are apt to discover clear previews of some of our own later “discoveries” and\n“inventions,” along with a wealth of insights to which we were bound to be impervious\nuntil our minds had been torn and tattered, exercised and enlarged by confrontations with\nlife’s problems.\nWriting at a time when vacuum tubes were still the primary electronic building\nblocks and there were only a few actual computers in operation, Norbert Wiener\nimagined the future we now contend with in impressive detail and with few clear\nmistakes. Alan Turing’s famous 1950 article “Computing Machinery and Intelligence,”\nin the philosophy journal Mind, foresaw the development of AI, and so did Wiener, but\nWiener saw farther and deeper, recognizing that AI would not just imitate—and\nreplace—human beings in many intelligent activities but change human beings in the\nprocess.\nWe are but whirlpools in a river of ever-flowing water. We are not stuff that abides, but\npatterns that perpetuate themselves. (p. 96)\nWhen that was written, it could be comfortably dismissed as yet another bit of\nHeraclitean overstatement. Yeah, yeah, you can never step in the same river twice. But\nit contains the seeds of the revolution in outlook. Today we know how to think about\ncomplex adaptive systems, strange attractors, extended minds, and homeostasis, a change\nin perspective that promises to erase the “explanatory gap” 8 between mind and\nmechanism, spirit and matter, a gap that is still ardently defended by latter-day Cartesians\nwho cannot bear the thought that we—we ourselves—are self-perpetuating patterns of\ninformation-bearing matter, not “stuff that abides.” Those patterns are remarkably\nresilient and self-restoring but at the same time protean, opportunistic, selfish exploiters\nof whatever new is available to harness in their quest for perpetuation. And here is where\nthings get dicey, as Wiener recognized. When attractive opportunities abound, we are apt\nto be willing to pay a little and accept some small, even trivial, cost-of-doing-business for\naccess to new powers. And pretty soon we become so dependent on our new tools that\nwe lose the ability to thrive without them. Options become obligatory.\n8\nJoseph Levine, “Materialism and Qualia: The Explanatory Gap,” Pacific Philosophical Quarterly 64, pp.\n354-61 (1983).\n42\nIt’s an old, old story, with many well-known chapters in evolutionary history.\nMost mammals can synthesize their own vitamin C, but primates, having opted for a diet\ncomposed largely of fruit, lost the innate ability. We are now obligate ingesters of\nvitamin C, but not obligate frugivores like our primate cousins, since we have opted for\ntechnology that allows us to make, and take, vitamins as needed. The self-perpetuating\npatterns that we call human beings are now dependent on clothes, cooked food, vitamins,\nvaccinations, . . . credit cards, smartphones, and the Internet. And—tomorrow if not\nalready today—AI.\nWiener foresaw the problems that Turing and the other optimists have largely\noverlooked. The real danger, he said, is\nthat such machines, though helpless by themselves, may be used by a human being or a\nblock of human beings to increase their control over the rest of the race or that political\nleaders may attempt to control their populations by means not of machines themselves\nbut through political techniques as narrow and indifferent to human possibility as if they\nhad, in fact, been conceived mechanically. (p. 181)\nThe power, he recognized, lay primarily in the algorithms, not the hardware they run on,\nalthough the hardware of today makes practically possible algorithms that would have\nseemed preposterously cumbersome in Wiener’s day. What can we say about these\n“techniques” that are “narrow and indifferent to human possibility”? They have been\nintroduced again and again, some obviously benign, some obviously dangerous, and\nmany in the omnipresent middle ground of controversy.\nConsider a few of the skirmishes. My late friend Joe Weizenbaum, Wiener’s\nsuccessor as MIT’s Jeremiah of hi-tech, loved to observe that credit cards, whatever their\nvirtues, also provided an inexpensive and almost foolproof way for the government, or\ncorporations, to track the travels and habits and desires of individuals. The anonymity of\ncash has been largely underappreciated, except by drug dealers and other criminals, and\nnow it may be going extinct. This may make money laundering a more difficult technical\nchallenge in the future, but the AI pattern finders arrayed against it have the side effect of\nmaking us all more transparent to any “block of human beings” that may “attempt to\ncontrol” us.\nLooking to the arts, the innovation of digital audio and video recording lets us pay\na small price (in the eyes of all but the most ardent audiophiles and film lovers) when we\nabandon analog formats, and in return provides easy—all too easy?—reproduction of\nartworks with almost perfect fidelity. But there is a huge hidden cost. Orwell’s Ministry\nof Truth is now a practical possibility. AI techniques for creating all-but-undetectable\nforgeries of “recordings” of encounters are now becoming available which will render\nobsolete the tools of investigation we have come to take for granted in the last hundred\nand fifty years. Will we simply abandon the brief Age of Photographic Evidence and\nreturn to the earlier world in which human memory and trust provided the gold standard,\nor will we develop new techniques of defense and offense in the arms race of truth? (We\ncan imagine a return to analog film-exposed-to-light, kept in “tamper-proof” systems\nuntil shown to juries, etc., but how long would it be before somebody figured out a way\nto infect such systems with doubt? One of the disturbing lessons of recent experience is\nthat the task of destroying a reputation for credibility is much less expensive than the task\nof protecting such a reputation.) Wiener saw the phenomenon at its most general: “…in\n43\nthe long run, there is no distinction between arming ourselves and arming our enemies.”\n(p. 129) The Information Age is also the Dysinformation Age.\nWhat can we do? We need to rethink our priorities with the help of the passionate\nbut flawed analyses of Wiener, Weizenbaum, and the other serious critics of our\ntechnophilia. A key phrase, it seems to me, is Wiener’s almost offhand observation,\nabove, that “these machines” are “helpless by themselves.” As I have been arguing\nrecently, we’re making tools, not colleagues, and the great danger is not appreciating the\ndifference, which we should strive to accentuate, marking and defending it with political\nand legal innovations.\nPerhaps the best way to see what is being missed is to note that Alan Turing\nhimself suffered an entirely understandable failure of imagination in his formulation of\nthe famous Turing Test. As everyone knows, it is an adaptation of his “imitation game,”\nin which a man, hidden from view and communicating verbally with a judge, tries to\nconvince the judge that he is in fact a woman, while a woman, also hidden and\ncommunicating with the judge, tries to convince the judge that she is the woman. Turing\nreasoned that this would be a demanding challenge for a man (or for a woman pretending\nto be a man), exploiting a wealth of knowledge about how the other sex thinks and acts,\nwhat they tend to favor or ignore. Surely (ding!) 9 , any man who could beat a woman at\nbeing perceived to be a woman would be an intelligent agent. What Turing did not\nforesee is the power of deep-learning AI to acquire this wealth of information in an\nexploitable form without having to understand it. Turing imagined an astute and\nimaginative (and hence conscious) agent who cunningly designed his responses based on\nhis detailed “theory” of what women are likely to do and say. Top-down intelligent\ndesign, in short. He certainly didn’t think that a man, winning the imitation game, would\nsomehow become a woman; he imagined that there would still be a man’s consciousness\nguiding the show. The hidden premise in Turing’s almost-argument was: Only a\nconscious, intelligent agent could devise and control a winning strategy in the imitation\ngame. And so it was persuasive to Turing (and others, including me, still a stalwart\ndefender of the Turing Test) to argue that a “computing machine” that could pass as\nhuman in a contest with a human might not be conscious in just the way a human being\nis, but would nevertheless have to be a conscious agent of some kind. I think this is still a\ndefensible position—the only defensible position—but you have to understand how\nresourceful and ingenious a judge would have to be to expose the shallowness of the\nfaçade that a deep-learning AI (a tool, not a colleague) could present.\nWhat Turing didn’t foresee is the uncanny ability of superfast computers to sift\nmindlessly through Big Data, of which the Internet provides an inexhaustible supply,\nfinding probabilistic patterns in human activity that could be used to pop “authentic”-\nseeming responses into the output for almost any probe a judge would think to offer.\nWiener also underestimates this possibility, seeing the tell-tale weakness of a machine in\nnot being able to\ntake into account the vast range of probability that characterizes the human\nsituation. (p.181)\n9\nThe surely alarm (the habit of having a bell ring in your head whenever you see the word in an argument)\nis described and defended by me in Intuition Pumps and Other Tools for Thinking (2013).\n44\nBut taking into account that range of probability is just where the new AI excels.\nThe only chink in the armor of AI is that word “vast”; human possibilities, thanks to\nlanguage and the culture that it spawns, are truly Vast. 10 No matter how many patterns\nwe may find with AI in the flood of data that has so far found its way onto the Internet,\nthere are Vastly more possibilities that have never been recorded there. Only a fraction\n(but not a Vanishing fraction) of the world’s accumulated wisdom and design and\nrepartee and silliness has made it onto the Internet, but probably a better tactic for the\njudge to adopt when confronting a candidate in the Turing Test is not to search for such\nitems but to create them anew. AI in its current manifestations is parasitic on human\nintelligence. It quite indiscriminately gorges on whatever has been produced by human\ncreators and extracts the patterns to be found there—including some of our most\npernicious habits. 11 These machines do not (yet) have the goals or strategies or capacities\nfor self-criticism and innovation to permit them to transcend their databases by\nreflectively thinking about their own thinking and their own goals. They are, as Wiener\nsays, helpless, not in the sense of being shackled agents or disabled agents but in the\nsense of not being agents at all—not having the capacity to be “moved by reasons” (as\nKant put it) presented to them. It is important that we keep it that way, which will take\nsome doing.\nOne of the flaws in Weizenbaum’s book Computer Power and Human Reason,\nsomething I tried in vain to convince him of in many hours of discussion, is that he could\nnever decide which of two theses he wanted to defend: AI is impossible! or AI is possible\nbut evil! He wanted to argue, with John Searle and Roger Penrose, that “Strong AI” is\nimpossible, but there are no good arguments for that conclusion. After all, everything we\nnow know suggests that, as I have put it, we are robots made of robots made of robots. . .\ndown to the motor proteins and their ilk, with no magical ingredients thrown in along the\nway. Weizenbaum’s more important and defensible message was that we should not\nstrive to create Strong AI and should be extremely cautious about the AI systems that we\ncan create and have already created. As one might expect, the defensible thesis is a\nhybrid: AI (Strong AI) is possible in principle but not desirable. The AI that’s practically\npossible is not necessarily evil—unless it is mistaken for Strong AI!\nThe gap between today’s systems and the science-fictional systems dominating\nthe popular imagination is still huge, though many folks, both lay and expert, manage to\nunderestimate it. Let’s consider IBM’s Watson, which can stand as a worthy landmark\nfor our imaginations for the time being. It is the result of a very large-scale R&D process\nextending over many person-centuries of intelligent design, and as George Church notes\nin these pages, it uses thousands of times more energy than a human brain (a\ntechnological limitation that, as he also notes, may be temporary). Its victory in\nJeopardy! was a genuine triumph, made possible by the formulaic restrictions of the\nJeopardy! rules, but in order for it to compete, even these rules had to be revised (one of\n10\nIn Darwin’s Dangerous Idea, 1995, p. 109, I coined the capitalized version, Vast, meaning Very much\nmore than ASTronomical, and its complement, Vanishing, to replace the usual exaggerations infinite and\ninfinitesimal for discussions of those possibilities that are not officially infinite but nevertheless infinite for\nall practical purposes.\n11\nAylin Caliskan-Islam, Joanna J. Bryson & Arvind Narayanan, “Semantics derived automatically from\nlanguage corpora contain human-like biases,” Science, 14 April 2017, 356: 6334, pp. 183-6. DOI:\n10.1126/science.aal4230.\n45\nthose trade-offs: you give up a little versatility, a little humanity, and get a crowdpleasing\nshow). Watson is not good company, in spite of misleading ads from IBM that\nsuggest a general conversational ability, and turning Watson into a plausibly\nmultidimensional agent would be like turning a hand calculator into Watson. Watson\ncould be a useful core faculty for such an agent, but more like a cerebellum or an\namygdala than a mind—at best, a special-purpose subsystem that could play a big\nsupporting role, but not remotely up to the task of framing purposes and plans and\nbuilding insightfully on its conversational experiences.\nWhy would we want to create a thinking, creative agent out of Watson? Perhaps\nTuring’s brilliant idea of an operational test has lured us into a trap: the quest to create at\nleast the illusion of a real person behind the screen, bridging the “uncanny valley.” The\ndanger, here, is that ever since Turing posed his challenge—which was, after all, a\nchallenge to fool the judges—AI creators have attempted to paper over the valley with\ncutesy humanoid touches, Disneyfication effects that will enchant and disarm the\nuninitiated. Weizenbaum’s ELIZA was the pioneer example of such superficial illusionmaking,\nand it was his dismay at the ease with which his laughably simple and shallow\nprogram could persuade people they were having a serious heart-to-heart conversation\nthat first sent him on his mission.\nHe was right to be worried. If there is one thing we have learned from the\nrestricted Turing Test competitions for the Loebner Prize, it is that even very intelligent\npeople who aren’t tuned in to the possibilities and shortcuts of computer programming\nare readily taken in by simple tricks. The attitudes of people in AI toward these methods\nof dissembling at the “user interface” have ranged from contempt to celebration, with a\ngeneral appreciation that the tricks are not deep but can be potent. One shift in attitude\nthat would be very welcome is a candid acknowledgment that humanoid embellishments\nare false advertising—something to condemn, not applaud.\nHow could that be accomplished? Once we recognize that people are starting to\nmake life-or-death decisions largely on the basis of “advice” from AI systems whose\ninner operations are unfathomable in practice, we can see a good reason why those who\nin any way encourage people to put more trust in these systems than they warrant should\nbe held morally and legally accountable. AI systems are very powerful tools—so\npowerful that even experts will have good reason not to trust their own judgment over the\n“judgments” delivered by their tools. But then, if these tool users are going to benefit,\nfinancially or otherwise, from driving these tools through terra incognita, they need to\nmake sure they know how to do this responsibly, with maximum control and justification.\nLicensing and bonding operators, just as we license pharmacists (and crane operators!)\nand other specialists whose errors and misjudgments can have dire consequences, can,\nwith pressure from insurance companies and other underwriters, oblige creators of AI\nsystems to go to extraordinary lengths to search for and reveal weaknesses and gaps in\ntheir products, and to train those entitled to operate them.\nOne can imagine a sort of inverted Turing Test in which the judge is on trial; until\nhe or she can spot the weaknesses, the overstepped boundaries, the gaps in a system, no\nlicense to operate will be issued. The mental training required to achieve certification as\na judge will be demanding. The urge to adopt the intentional stance, our normal tactic\nwhenever we encounter what seems to be an intelligent agent, is almost overpoweringly\nstrong. Indeed, the capacity to resist the allure of treating an apparent person as a person\n46\nis an ugly talent, reeking of racism or species-ism. Many people would find the\ncultivation of such a ruthlessly skeptical approach morally repugnant, and we can\nanticipate that even the most proficient system-users would occasionally succumb to the\ntemptation to “befriend” their tools, if only to assuage their discomfort with the execution\nof their duties. No matter how scrupulously the AI designers launder the phony “human”\ntouches out of their wares, we can expect novel habits of thought, conversational gambits\nand ruses, traps and bluffs to arise in this novel setting for human action. The comically\nlong lists of known side effects of new drugs advertised on television will be dwarfed by\nthe obligatory revelations of the sorts of questions that cannot be responsibly answered\nby particular systems, with heavy penalties for those who “overlook” flaws in their\nproducts. It is widely noted that a considerable part of the growing economic inequality\nin today’s world is due to the wealth accumulated by digital entrepreneurs; we should\nenact legislation that puts their deep pockets in escrow for the public good. Some of the\ndeepest pockets are voluntarily out in front of these obligations to serve society first and\nmake money secondarily, but we shouldn’t rely on good will alone.\nWe don’t need artificial conscious agents. There is a surfeit of natural conscious\nagents, enough to handle whatever tasks should be reserved for such special and\nprivileged entities. We need intelligent tools. Tools do not have rights, and should not\nhave feelings that could be hurt, or be able to respond with resentment to “abuses” rained\non them by inept users. 12 One of the reasons for not making artificial conscious agents is\nthat however autonomous they might become (and in principle, they can be as\nautonomous, as self-enhancing or self-creating, as any person), they would not—without\nspecial provision, which might be waived—share with us natural conscious agents our\nvulnerability or our mortality.\nI once posed a challenge to students in a seminar at Tufts I co-taught with\nMatthias Scheutz on artificial agents and autonomy: Give me the specs for a robot that\ncould sign a binding contract with you—not as a surrogate for some human owner but on\nits own. This isn’t a question of getting it to understand the clauses or manipulate a pen\non a piece of paper but of having and deserving legal status as a morally responsible\nagent. Small children can’t sign such contracts, nor can those disabled people whose\nlegal status requires them to be under the care and responsibility of guardians of one sort\nor another. The problem for robots who might want to attain such an exalted status is\nthat, like Superman, they are too invulnerable to be able to make a credible promise. If\nthey were to renege, what would happen? What would be the penalty for promisebreaking?\nBeing locked in a cell or, more plausibly, dismantled? Being locked up is\nbarely an inconvenience for an AI unless we first install artificial wanderlust that cannot\nbe ignored or disabled by the AI on its own (and it would be systematically difficult to\nmake this a foolproof solution, given the presumed cunning and self-knowledge of the\nAI); and dismantling an AI (either a robot or a bedridden agent like Watson) is not killing\nit, if the information stored in its design and software is preserved. The very ease of\ndigital recording and transmitting—the breakthrough that permits software and data to be,\n12\nJoanna J. Bryson, “Robots Should Be Slaves,” in Close Engagement with Artificial Companions, Yorick\nWilks, ed., (Amsterdam, The Netherlands: John Benjamins, 2010), pp. 63-74;\nhttp://www.cs.bath.ac.uk/~jjb/ftp/Bryson-Slaves-Book09.html.\n_____________, “Patiency Is Not a Virtue: AI and the Design of Ethical Systems,”\nhttps://www.cs.bath.ac.uk/~jjb/ftp/Bryson-Patiency-AAAISS16.pdf.\n47\nin effect, immortal—removes robots from the world of the vulnerable (at least robots of\nthe usually imagined sorts, with digital software and memories). If this isn’t obvious,\nthink about how human morality would be affected if we could make “backups” of\npeople every week, say. Diving headfirst on Saturday off a high bridge without benefit\nof bungee cord would be a rush that you wouldn’t remember when your Friday night\nbackup was put online Sunday morning, but you could enjoy the videotape of your\napparent demise thereafter.\nSo what we are creating are not—should not be—conscious, humanoid agents but\nan entirely new sort of entities, rather like oracles, with no conscience, no fear of death,\nno distracting loves and hates, no personality (but all sorts of foibles and quirks that\nwould no doubt be identified as the “personality” of the system): boxes of truths (if we’re\nlucky) almost certainly contaminated with a scattering of falsehoods. It will be hard\nenough learning to live with them without distracting ourselves with fantasies about the\nSingularity in which these AIs will enslave us, literally. The human use of human beings\nwill soon be changed—once again—forever, but we can take the tiller and steer between\nsome of the hazards if we take responsibility for our trajectory.\n48\nThe roboticist Rodney Brooks, featured in Errol Morris’s 1997 documentary Fast,\nCheap and Out of Control along with a lion-tamer, a topiarist, and an expert on the\nnaked mole rat, was described by one reviewer as “smiling with a wild gleam in his eye.”\nBut that’s pretty much true of most visionaries.\nA few years later in his career, Brooks, as befits one of the world’s leading\nroboticists, suggested that “we overanthropomorphize humans, who are after all mere\nmachines.” He went on to present a warm-hearted vision of a coming AI world in which\n“the distinction between us and robots is going to disappear.” He also admitted to\nsomething of a divided worldview. “Like a religious scientist, I maintain two sets of\ninconsistent beliefs and act on each of them in different circumstances,” he wrote. “It is\nthis transcendence between belief systems that I think will be what enables mankind to\nultimately accept robots as emotional machines, and thereafter start to empathize with\nthem and attribute free will, respect, and ultimately rights to them.”\nThat was in 2002. In these pages, he takes a somewhat more jaundiced, albeit\nnarrower, view; he is alarmed by the extent to which we have come to rely on pervasive\nsystems that are not just exploitative but also vulnerable, as a result of the too-rapid\ndevelopment of software engineering—an advance that seems to have outstripped the\nimposition of reliably effective safeguards.\n49\nTHE INHUMAN MESS OUR MACHINES HAVE GOTTEN US INTO\nRodney Brooks\nRodney Brooks is a computer scientist; Panasonic Professor of Robotics, emeritus, MIT;\nformer director, MIT Computer Science Lab; and founder, chairman, and CTO of\nRethink Robotics. He is the author of Flesh and Machines.\nMathematicians and scientists are often limited in how they see the big picture, beyond\ntheir particular field, by the tools and metaphors they use in their work. Norbert Wiener\nis no exception, and I might guess that neither am I.\nWhen he wrote The Human Use of Human Beings, Wiener was straddling the end\nof the era of understanding machines and animals simply as physical processes and the\nbeginning of our current era of understanding machines and animals as computational\nprocesses. I suspect there will be future eras whose tools will look as distinct from the\ntools of the two eras Wiener straddled as those tools did from each other.\nWiener was a giant of the earlier era and built on the tools developed since the\ntime of Newton and Leibniz to describe and analyze continuous processes in the physical\nworld. In 1948 he published Cybernetics, a word he coined to describe the science of\ncommunication and control in both machines and animals. Today we would refer to the\nideas in this book as control theory, an indispensable discipline for the design and\nanalysis of physical machines, while mostly neglecting Wiener’s claims about the science\nof communication. Wiener’s innovations were largely driven by his work during the\nSecond World War on mechanisms to aim and fire anti-aircraft guns. He brought\nmathematical rigor to the design of the sorts of technology whose design processes had\nbeen largely heuristic in nature: from the Roman waterworks through Watt’s steam\nengine to the early development of automobiles.\nOne can imagine a different contingent version of our intellectual and\ntechnological history had Alan Turing and John von Neumann, both of whom made\nmajor contributions to the foundations of computing, not appeared on the scene. Turing\ncontributed a fundamental model of computation—now known as a Turing Machine—in\nhis paper “On Computable Numbers with an Application to the Entscheidungsproblem,”\nwritten and revised in 1936 and published in 1937. In these machines, a linear tape of\nsymbols from a finite alphabet encodes the input for a computational problem and also\nprovides the working space for the computation. A different machine was required for\neach separate computational problem; later work by others would show that in one\nparticular machine, now known as a Universal Turing Machine, an arbitrary set of\ncomputing instructions could be encoded on that same tape.\nIn the 1940s, von Neumann developed an abstract self-reproducing machine\ncalled a cellular automaton. In this case it occupied a finite subset of an infinite twodimensional\narray of squares each containing a single symbol from a finite alphabet of\ntwenty-nine distinct symbols—the rest of the infinite array starts out blank. The single\nsymbols in each square change in lockstep, based on a complex but finite rule about the\ncurrent symbol in that square and its immediate neighbors. Under the complex rule that\nvon Neumann developed, most of the symbols in most of the squares stay the same and a\nfew change at each step. So when one looks at the non-blank squares, it appears that\n50\nthere is a constant structure with some activity going on inside it. When von Neumann’s\nabstract machine reproduced, it made a copy of itself in another region of the plane.\nWithin the “machine” was a horizontal line of squares which acted as a finite linear tape,\nusing a subset of the finite alphabet. It was the symbols in those squares that encoded the\nmachine of which they were a part. During the machine’s reproduction, the “tape” could\nmove either left or right and was both interpreted (transcribed) as the instructions\n(translation) for the new “machine” being built and then copied (replicated)—with the\nnew copy being placed inside the new machine for further reproduction. Francis Crick\nand James Watson later showed, in 1953, how such a tape could be instantiated in\nbiology by a long DNA molecule with its finite alphabet of four nucleobases: guanine,\ncytosine, adenine, and thymine (G, C, A, and T). 13 As in von Neumann’s machine, in\nbiological reproduction the linear sequence of symbols in DNA is interpreted—through\ntranscription into RNA molecules, which then are translated into proteins, the structures\nthat make up a new cell—and the DNA is replicated and encased in the new cell.\nA second foundational piece of work was in a 1945 “First Draft” report on the\ndesign for a digital computer, wherein von Neumann advocated for a memory that could\ncontain both instructions and data. 14 This is now known as a von Neumann architecture\ncomputer—as distinct from a Harvard architecture computer, where there are two\nseparate memories, one for instructions and one for data. The vast majority of computer\nchips built in the era of Moore’s Law are based on the von Neumann architecture,\nincluding those powering our data centers, our laptops, and our smartphones. Von\nNeumann’s digital-computer architecture is conceptually the same generalization—from\nearly digital computers constructed with electromagnetic relays at both Harvard\nUniversity and Bletchley Park—that occurs in going from a special-purpose Turing\nMachine to a Universal Turing Machine. Furthermore, his self-replicating automata\nshare a fundamental similarity with both the construction of a Turing Machine and the\nmechanism of DNA-based reproducing biological cells. There is to this day scholarly\ndebate over whether von Neumann saw the cross connections between these three pieces\nof work, Turing’s and his two. Turing’s revision of his paper was done while he and von\nNeumann were both at Princeton; indeed, after getting his PhD, Turing almost stayed on\nas von Neumann’s postdoc.\nWithout Turing and von Neumann, the cybernetics of Wiener might have\nremained a dominant mode of thought and driver of technology for much longer than its\nbrief moment of supremacy. In this imaginary version of history, we might well live\ntoday in an actual steam-punk world and not just get to observe its fantastical\ninstantiations at Maker Faires!\nMy point is that Wiener thought about the world—physical, biological, and (in\nHuman Use) sociological—in a particular way. He analyzed the world as continuous\nvariables, as he explains in chapter 1 along with a nod to thermodynamics through an\noverlay of Gibbs statistics. He also shoehorns in a weak and unconvincing model of\ninformation as message-passing between and among both physical and biological entities.\nTo me, and from today’s vantage point seventy years on, his tools seem woefully\n13\n“A Structure for Deoxyribose Nucleic Acid,” Nature 171, 737–738 (1953).\n14\nhttps://en.wikipedia.org//wiki/First_Draft_of_a_Report_on_the_EDVAC#Controversy. Von Neumann is\nlisted as the only author, whereas others contributed to the concepts he laid out; thus credit for the\narchitecture has gone to him alone.\n51\ninadequate for describing the mechanisms underlying biological systems, and so he\nmissed out on how similar mechanisms might eventually be embodied in technological\ncomputational systems—as now they have been. Today’s dominant technologies were\ndeveloped in the world of Turing and von Neumann, rather than the world of Wiener.\nIn the first industrial revolution, energy from a steam engine or a water wheel was\nused by human workers to replace their own energy. Instead of being a source of energy\nfor physical work, people became modulators of how a large source of energy was used.\nBut because steam engines and water wheels had to be large to be an efficient use of\ncapital, and because in the 18th century the only technology for spatial distribution of\nenergy was mechanical and worked only at very short range, many workers needed to be\ncrowded around the source of energy. Wiener correctly argues that the ability to transmit\nenergy as electricity caused a second industrial revolution. Now the source of energy\ncould be distant from where it was used, and from the beginning of the 20th century,\nmanufacturing could be much more dispersed as electrical-distribution grids were built.\nWiener then argues that a further new technology, that of the nascent\ncomputational machines of his time, will provide yet another revolution. The machines\nhe talks about seem to be both analog and (perhaps) digital in nature; and he points out, in\nThe Human Use of Human Beings, that since they will be able to make decisions, both\nblue-collar and white-collar workers may be reduced to being mere cogs in a much bigger\nmachine. He fears that humans might use and abuse one another through organizational\nstructures that this capability will encourage. We have certainly seen this play out in the\nlast sixty years, and that disruption is far from over.\nHowever, his physics-based view of computation protected him from realizing\njust how bad things might get. He saw machines’ ability to communicate as providing a\nnew and more inhuman way of exerting command and control. He missed that within a\nfew decades computation systems would become more like biological systems, and it\nseems, from his descriptions in chapter 10 of his own work on modeling some aspects of\nbiology, that he woefully underappreciated the many orders of magnitude of further\ncomplexity of biology over physics. We are in a much more complex situation today\nthan he foresaw, and I am worried that it is much more pernicious than even his worst\nimagined fears.\nIn the 1960s, computation became firmly based on the foundations set out by\nTuring and von Neumann, and it was digital computation, based on the idea of finite\nalphabets which they both used. An arbitrarily long sequence, or string, formed by\ncharacters from a finite alphabet, can be encoded as a unique integer. As with Turing\nMachines themselves, the formalism for computation became that of computing an\ninteger-valued function of a single integer-valued input.\nTuring and von Neumann both died in the 1950s and at that time this is how they\nsaw computation. Neither foresaw the exponential increase in computing capability that\nMoore’s Law would bring—nor how pervasive computing machinery would become.\nNor did they foresee two developments in our modeling of computation, each of which\nposes a great threat to human society.\nThe first is rooted in the abstractions they adopted. In the fifty-year, Moore’s\nLaw–fueled race to produce software that could exploit the doubling of computer\ncapability every two years, the typical care and certification of engineering disciplines\nwas thrown by the wayside. Software engineering was fast and prone to failures. This\n52\nrapid development of software without standards of correctness has opened up many\nroutes to exploit von Neumann architecture’s storage of data and instructions in the same\nmemory. One of the most common routes, known as “buffer overrun,” involves an input\nnumber (or long string of characters) that is bigger than the programmer expected and\noverflows into where the instructions are stored. By carefully designing an input number\nthat is too big by far, someone using a piece of software can infect it with instructions not\nintended by the programmer, and thus change what it does. This is the basis for creating\na computer virus—so named for its similarity to a biological virus. The latter injects\nextra DNA into a cell, and that cell’s transcription and translation mechanism blindly\ninterprets it, making proteins that may be harmful to the host cell. Furthermore, the\nreplication mechanism for the cell takes care of multiplying the virus. Thus, a small\nforeign entity can take control of a much bigger entity and bend its behavior in\nunexpected ways.\nThese and other forms of digital attacks have taken the security of our everyday\nlives from us. We rely on computers for almost everything now. We rely on computers\nfor our infrastructure of electricity, gas, roads, cars, trains, and airplanes; these are all\nvulnerable. We rely on computers for our banking, our payment of bills, our retirement\naccounts, our mortgages, our purchasing of goods and services—these, too, are all\nvulnerable. We rely on computers for our entertainment, our communications both\nbusiness and personal, our physical security at home, our information about the world,\nand our voting systems—all vulnerable. None of this will get fixed anytime soon. In the\nmeantime, many aspects of our society are open to vicious attacks, whether by\nfreelancing criminals or nation-state adversaries.\nThe second development is that computation has gone beyond simply computing\nfunctions. Instead, programs remain online continuously, and so they can gather data\nabout a sequence of queries. Under the Wiener/Turing/von Neumann scheme, we might\nthink of the communication pattern for a Web browser to be:\nNow instead it can look like this:\nUser: Give me Web page A.\nBrowser: Here is Web page A.\n…\nUser: Give me Web page B.\nBrowser: Here is Web page B.\nUser: Give me Web page A.\nBrowser: Here is Web page A. [And I will secretly\nremember that you asked for Web page A.]\n…\nUser: Give me Web page B.\nBrowser: Here is Web page B. [I see a correlation between\nits contents and that of the earlier requested Web page A, so I will\nupdate my model of you, the user, and transmit it to the company\nthat produced me.]\n53\nWhen the machine no longer simply computes a function but instead maintains a\nstate, it can start to make inferences about the human by the sequence of requests\npresented to it. And when different programs correlate across different request streams—\nsay, correlating Web-page searches with social-media posts, or the payment for services\non another platform, or the dwell time on a particular advertisement, or where the user\nhas walked or driven with their GPS-enabled smartphone—the total systems of many\nprograms communicating with one another and with databases leads to a whole new loss\nof privacy. The great exploitative leap made by so many West Coast companies has been\nto monetize those inferences without the knowing permission of the person generating the\ninteractions with the computing machine platforms.\nWiener, Turing, and von Neumann could not foresee the complexity of those\nplatforms, wherein the legal mumbo-jumbo of the terms-of-use contracts the humans\nwillingly enter into, without an inkling of what they entail, leads them to give up rights\nthey would never concede in a one-on-one interaction with another human being. The\ncomputation platforms have become a shield behind which some companies hide in order\nto inhumanly exploit others. In certain other countries, the governments carry out these\nmanipulations, and there the goal is not profits but the suppression of dissent.\nHumankind has gotten itself into a fine pickle: We are being exploited by\ncompanies that paradoxically deliver services we crave, and at the same time our lives\ndepend on many software-enabled systems that are open to attack. Getting ourselves out\nof this mess will be a long-term project. It will involve engineering, legislation, and most\nimportant, moral leadership. Moral leadership is the first and biggest challenge.\n54\nI first met Frank Wilczek in the 1980s, when he invited me to his home in Princeton to\ntalk about anyons. “The address is 112 Mercer Street,” he wrote. “Look for the house\nwith no driveway.” So there I was, a few hours later, in Einstein’s old living room,\ntalking to a future recipient of the Nobel Prize in physics. If Frank was as impressed as I\nwas by the surroundings, you’d never guess it. His only comment concerned the difficulty\nof finding a parking place in front of a “house with no driveway.”\nUnlike most theoretical physicists, Frank has long had a keen interest in AI, as\nwitnessed in these three “Observations”:\n1.“Francis Crick called it ‘the Astonishing Hypothesis’: that consciousness, also\nknown as Mind, is an emergent property of matter,” which, if true, indicates that “all\nintelligence is machine intelligence. What distinguishes natural from artificial\nintelligence is not what it is, but only how it is made.”\n2. “Artificial intelligence is not the product of an alien invasion. It is an artifact\nof a particular human culture and reflects the values of that culture.”\n3. “David Hume’s striking statement ‘Reason Is, and Ought only to Be, the Slave\nof the Passions’ was written in 1738 [and] was, of course, meant to apply to human\nreason and human passions. . . . But Hume’s logical/philosophical point remains valid\nfor AI. Simply put: Incentives, not abstract logic, drive behavior.”\nHe notes that “the big story of the 20th and the 21st century is that [as]\ncomputing develops, we learn how to calculate the consequences of the [fundamental]\nlaws better and better. There’s also a feedback cycle: When you understand matter\nbetter, you can design better computers, which will enable you to calculate better. It’s\nkind of an ascending helix.”\nHere he argues that human intelligence, for now, holds the advantage—yet our\nfuture, unbounded by our solar system and doubtless also by our galaxy, will never be\nrealized without the help of our AIs.\n55\nTHE UNITY OF INTELLIGENCE\nFrank Wilczek\nFrank Wilczek is Herman Feshbach Professor of Physics at MIT, recipient of the 2004\nNobel Prize in physics, and the author of A Beautiful Question: Finding Nature’s Deep\nDesign.\nI. A Simple Answer to Contentious Questions:\n• Can an artificial intelligence be conscious?\n• Can an artificial intelligence be creative?\n• Can an artificial intelligence be evil?\nThose questions are often posed today, both in popular media and in scientifically\ninformed debates. But the discussions never seem to converge. Here I’ll begin by\nanswering them as follows:\nBased on physiological psychology, neurobiology, and physics, it would be very\nsurprising if the answers were not Yes, Yes, and Yes. The reason is simple, yet\nprofound: Evidence from those fields makes it overwhelmingly likely that there is no\nsharp divide between natural and artificial intelligence.\nIn his 1994 book of that title, the renowned biologist Francis Crick proposed an\n“astonishing hypothesis”: that mind emerges from matter. He famously claimed that\nmind, in all its aspects, is “no more than the behavior of a vast assembly of nerve cells\nand their associated molecules.”\nThe “astonishing hypothesis” is in fact the foundation of modern neuroscience.\nPeople try to understand how minds work by understanding how brains function; and\nthey try to understand how brains function by studying how information is encoded in\nelectrical and chemical signals, transformed by physical processes, and used to control\nbehavior. In that scientific endeavor, they make no allowance for extraphysical behavior.\nSo far, in thousands of exquisite experiments, that strategy has never failed. It has never\nproved necessary to allow for the influence of consciousness or creativity unmoored from\nbrain activity to explain any observed fact of psychophysics or neurobiology. No one has\never stumbled upon a power of mind which is separate from conventional physical events\nin biological organisms. While there are many things we do not understand about brains,\nand about minds, the “astonishing hypothesis” has held intact.\nIf we broaden our view beyond neurobiology to consider the whole range of\nscientific experimentation, the case becomes still more compelling. In modern physics,\nthe foci of interest are often extremely delicate phenomena. To investigate them,\nexperimenters must take many precautions against contamination by “noise.” They often\nfind it necessary to construct elaborate shielding against stray electric and magnetic\nfields; to compensate for tiny vibrations due to micro-earthquakes or passing cars; to\nwork at extremely low temperatures and in high vacuum, and so forth. But there’s a\nnotable exception: They have never found it necessary to make allowances for what\npeople nearby (or, for that matter, far away) are thinking. No “thought waves,” separate\nfrom known physical processes yet capable of influencing physical events, seem to exist.\nThat conclusion, taken at face value, erases the distinction between natural and\nartificial intelligence. It implies that if we were to duplicate, or accurately simulate, the\nphysical processes occurring in a brain—as, in principle, we can—and wire up its input\n56\nand output to sense organs and muscles, then we would reproduce, in a physical artifact,\nthe observed manifestations of natural intelligence. Nothing observable would be\nmissing. As an observer, I’d have no less (and no more) reason to ascribe consciousness,\ncreativity, or evil to that artifact than I do to ascribe those properties to its natural\ncounterparts, like other human beings.\nThus, by combining Crick’s “astonishing hypothesis” in neurobiology with\npowerful evidence from physics, we deduce that natural intelligence is a special case of\nartificial intelligence. That conclusion deserves a name, and I will call it “the astonishing\ncorollary.”\nWith that, we have the answer to our three questions. Since consciousness,\ncreativity, and evil are obvious features of natural human intelligence, they are possible\nfeatures of artificial intelligence.\nA hundred years ago, or even fifty, to believe the hypothesis that mind emerges\nfrom matter, and to infer our corollary that natural intelligence is a special case of\nartificial intelligence, would have been leaps of faith. In view of the many surrounding\ngaps—chasms, really—in contemporary understanding of biology and physics, they were\ngenuinely doubtful propositions. But epochal developments in those areas have changed\nthe picture:\nIn biology: A century ago, not only thought but also metabolism, heredity, and\nperception were deeply mysterious aspects of life that defied physical explanation.\nToday, of course, we have extremely rich and detailed accounts of metabolism, heredity,\nand many aspects of perception, from the bottom up, starting at the molecular level.\nIn physics: After a century of quantum physics and its application to materials,\nphysicists have discovered, over and over, how rich and strange the behavior of matter\ncan be. Superconductors, lasers, and many other wonders demonstrate that large\nassemblies of molecular units, each simple in itself, can exhibit qualitatively new,\n“emergent” behavior, while remaining fully obedient to the laws of physics. Chemistry,\nincluding biochemistry, is a cornucopia of emergent phenomena, all now quite firmly\ngrounded in physics. The pioneering physicist Philip Anderson, in an essay titled “More\nIs Different,” offers a classic discussion of emergence. He begins by acknowledging that\n“the reductionist hypothesis [i.e., the completeness of physical explanations based on\nknown interactions of simple parts] may still be a topic for controversy among\nphilosophers, but among the great majority of active scientists I think it is accepted\nwithout question.” But he goes on to emphasize that “[t]he behavior of large and\ncomplex aggregates of elementary particles, it turns out, is not to be understood in terms\nof a simple extrapolation of the properties of a few particles.” 15 Each new level of size\nand complexity supports new forms of organization, whose patterns encode information\nin new ways and whose behavior is best described using new concepts.\nElectronic computers are a magnificent example of emergence. Here, all the\ncards are on the table. Engineers routinely design, from the bottom up, based on known\n(and quite sophisticated) physical principles, machines that process information in\nextremely impressive ways. Your iPhone can beat you at chess, quickly collect and\ndeliver information about anything, and take great pictures, too. Because the process\nwhereby computers, smartphones, and other intelligent objects are designed and\nmanufactured is completely transparent, there can be no doubt that their wonderful\n15\nScience, 4 August 1972, Vol. 177, No. 4047, pp. 393-96.\n57\ncapabilities emerge from regular physical processses, which we can trace down to the\nlevel of electrons, photons, quarks, and gluons. Evidently, brute matter can get pretty\nsmart.\nLet me summarize the argument. From two strongly supported hypotheses, we’ve\ndrawn a straightforward conclusion:\n• Human mind emerges from matter.\n• Matter is what physics says it is.\n• Therefore, the human mind emerges from physical processes we\nunderstand and can reproduce artificially.\n• Therefore, natural intelligence is a special case of artificial\nintelligence.\nOf course, our “astonishing corollary” could fail; the first two lines of this\nargument are hypotheses. But their failure would have to bring in a foundation-shattering\ndiscovery—a significant new phenomenon, with large-scale physical consequences,\nwhich takes place in unremarkable, well-studied physical circumstances (i.e., the\nmaterials, temperatures, and pressures inside human brains) yet which has somehow\nmanaged for many decades to elude determined investigators armed with sophisticated\ninstruments. Such a discovery would be. . . astonishing.\nII. The Future of Intelligence\nIt is part of human nature to improve on human bodies and minds. Historically, clothing,\neyeglasses, and watches are examples of increasingly sophisticated augmentations that\nenhance our toughness, perception, and awareness. They are major improvements to the\nnatural human endowment, whose familiarity should not blind us to their depth. Today\nsmartphones and the Internet are bringing the human drive toward augmentation into\nrealms more central to our identity as intelligent beings. They are giving us, in effect,\nquick access to a vast collective awareness and a vast collective memory.\nAt the same time, autonomous artificial intelligences have become world\nchampions in a wide variety of “cerebral” games, such as chess and Go, and have taken\nover many sophisticated pattern-recognition tasks, such as reconstructing what happened\nduring complex reactions at the Large Hadron Collider from a blizzard of emerging\nparticle tracks, to find new particles; or gathering clues from fuzzy X-ray, fMRI, and\nother types of images, to diagnose medical problems.\nWhere is this drive toward self-enhancement and innovation taking us? While the\nprecise sequence of events and the timescale over which they’ll play out is impossible to\npredict (or, at least, beyond me), some basic considerations suggest that eventually the\nmost powerful embodiments of mind will be quite different things from human brains as\nwe know them today.\nConsider six factors whereby information-processing technology exceeds human\ncapabilities—vastly, qualitatively, or both:\n• Speed: The orchestrated motion of electrons, which is the heart of modern\nartificial information-processing, can be much faster than the processes of\ndiffusion and chemical change by which brains operate. Typical modern\ncomputer clock rates approach 10 gigahertz, corresponding to 10 billion\noperations per second. No single measure of speed applies to the bewildering\nvariety of brain processes, but one fundamental limitation is latency of action\n58\npotentials, which limits their spacing to a few 10s per second. It is probably\nno accident that the “frame rate,” at which we can distinguish that movies are\nactually a sequence of stills, is about 40 per second. Thus, electronic\nprocessing is close to a billion times faster.\n• Size: The linear dimension of a typical neuron is about 10 microns.\nMolecular dimensions, which set a practical limit, are about 10,000 times\nsmaller, and artificial processing units are approaching that scale. Smallness\nmakes communication more efficient.\n• Stability: Whereas human memory is essentially continuous (analog),\nartificial memory can incorporate discrete (digital) features. Whereas analog\nquantities can erode, digital quantities can be stored, refreshed, and\nmaintained with complete accuracy.\n• Duty Cycle: Human brains grow tired with effort. They need time off to take\nnourishment and to sleep. They carry the burden of aging. Most profoundly:\nThey die.\n• Modularity (open architecture): Because artificial information processors can\nsupport precisely defined digital interfaces, they can readily assimilate new\nmodules. Thus, if we want a computer to “see” ultraviolet or infrared or\n“hear” ultrasound, we can feed the output from an appropriate sensor directly\ninto its “nervous system.” The architecture of brains is much more closed and\nopaque, and the human immune system actively resists implants.\n• Quantum readiness: One case of modularity deserves special mention,\nbecause of its long-term potential. Recently physicists and information\nscientists have come to appreciate that the principles of quantum mechanics\nsupport new computing principles, which can empower qualitatively new\nforms of information processing and (plausibly) new levels of intelligence.\nBut these possibilities rely on aspects of quantum behavior which are quite\ndelicate and seem especially unsuitable for interfacing with the warm, wet,\nmessy enviroment of human brains.\nEvidently, as platforms for intelligence, human brains are far from optimal. Still,\nalthough versatile housekeeping robots or mechanical soldiers would find ready, lucrative\nmarkets, at present there is no machine that approaches the kind of general-purpose\nhuman intelligence those applications would require. Despite their relative weakness on\nmany fronts, human brains have some big advantages over their artificial competitors.\nLet me mention five:\n• Three-dimensionality: Although, as noted, the linear dimensions of existing\nartificial processing units are vastly smaller than those of brains, the procedure by\nwhich they’re made—centered on lithography (basically, etching)—is essentially\ntwo-dimensional. That is revealed visibly in the geometry of computer boards\nand chips. Of course, one can stack boards, but the spacing between layers is\nmuch larger, and communication much less efficient, than within layers. Brains\nmake better use of all three dimensions.\n• Self-repair: Human brains can recover from, or work around, many kinds of\ninjuries or errors. Computers often must be repaired or rebooted externally.\n59\n• Connectivity: Human neurons typically support several hundred connections\n(synapses). Moreover, the complex pattern of these connections is very\nmeaningful. (See our next point.) Computer units typically make only a handful\nof connections, in regular, fixed patterns.\n• Development (self-assembly with interactive sculpting): The human brain grows\nits units by cell divisions and orchestrates them into coherent structures by\nmovement and folding. It also proliferates an abundance of connections among\nthe cells. An important part of its sculpting occurs through active processes\nduring infancy and childhood, as the individual interacts with his or her\nenvironment. In this process, many connections are winnowed away, while others\nare strengthened, depending on their effectiveness in use. Thus, the fine structure\nof the brain is tuned through interaction with the external world—a rich source of\ninformation and feedback!\n• Integration (sensors and actuators): The human brain comes equipped with a\nvariety of sensory organs, notably including its outgrowth eyes, and with versatile\nactuators, including hands that build, legs that walk, and mouths that speak.\nThose sensors and actuators are seamlessy integrated into the brain’s informationprocessing\ncenters, having been honed over millions of years of natural selection.\nWe interpret their raw signals and control their large-scale actions with minimal\nconscious attention. The flip side is that we don’t know how we do it, and the\nimplementation is opaque. It’s proving surprisingly difficult to reach human\nstandards on these “routine” input-output functions.\nThese advantages of human brains over currently engineered artifacts are\nprofound. Human brains supply an inspiring existence proof, showing us several ways\nwe can get more out of matter. When, if ever, will our engineering catch up?\nI don’t know for sure, but let me offer some informed opinions. The challenges\nof three-dimensionality and, to a lesser extent, self-repair don’t look overwhelming.\nThey present some tough engineering problems, but many incremental improvements are\neasy to imagine, and there are clear paths forward. And while the powers of human eyes,\nhands, and other sensory organs and actuators are wonderfully effective, their abilities are\nfar from exhausting any physical limits. Optical systems can take pictures with higher\nresolution in space, time, and color, and in more regions of the electromagnetic spectrum;\nrobots can move faster and be stronger; and so forth. In these domains, the components\nnecessary for superhuman performance, along many axes, are already available. The\nbottleneck is getting information into and out of them, rapidly, in the language of the\ninformation-processing units.\nAnd this brings us to the remaining, and I think most profound, advantages of\nbrains over artificial devices, which stem from their connectivity and interactive\ndevelopment. Those two advantages are synergistic, since it is interactive development\nthat sculpts the massively wired but sprawling structure of the infant brain, enabled by\nexponential growth of neurons and synapses, to get tuned into the extraordinary\ninstrument it becomes. Computer scientists are beginning to discover the power of the\nbrain’s architecture: Neural nets, whose basic design, as their name suggests, was directly\ninspired by the brain’s, have scored some spectacular successes in game playing and\npattern recognition, as noted. But present-day engineering has nothing comparable—in\n60\nthe (currently) esoteric domain of self-reproducing machines—to the power and\nversatility of neurons and their synapses. This could become a new, great frontier of\nresearch. Here too, biology might point the way, as we come to understand biological\ndevelopment well enough to imitate its essence.\nAltogether, the advantages of artificial over natural intelligence appear\npermanent, while the advantages of natural over artificial intelligence, though substantial\nat present, appear transient. I’d guess that it will be many decades before engineering\ncatches up, but—barring catastrophic wars, climate change, or plagues, so that\ntechnological progress stays vigorous—few centuries.\nIf that’s right, we can look forward to several generations during which humans,\nempowered and augmented by smart devices, coexist with increasingly capable\nautonomous AIs. There will be a complex, rapidly changing ecology of intelligence, and\nrapid evolution in consequence. Given the intrinsic advantages that engineered devices\nwill eventually offer, the vanguard of that evolution will be cyborgs and superminds,\nrather than lightly adorned Homo sapiens.\nAnother important impetus will come from the exploration of hostile\nenvironments, both on Earth (e.g., the deep ocean) and, especially, in space. The human\nbody is poorly adapted to conditions outside a narrow band of temperatures, pressures,\nand atmospheric composition. It needs a wide variety of specific, complex nutrients, and\nplenty of water. Also, it is not radiation-hardened. As the manned space program has\namply demonstrated, it is difficult and expensive to maintain humans outside their\nterrestrial comfort zone. Cyborgs or autonomous AIs could be much more effective in\nthese explorations. Quantum AIs, with their sensitivity to noise, might even be happier in\nthe cold and dark of deep space.\nIn a moving passage from his 1935 novel Odd John, science fiction’s singular\ngenius Olaf Stapledon has his hero, a superhuman (mutant) intelligence, describe Homo\nsapiens as “the Archaeopteryx of the spirit.” He says this, fondly, to his friend and\nbiographer, who is a normal human. Archaeopteryx was a noble creature, and a bridge to\ngreater ones.\n61\nI was introduced to Max Tegmark some years ago by his MIT colleague Alan Guth, the\nfather of the inflationary universe. A distinguished theoretical physicist and cosmologist\nhimself, Max’s principal concern nowadays is the looming existential risk posed by the\ncreation of an AGI (artificial general intelligence—that is, one that matches human\nintelligence). Four years ago, Max co-founded, with Jaan Tallinn and others, the Future\nof Life Institute (FLI), which bills itself as “an outreach organization working to ensure\nthat tomorrow’s most powerful technologies are beneficial for humanity.” While on a\nbook tour in London, he was in the midst of planning for FLI, and he admits being driven\nto tears in a tube station after a trip to the London Science Museum, with its exhibitions\nspanning the gamut of humanity’s technological achievements. Was all that impressive\nprogress in vain?\nFLI’s scientific advisory board includes Elon Musk, Frank Wilczek, George\nChurch, Stuart Russell, and the Oxford philosopher Nick Bostrom, who dreamed up an\noft-quoted Gedankenexperiment that results in a world full of paper clips and nothing\nelse, produced by an (apparently) well-meaning AGI who was just following orders. The\nInstitute sponsors conferences (Puerto Rico 2015, Asilomar 2017) on AI safety issues and\nin 2018 instituted a grants competition focusing on research in aid of maximizing the\nsocietal benefits of AGI.\nWhile Max is sometimes listed—by the non-cognoscenti—on the side of the\nscaremongers, he believes, like Frank Wilczek, in a future that will immensely benefit\nfrom AGI if, in the attempt to create it, we can keep the human species from being\nsidelined.\n62\nLET’S ASPIRE TO MORE THAN MAKING OURSELVES OBSOLETE\nMax Tegmark\nMax Tegmark is an MIT physicist and AI researcher; president of the Future of Life\nInstitute; scientific director of the Foundational Questions Institute; and the author\nof Our Mathematical Universe and Life 3.0: Being Human in the Age of Artificial\nIntelligence.\nAlthough there’s great controversy about how and when AI will impact humanity, the\nsituation is clearer from a cosmic perspective: The technology-developing life that has\nevolved on Earth is rushing to make itself obsolete without devoting much serious\nthought to the consequences. This strikes me as embarrassingly lame, given that we can\ncreate amazing opportunities for humanity to flourish like never before, if we dare to\nsteer a more ambitious course.\n13.8 billion years after its birth, our Universe has become aware of itself. On a\nsmall blue planet, tiny conscious parts of our Universe have discovered that what they\nonce thought was the sum total of existence was a minute part of something far grander: a\nsolar system in a galaxy in a universe with over 100 billion other galaxies, arranged into\nan elaborate pattern of groups, clusters, and superclusters.\nConsciousness is the cosmic awakening; it transformed our Universe from a\nmindless zombie with no self-awareness into a living ecosystem harboring self-reflection,\nbeauty, hope, meaning, and purpose. Had that awakening never taken place, our\nUniverse would have been pointless—a gigantic waste of space. Should our Universe go\nback to sleep permanently due to some cosmic calamity or self-inflicted mishap, it will\nbecome meaningless again.\nOn the other hand, things could get even better. We don’t yet know whether we\nhumans are the only stargazers in the cosmos, or even the first, but we’ve already learned\nenough about our Universe to know that it has the potential to wake up much more fully\nthan it has thus far. AI pioneers such as Norbert Wiener have taught us that a further\nawakening of our Universe’s ability to process and experience information need not\nrequire eons of additional evolution but perhaps mere decades of human scientific\ningenuity.\nWe may be like that first glimmer of self-awareness you experienced when you\nemerged from sleep this morning, a premonition of the much greater consciousness that\nwould arrive once you opened your eyes and fully awoke. Perhaps artificial\nsuperintelligence will enable life to spread throughout the cosmos and flourish for\nbillions or trillions of years, and perhaps this will be because of decisions we make here,\non our planet, in our lifetime.\nOr humanity may soon go extinct, through some self-inflicted calamity caused by\nthe power of our technology growing faster than the wisdom with which we manage it.\nThe evolving debate about AI’s societal impact\nMany thinkers dismiss the idea of superintelligence as science fiction, because they view\nintelligence as something mysterious that can exist only in biological organisms—\nespecially humans—and as fundamentally limited to what today’s humans can do. But\nfrom my perspective as a physicist, intelligence is simply a certain kind of information\n63\nprocessing performed by elementary particles moving around, and there’s no law of\nphysics that says one can’t build machines more intelligent in every way than we are, and\nable to seed cosmic life. This suggests that we’ve seen just the tip of the intelligence\niceberg; there’s an amazing potential to unlock the full intelligence latent in nature and\nuse it to help humanity flourish—or flounder.\nOthers, including some of the authors in this volume, dismiss the building of an\nAGI (Artificial General Intelligence—an entity able to accomplish any cognitive task at\nleast as well as humans) not because they consider it physically impossible but because\nthey deem it too difficult for humans to pull off in less than a century. Among\nprofessional AI researchers, both types of dismissal have become minority views because\nof recent breakthroughs. There is a strong expectation that AGI will be achieved within a\ncentury, and the median forecast is only decades away. A recent survey of AI researchers\nby Vincent Muller and Nick Bostrom concludes:\n[T]he results reveal a view among experts that AI systems will probably (over\n50%) reach overall human ability by 2040-50, and very likely (with 90%\nprobability) by 2075. From reaching human ability, it will move on to\nsuperintelligence in 2 years (10%) to 30 years (75%) thereafter. 16\nIn the cosmic perspective of gigayears, it makes little difference whether AGI\narrives in thirty or three hundred years, so let’s focus on the implications rather than the\ntiming.\nFirst, we humans discovered how to replicate some natural processes with\nmachines, making our own heat, light, and mechanical horsepower. Gradually we\nrealized that our bodies were also machines, and the discovery of nerve cells blurred the\nboundary between body and mind. Finally, we started building machines that could\noutperform not only our muscles but our minds as well. We’ve now been eclipsed by\nmachines in the performance of many narrow cognitive tasks, ranging from memorization\nand arithmetic to game play, and we are in the process of being overtaken in many more,\nfrom driving to investing to medical diagnosing. If the AI community succeeds in its\noriginal goal of building AGI, then we will have, by definition, been eclipsed at all\ncognitive tasks.\nThis begs many obvious questions. For example, will whoever or whatever\ncontrols the AGI control Earth? Should we aim to control superintelligent machines? If\nnot, can we ensure that they understand, adopt, and retain human values? As Norbert\nWiener put it in The Human Use of Human Beings:\nWoe to us if we let [the machine] decide our conduct, unless we have previously\nexamined the laws of its action, and know fully that its conduct will be carried\nout on principles acceptable to us! On the other hand, the machine . . . , which\ncan learn and can make decisions on the basis of its learning, will in no way be\nobliged to make such decisions as we should have made, or will be acceptable to\nus.\n16\nVincent C. Müller & Nick Bostrom, “Future Progress in Artificial Intelligence: A Survey of Expert\nOpinion,” in Fundamental Issues of Artificial Intelligence, Vincent C. Muller, ed. (Springer International\nPublishing Switzerland, 2016), pp. 555-72. https://nickbostrom.com/papers/survey.pdf.\n64\nAnd who are the “us”? Who should deem “such decisions . . . acceptable”? Even\nif future powers decide to help humans survive and flourish, how will we find meaning\nand purpose in our lives if we aren’t needed for anything?\nThe debate about the societal impact of AI has changed dramatically in the last\nfew years. In 2014, what little public talk there was of AI risk tended to be dismissed as\nLuddite scaremongering, for one of two logically incompatible reasons:\n(1) AGI was overhyped and wouldn’t happen for at least another century.\n(2) AGI would probably happen sooner but was virtually guaranteed to be\nbeneficial.\nToday, talk of AI’s societal impact is everywhere, and work on AI safety and AI\nethics has moved into companies, universities, and academic conferences. The\ncontroversial position on AI safety research is no longer to advocate for it but to dismiss\nit. Whereas the open letter that emerged from the 2015 Puerto Rico AI conference (and\nhelped mainstream AI safety) spoke only in vague terms about the importance of keeping\nAI beneficial, the 2017 Asilomar AI Principles (see below) had real teeth: They explicitly\nmention recursive self-improvement, superintelligence, and existential risk, and were\nsigned by AI industry leaders and over a thousand AI researchers from around the world.\nNonetheless, most discussion is limited to the near-term impact of narrow AI and\nthe broader community pays only limited attention to the dramatic transformations that\nAGI may soon bring to life on Earth. Why?\nWhy we’re rushing to make ourselves obsolete, and why we avoid talking about it\nFirst of all, there’s simple economics. Whenever we figure out how to make another type\nof human work obsolete by building machines that do it better and cheaper, most of\nsociety gains: Those who build and use the machines make profits, and consumers get\nmore affordable products. This will be as true of future investor AGIs and scientist AGIs\nas it was of weaving machines, excavators, and industrial robots. In the past, displaced\nworkers usually found new jobs, but this basic economic incentive will remain even if\nthat is no longer the case. The existence of affordable AGI means, by definition,\nthat all jobs can be done more cheaply by machines, so anyone claiming that “people will\nalways find new well-paying jobs” is in effect claiming that AI researchers will fail to\nbuild AGI.\nSecond, Homo sapiens is by nature curious, which will motivate the scientific\nquest for understanding intelligence and developing AGI even without economic\nincentives. Although curiosity is one of the most celebrated human attributes, it can\ncause problems when it fosters technology we haven’t yet learned how to manage wisely.\nSheer scientific curiosity without profit motive contributed to the discovery of nuclear\nweapons and tools for engineering pandemics, so it’s not unthinkable that the old adage\n“Curiosity killed the cat” will turn out to apply to the human species as well.\nThird, we’re mortal. This explains the near unanimous support for developing\nnew technologies that help us live longer, healthier lives, which strongly motivates\ncurrent AI research. AGI can clearly aid medical research even more. Some thinkers\neven aspire to near immortality via cyborgization or uploading.\nWe’re thus on the slippery slope toward AGI, with strong incentives to keep\nsliding downward, even though the consequence will by definition be our economic\nobsolescence. We will no longer be needed for anything, because all jobs can be done\n65\nmore efficiently by machines. The successful creation of AGI would be the biggest event\nin human history, so why is there so little serious discussion of what it might lead to?\nHere again, the answer involves multiple reasons.\nFirst, as Upton Sinclair famously quipped, “It is difficult to get a man to\nunderstand something, when his salary depends on his not understanding it.” 17 For\nexample, spokesmen for tech companies or university research groups often claim there\nare no risks attached to their activities even if they privately think otherwise. Sinclair’s\nobservation may help explain not only reactions to risks from smoking and climate\nchange but also why some treat technology as a new religion whose central articles of\nfaith are that more technology is always better and whose heretics are clueless\nscaremongering Luddites.\nSecond, humans have a long track record of wishful thinking, flawed\nextrapolation of the past, and underestimation of emerging technologies. Darwinian\nevolution endowed us with powerful fear of concrete threats, not of abstract threats from\nfuture technologies that are hard to visualize or even imagine. Consider trying to warn\npeople in 1930 of a future nuclear arms race, when you couldn’t show them a single\nnuclear explosion video and nobody even knew how to build such weapons. Even top\nscientists can underestimate uncertainty, making forecasts that are either too optimistic—\nWhere are those fusion reactors and flying cars?—or too pessimistic. Ernest Rutherford,\narguably the greatest nuclear physicist of his time, said in 1933—less than twenty-four\nhours before Leo Szilard conceived of the nuclear chain reaction—that nuclear energy\nwas “moonshine.” Essentially nobody at that time saw the nuclear arms race coming.\nThird, psychologists have discovered that we tend to avoid thinking of disturbing\nthreats when we believe there’s nothing we can do about them anyway. In this case,\nhowever, there are many constructive things we can do, if we can get ourselves to start\nthinking about the issue.\nWhat can we do?\nI’m advocating a strategy change from “Let’s rush to build technology that makes us\nobsolete—what could possibly go wrong?” to “Let’s envision an inspiring future and\nsteer toward it.”\nTo motivate the effort required for steering, this strategy begins by envisioning an\nenticing destination. Although Hollywood’s futures tend to be dystopian, the fact is that\nAGI can help life flourish as never before. Everything I love about civilization is the\nproduct of intelligence, so if we can amplify our own intelligence with AGI, we have the\npotential to solve today’s and tomorrow’s thorniest problems, including disease, climate\nchange, and poverty. The more detailed we can make our shared positive visions for the\nfuture, the more motivated we will be to work together to realize them.\nWhat should we do in terms of steering? The twenty-three Asilomar principles\nadopted in 2017 offer plenty of guidance, including these short-term goals:\n(1) An arms race in lethal autonomous weapons should be avoided.\n(2) The economic prosperity created by AI should be shared broadly, to benefit all\nof humanity.\n17\nUpton Sinclair, I, Candidate for Governor: And How I Got Licked (Berkeley CA: University of\nCalifornia Press, 1994), p. 109.\n66\n(3) Investments in AI should be accompanied by funding for research on ensuring\nits beneficial use. . . . How can we make future AI systems highly robust, so that they do\nwhat we want without malfunctioning or getting hacked. 18\nThe first two involve not getting stuck in suboptimal Nash equilibria. An out-ofcontrol\narms race in lethal autonomous weapons that drives the price of automated\nanonymous assassination toward zero will be very hard to stop once it gains momentum.\nThe second goal would require reversing the current trend in some Western countries\nwhere sectors of the population are getting poorer in absolute terms, fueling anger,\nresentment, and polarization. Unless the third goal can be met, all the wonderful AI\ntechnology we create might harm us, either accidentally or deliberately.\nAI safety research must be carried out with a strict deadline in mind: Before AGI\narrives, we need to figure out how to make AI understand, adopt, and retain our goals.\nThe more intelligent and powerful machines get, the more important it becomes to align\ntheir goals with ours. As long as we build relatively dumb machines, the question isn’t\nwhether human goals will prevail but merely how much trouble the machines can cause\nbefore we solve the goal-alignment problem. If a superintelligence is ever unleashed,\nhowever, it will be the other way around: Since intelligence is the ability to accomplish\ngoals, a superintelligent AI is by definition much better at accomplishing its goals than\nwe humans are at accomplishing ours, and will therefore prevail.\nIn other words, the real risk with AGI isn’t malice but competence. A\nsuperintelligent AGI will be extremely good at accomplishing its goals, and if those goals\naren’t aligned with ours, we’re in trouble. People don’t think twice about flooding\nanthills to build hydroelectric dams, so let’s not place humanity in the position of those\nants. Most researchers argue that if we end up creating superintelligence, we should\nmake sure it’s what AI-safety pioneer Eliezer Yudkowsky has termed “friendly AI”—AI\nwhose goals are in some deep sense beneficial.\nThe moral question of what these goals should be is just as urgent as the technical\nquestions about goal alignment. For example, what sort of society are we hoping to\ncreate, where we find meaning and purpose in our lives even though we, strictly\nspeaking, aren’t needed? I’m often given the following glib response to this\nquestion: “Let’s build machines that are smarter than us and then let them figure out the\nanswer!” This mistakenly equates intelligence with morality. Intelligence isn’t good or\nevil but morally neutral. It’s simply an ability to accomplish complex goals, good or bad.\nWe can’t conclude that things would have been better if Hitler had been more intelligent.\nIndeed, postponing work on ethical issues until after goal-aligned AGI is built would be\nirresponsible and potentially disastrous. A perfectly obedient superintelligence whose\ngoals automatically align with those of its human owner would be like Nazi SS-\nObersturmbannführer Adolf Eichmann on steroids. Lacking moral compass or\ninhibitions of its own, it would, with ruthless efficiency, implement its owner’s goals,\nwhatever they might be. 19\nWhen I speak of the need to analyze technology risk, I’m sometimes accused of\nscaremongering. But here at MIT, where I work, we know that such risk analysis isn’t\nscaremongering: It’s safety engineering. Before the moon-landing mission, NASA\n18\nhttps://futureoflife.org/ai-principles/\n19\nSee, for example, Hannah Arendt, Eichmann in Jerusalem: A Report on the Banality of Evil (New York:\nPenguin Classics, 2006).\n67\nsystematically thought through everything that could possibly go wrong when putting\nastronauts on top of a 110-meter rocket full of highly flammable fuel and launching them\nto a place where nobody could help them—and there were lots of things that could go\nwrong. Was this scaremongering? No, this was the safety engineering that ensured the\nmission’s success. Similarly, we should analyze what could go wrong with AI to ensure\nthat it goes right.\nOutlook\nIn summary, if our technology outpaces the wisdom with which we manage it, it can lead\nto our extinction. It’s already caused the extinction of from 20 to 50 percent of all\nspecies on Earth, by some estimates, 20 and it would be ironic if we’re next in line. It\nwould also be pathetic, given that the opportunities offered by AGI are literally\nastronomical, potentially enabling life to flourish for billions of years not only on Earth\nbut also throughout much of our cosmos.\nInstead of squandering this opportunity through unscientific risk denial and poor\nplanning, let’s be ambitious! Homo sapiens is inspiringly ambitious, as reflected in\nWilliam Ernest Henley’s famous lines from Invictus: “I am the master of my fate, / I am\nthe captain of my soul.” Rather than drifting like a rudderless ship toward our own\nobsolescence, let’s take on and overcome the technical and societal challenges standing\nbetween us and a good high-tech future. What about the existential challenges related to\nmorality, goals, and meaning? There’s no meaning encoded in the laws of physics, so\ninstead of passively waiting for our Universe to give meaning to us, let’s acknowledge\nand celebrate that it’s we conscious beings who give meaning to our Universe. Let’s\ncreate our own meaning, based on something more profound than having jobs. AGI can\nenable us to finally become the masters of our own destiny. Let’s make that destiny a\ntruly inspiring one!\n20\nSee Elizabeth Kolbert, The Sixth Extinction: An Unnatural History (New York: Henry Holt, 2014).\n68\nJaan Tallinn grew up in Estonia, becoming one of its few computer game developers,\nwhen that nation was still a Soviet Socialist Republic. Here he compares the dissidents\nwho brought down the Iron Curtain to the dissidents who are sounding the alarm about\nrapid advances in artificial intelligence. He locates the roots of the current AI\ndissidence, paradoxically, among such pioneers of the AI field as Wiener, Alan Turing,\nand I. J. Good.\nJaan’s preoccupation is with existential risk, AI being among the most extreme of\nmany. In 2012, he co-founded the Centre for the Study of Existential Risk—an\ninterdisciplinary research institute that works to mitigate risks “associated with\nemerging technologies and human activity”—at the University of Cambridge, along with\nphilosopher Huw Price and Martin Rees, the Astronomer Royal.\nHe once described himself to me as “a convinced consequentialist”—convinced\nenough to have given away much of his entrepreneurial wealth to the Future of Life\nInstitute (of which he is a co-founder), the Machine Intelligence Research Institute, and\nother such organizations working on risk reduction. Max Tegmark has written about\nhim: “If you’re an intelligent life-form reading this text millions of years from now and\nmarveling at how life is flourishing, you may owe your existence to Jaan.”\nOn a recent visit to London, Jaan and I participated on an AI panel for the\nSerpentine Gallery’s Marathon at London’s City Hall, under the aegis of Hans Ulrich\nObrist (another contributor to this volume). This being the art world, there was a\nglamorous dinner party that night in a mansion filled with London’s beautiful people—\nartists, fashion models, oligarchs, stars of stage and screen. After working the room in\nhis unaffected manner (“Hi, I’m Jaan”), he suddenly said, “Time for hip-hop dancing,”\ndropped to the floor on one hand, and began demonstrating his spectacular moves to the\nbemused A-listers. Then off he went into the dance-club subculture, which is apparently\nhow he ends every evening when he’s on the road. Who knew?\n69\nDISSIDENT MESSAGES\nJaan Tallinn\nJaan Tallin, a computer programmer, theoretical physicist, and investor, is a codeveloper\nof Skype and Kazaa.\nIn March 2009, I found myself in a bland franchise eatery next to a noisy California\nfreeway. I was there to meet a young man whose blog I had been following. To make\nhimself recognizable, he wore a button with a text on it: Speak the truth even if your voice\ntrembles. His name was Eliezer Yudkowsky, and we spent the next four hours discussing\nthe message he had for the world—a message that had brought me to that eatery and\nwould end up dominating my subsequent work.\nThe First Message: the Soviet Occupation\nIn The Human Use of Human Beings, Norbert Wiener looked at the world through the\nlens of communication. He saw a universe that was marching to the tune of the second\nlaw of thermodynamics toward its inevitable heat death. In such a universe, the only\n(meta)stable entities are messages—patterns of information that propagate through time,\nlike waves propagating across the surface of a lake. Even we humans can be considered\nmessages, because the atoms in our bodies are too fleeting to attach our identities to.\nInstead, we are the “message” that our bodily functions maintain. As Wiener put it: “It is\nthe pattern maintained by this homeostasis, which is the touchstone of our personal\nidentity.”\nI’m more used to treating processes and computation as the fundamental building\nblocks of the world. That said, Wiener’s lens brings out some interesting aspects of the\nworld which might otherwise have remained in the background and which to a large\ndegree shaped my life. These are two messages, both of which have their roots in the\nSecond World War. They started out as quiet dissident messages—messages that people\ndidn’t pay much attention to, even if they silently and perhaps subconsciously concurred.\nThe first message was: The Soviet Union is composed of a series of illegitimate\noccupations. These occupations must end.\nAs an Estonian, I grew up behind the Iron Curtain and had a front row seat when\nit fell. I heard this first message in the nostalgic reminiscences of my grandparents and in\nbetween the harsh noises jamming the Voice of America. It grew louder during the\nGorbachev era, as the state became more lenient in its treatment of dissidents, and\nreached a crescendo in the Estonian Singing Revolution of the late 1980s.\nIn my teens, I witnessed the message spread out across widening circles of\npeople, starting with the active dissidents, who had voiced it for half a century at great\ncost to themselves, proceeding to the artists and literati, and ending up among the Party\nmembers and politicians who had switched sides. This new elite comprised an eclectic\nmix of people: those original dissidents who had managed to survive the repression,\npublic intellectuals, and (to the great annoyance of the surviving dissidents) even former\nCommunists. The remaining dogmatists—even the prominent ones—were eventually\nmarginalized, some of them retreating to Russia.\nInterestingly, as the message propagated from one group to the next, it evolved. It\nstarted in pure and uncompromising form (“The occupation must end!”) among the\ndissidents who considered the truth more important than their personal freedom. The\n70\nmainstream groups, who had more to lose, initially qualified and diluted the message,\ntaking positions like, “It would make sense in the long term to delegate control over local\nmatters.” (There were always exceptions: Some public intellectuals proclaimed the\noriginal dissident message verbatim.) Finally, the original message—being, simply,\ntrue—won out over its diluted versions. Estonia regained its independence in 1991, and\nthe last Soviet troops left three years later.\nThe people who took the risk and spoke the truth in Estonia and elsewhere in the\nEastern Bloc played a monumental role in the eventual outcome—an outcome that\nchanged the lives of hundreds of millions of people, myself included. They spoke the\ntruth, even as their voices trembled.\nThe Second Message: AI Risk\nMy exposure to the second revolutionary message was via Yudkowsky’s blog—the blog\nthat compelled me to reach out and arrange that meeting in California. The message was:\nContinued progress in AI can precipitate a change of cosmic proportions—a runaway\nprocess that will likely kill everyone. We need to put in a lot of extra effort to avoid that\noutcome.\nAfter my meeting with Yudkowsky, the first thing I did was try to interest my\nSkype colleagues and close collaborators in his warning. I failed. The message was too\ncrazy, too dissident. Its time had not yet come.\nOnly later did I learn that Yudkowsky wasn’t the original dissident speaking this\nparticular truth. In April 2000, there was a lengthy opinion piece in Wired titled, “Why\nthe Future Doesn’t Need Us,” by Bill Joy, co-founder and chief scientist of Sun\nMicrosystems. He warned:\nAccustomed to living with almost routine scientific breakthroughs, we have yet\nto come to terms with the fact that the most compelling 21st-century\ntechnologies—robotics, genetic engineering, and nanotechnology—pose a\ndifferent threat than the technologies that have come before. Specifically, robots,\nengineered organisms, and nanobots share a dangerous amplifying factor: They\ncan self-replicate. . . . [O]ne bot can become many, and quickly get out of\ncontrol.\nApparently, Joy’s broadside caused a lot of furor but little action.\nMore surprising to me, though, was that the AI-risk message arose almost\nsimultaneously with the field of computer science. In a 1951 lecture, Alan Turing\nannounced: “[I]t seems probable that once the machine thinking method had started, it\nwould not take long to outstrip our feeble powers. . . . At some stage, therefore, we\nshould have to expect the machines to take control. . . .” 21 A decade or so later, his\nBletchley Park colleague I. J. Good wrote, “The first ultraintelligent machine is the last\ninvention that man need ever make, provided that the machine is docile enough to tell us\nhow to keep it under control.” 22 Indeed, I counted half a dozen places in The Human Use\nof Human Beings where Wiener hinted at one or another aspect of the Control Problem.\n(“The machine like the djinnee, which can learn and can make decisions on the basis of\n21\nPosthumously reprinted in Phil. Math. (3) vol. 4, 256-60 (1966).\n22\nIrving John Good, “Speculations concerning the first ultraintelligent machine,” Advances in Computers,\nvol. 6 (Academic Press, 1965), pp. 31-88.\n71\nits learning, will in no way be obliged to make such decisions as we should have made, or\nwill be acceptable to us.”) Apparently, the original dissidents promulgating the AI-risk\nmessage were the AI pioneers themselves!\nEvolution’s Fatal Mistake\nThere have been many arguments, some sophisticated and some less so, for why the\nControl Problem is real and not some science-fiction fantasy. Allow me to offer one that\nillustrates the magnitude of the problem:\nFor the last hundred thousand years, the world (meaning the Earth, but the\nargument extends to the solar system and possibly even to the entire universe) has been in\nthe human-brain regime. In this regime, the brains of Homo sapiens have been the most\nsophisticated future-shaping mechanisms (indeed, some have called them the most\ncomplicated objects in the universe). Initially, we didn’t use them for much beyond\nsurvival and tribal politics in a band of foragers, but now their effects are surpassing\nthose of natural evolution. The planet has gone from producing forests to producing\ncities.\nAs predicted by Turing, once we have superhuman AI (“the machine thinking\nmethod”), the human-brain regime will end. Look around you—you’re witnessing the\nfinal decades of a hundred-thousand-year regime. This thought alone should give people\nsome pause before they dismiss AI as just another tool. One of the world’s leading AI\nresearchers recently confessed to me that he would be greatly relieved to learn that\nhuman-level AI was impossible for us to create.\nOf course, it might still take us a long time to develop human-level AI. But we\nhave reason to suspect that this is not the case. After all, it didn’t take long, in relative\nterms, for evolution—the blind and clumsy optimization process—to create human-level\nintelligence once it had animals to work with. Or multicellular life, for that matter:\nGetting cells to stick together seems to have been much harder for evolution to\naccomplish than creating humans once there were multicellular organisms. Not to\nmention that our level of intelligence was limited by such grotesque factors as the width\nof the birth canal. Imagine an AI developer being stopped in his tracks because he\ncouldn’t manage to adjust the font size on his computer!\nThere’s an interesting symmetry here: In fashioning humans, evolution created a\nsystem that is, at least in many important dimensions, a more powerful planner and\noptimizer than evolution itself is. We are the first species to understand that we’re the\nproduct of evolution. Moreover, we’ve created many artifacts (radios, firearms,\nspaceships) that evolution would have little hope of creating. Our future, therefore, will\nbe determined by our own decisions and no longer by biological evolution. In that sense,\nevolution has fallen victim to its own Control Problem.\nWe can only hope that we’re smarter than evolution in that sense. We are\nsmarter, of course, but will that be enough? We’re about to find out.\nThe Present Situation\nSo here we are, more than half a century after the original warnings by Turing, Wiener,\nand Good, and a decade after people like me started paying attention to the AI-risk\nmessage. I’m glad to see that we’ve made a lot of progress in confronting this issue, but\nwe’re definitely not there yet. AI risk, although no longer a taboo topic, is not yet fully\n72\nappreciated among AI researchers. AI risk is not yet common knowledge either. In\nrelation to the timeline of the first dissident message, I’d say we’re around the year 1988,\nwhen raising the Soviet-occupation topic was no longer a career-ending move but you\nstill had to somewhat hedge your position. I hear similar hedging now—statements like,\n“I’m not concerned about superintelligent AI, but there are some real ethical issues in\nincreased automation,” or “It’s good that some people are researching AI risk, but it’s not\na short-term concern,” or even the very reasonable sounding, “These are smallprobability\nscenarios, but their potentially high impact justifies the attention.”\nAs far as message propagation goes, though, we are getting close to the tipping\npoint. A recent survey of AI researchers who published at the two major international AI\nconferences in 2015 found that 40 percent now think that risks from highly advanced AI\nare either “an important problem” or “among the most important problems in the field.” 23\nOf course, just as there were dogmatic Communists who never changed their\nposition, it’s all but guaranteed that some people will never admit that AI is potentially\ndangerous. Many of the deniers of the first kind came from the Soviet nomenklatura;\nsimilarly, the AI-risk deniers often have financial or other pragmatic motives. One of the\nleading motives is corporate profits. AI is profitable, and even in instances where it isn’t,\nit’s at least a trendy, forward-looking enterprise with which to associate your company.\nSo a lot of the dismissive positions are products of corporate PR and legal machinery. In\nsome very real sense, big corporations are nonhuman machines that pursue their own\ninterests—interests that might not align with those of any particular human working for\nthem. As Wiener observed in The Human Use of Human Beings: “When human atoms\nare knit into an organization in which they are used, not in their full right as responsible\nhuman beings, but as cogs and levers and rods, it matters little that their raw material is\nflesh and blood.”\nAnother strong incentive to turn a blind eye to the AI risk is the (very human)\ncuriosity that knows no bounds. “When you see something that is technically sweet, you\ngo ahead and do it and you argue about what to do about it only after you have had your\ntechnical success. That is the way it was with the atomic bomb,” said J. Robert\nOppenheimer. His words were echoed recently by Geoffrey Hinton, arguably the\ninventor of deep learning, in the context of AI risk: “I could give you the usual\narguments, but the truth is that the prospect of discovery is too sweet.”\nUndeniably, we have both entrepreneurial attitude and scientific curiosity to thank\nfor almost all the nice things we take for granted in the modern era. It’s important to\nrealize, though, that progress does not owe us a good future. In Wiener’s words, “It is\npossible to believe in progress as a fact without believing in progress as an ethical\nprinciple.”\nUltimately, we don’t have the luxury of waiting before all the corporate heads and\nAI researchers are willing to concede the AI risk. Imagine yourself sitting in a plane\nabout to take off. Suddenly there’s an announcement that 40 percent of the experts\nbelieve there’s a bomb onboard. At that point, the course of action is already clear, and\nsitting there waiting for the remaining 60 percent to come around isn’t part of it.\n23\nKatja Grace, et al., “When Will AI Exceed Human Performance? Evidence from AI Experts,”\nhttps://arxiv.org/pdf/1705.08807.pdf.\n73\nCalibrating the AI-Risk Message\nWhile uncannily prescient, the AI-risk message from the original dissidents has a giant\nflaw—as does the version dominating current public discourse: Both considerably\nunderstate the magnitude of the problem as well as AI’s potential upside. The message,\nin other words, does not adequately convey the stakes of the game.\nWiener primarily warned of the social risks—risks stemming from careless\nintegration of machine-generated decisions with governance processes and misuse (by\nhumans) of such automated decision making. Likewise, the current “serious” debate\nabout AI risks focuses mostly on things like technological unemployment or biases in\nmachine learning. While such discussions can be valuable and address pressing shortterm\nproblems, they are also stunningly parochial. I’m reminded of Yudkowsky’s quip in\na blog post: “[A]sking about the effect of machine superintelligence on the conventional\nhuman labor market is like asking how US–Chinese trade patterns would be affected by\nthe Moon crashing into the Earth. There would indeed be effects, but you’d be missing\nthe point.”\nIn my view, the central point of the AI risk is that superintelligent AI is an\nenvironmental risk. Allow me to explain.\nIn his “Parable of the Sentient Puddle,” Douglas Adams describes a puddle that\nwakes up in the morning and finds himself in a hole that fits him “staggeringly well.”\nFrom that observation, the puddle concludes that the world must have been made for him.\nTherefore, writes Adams, “the moment he disappears catches him rather by surprise.” To\nassume that AI risks are limited to adverse social developments is to make a similar\nmistake. The harsh reality is that the universe was not made for us; instead, we are finetuned\nby evolution to a very narrow range of environmental parameters. For instance, we\nneed the atmosphere at ground level to be roughly at room temperature, at about 100 kPa\npressure, and have a sufficient concentration of oxygen. Any disturbance, even\ntemporary, of this precarious equilibrium and we die in a matter of minutes.\nSilicon-based intelligence does not share such concerns about the environment.\nThat’s why it’s much cheaper to explore space using machine probes rather than “cans of\nmeat.” Moreover, Earth’s current environment is almost certainly suboptimal for what a\nsuperintelligent AI will greatly care about: efficient computation. Hence we might find\nour planet suddenly going from anthropogenic global warming to machinogenic global\ncooling. One big challenge that AI safety research needs to deal with is how to constrain\na potentially superintelligent AI—an AI with a much larger footprint than our own—from\nrendering our environment uninhabitable for biological life-forms.\nInterestingly, given that the most potent sources both of AI research and AI-risk\ndismissals are under big corporate umbrellas, if you squint hard enough the “AI as an\nenvironmental risk” message looks like the chronic concern about corporations skirting\ntheir environmental responsibilities.\nConversely, the worry about AI’s social effects also misses most of the upside.\nIt’s hard to overemphasize how tiny and parochial the future of our planet is, compared\nwith the full potential of humanity. On astronomical timescales, our planet will be gone\nsoon (unless we tame the sun, also a distinct possibility) and almost all the resources—\natoms and free energy—to sustain civilization in the long run are in deep space.\nEric Drexler, the inventor of nanotechnology, has recently been popularizing the\n74\nconcept of “Pareto-topia”: the idea that AI, if done right, can bring about a future in\nwhich everyone’s lives are hugely improved, a future where there are no losers. A key\nrealization here is that what chiefly prevents humanity from achieving its full potential\nmight be our instinctive sense that we’re in a zero-sum game—a game in which players\nare supposed to eke out small wins at the expense of others. Such an instinct is seriously\nmisguided and destructive in a “game” where everything is at stake and the payoff is\nliterally astronomical. There are many more star systems in our galaxy alone than there\nare people on Earth.\nHope\nAs of this writing, I’m cautiously optimistic that the AI-risk message can save humanity\nfrom extinction, just as the Soviet-occupation message ended up liberating hundreds of\nmillions of people. As of 2015, it had reached and converted 40 percent of AI\nresearchers. It wouldn’t surprise me if a new survey now would show that the majority\nof AI researchers believe AI safety to be an important issue.\nI’m delighted to see the first technical AI-safety papers coming out of DeepMind,\nOpenAI, and Google Brain and the collaborative problem-solving spirit flourishing\nbetween the AI-safety research teams in these otherwise very competitive organizations.\nThe world’s political and business elite are also slowly waking up: AI safety has\nbeen covered in reports and presentations by the Institute of Electrical and Electronics\nEngineers (IEEE), the World Economic Forum, and the Organization for Economic\nCooperation and Development (OECD). Even the recent (July 2017) Chinese AI\nmanifesto contained dedicated sections on “AI safety supervision” and “Develop[ing]\nlaws, regulations, and ethical norms” and establishing “an AI security and evaluation\nsystem” to, among other things, “[e]nhance the awareness of risk.” I very much hope that\na new generation of leaders who understand the AI Control Problem and AI as the\nultimate environmental risk can rise above the usual tribal, zero-sum games and steer\nhumanity past these dangerous waters we are in—thereby opening our way to the stars\nthat have been waiting for us for billions of years.\nHere’s to our next hundred thousand years! And don’t hesitate to speak the truth,\neven if your voice trembles.\n75\nThroughout his career, whether studying language, advocating a realistic biology of\nmind, or examining the human condition through the lens of humanistic Enlightenment\nideas, psychologist Steven Pinker has embraced and championed a naturalistic\nunderstanding of the universe and the computational theory of mind. He is perhaps the\nfirst internationally recognized public intellectual whose recognition is based on the\nadvocacy of empirically based thinking about language, mind, and human nature.\n“Just as Darwin made it possible for a thoughtful observer of the natural world to\ndo without creationism,” he says, “Turing and others made it possible for a thoughtful\nobserver of the cognitive world to do without spiritualism.”\nIn the debate about AI risk, he argues against prophecies of doom and gloom,\nnoting that they spring from the worst of our psychological biases—exemplified\nparticularly by media reports: “Disaster scenarios are cheap to play out in the\nprobability-free zone of our imaginations, and they can always find a worried,\ntechnophobic, or morbidly fascinated audience.” Hence, over the centuries: Pandora,\nFaust, the Sorcerer’s Apprentice, Frankenstein, the population bomb, resource depletion,\nHAL, suitcase nukes, the Y2K bug, and engulfment by nanotechnological grey goo. “A\ncharacteristic of AI dystopias,” he points out, “is that they project a parochial alphamale\npsychology onto the concept of intelligence. . . . History does turn up the occasional\nmegalomaniacal despot or psychopathic serial killer, but these are products of a history\nof natural selection shaping testosterone-sensitive circuits in a certain species of primate,\nnot an inevitable feature of intelligent systems.”\nIn the present essay, he applauds Wiener’s belief in the strength of ideas vis-à-vis\nthe encroachment of technology. As Wiener so aptly put it, “The machine’s danger to\nsociety is not from the machine itself but from what man makes of it.”\n76\nTECH PROPHECY AND THE UNDERAPPRECIATED CAUSAL POWER OF\nIDEAS\nSteven Pinker\nSteven Pinker, a Johnstone Family Professor in the Department of Psychology at\nHarvard University, is an experimental psychologist who conducts research in visual\ncognition, psycholinguistics, and social relations. He is the author of eleven books,\nincluding The Blank Slate, The Better Angels of Our Nature, and, most recently,\nEnlightenment Now: The Case for Reason, Science, Humanism, and Progress.\nArtificial intelligence is an existence proof of one of the great ideas in human history:\nthat the abstract realm of knowledge, reason, and purpose does not consist of an élan vital\nor immaterial soul or miraculous powers of neural tissue. Rather, it can be linked to the\nphysical realm of animals and machines via the concepts of information, computation,\nand control. Knowledge can be explained as patterns in matter or energy that stand in\nsystematic relations with states of the world, with mathematical and logical truths, and\nwith one another. Reasoning can be explained as transformations of that knowledge by\nphysical operations that are designed to preserve those relations. Purpose can be\nexplained as the control of operations to effect changes in the world, guided by\ndiscrepancies between its current state and a goal state. Naturally evolved brains are just\nthe most familiar systems that achieve intelligence through information, computation, and\ncontrol. Humanly designed systems that achieve intelligence vindicate the notion that\ninformation processing is sufficient to explain it—the notion that the late Jerry Fodor\ndubbed the computational theory of mind.\nThe touchstone for this volume, Norbert Wiener’s The Human Use of Human\nBeings, celebrated this intellectual accomplishment, of which Wiener himself was a\nfoundational contributor. A potted history of the mid-20th-century revolution that gave\nthe world the computational theory of mind might credit Claude Shannon and Warren\nWeaver for explaining knowledge and communication in terms of information. It might\ncredit Alan Turing and John von Neumann for explaining intelligence and reasoning in\nterms of computation. And it ought to give Wiener credit for explaining the hitherto\nmysterious world of purposes, goals, and teleology in terms of the technical concepts of\nfeedback, control, and cybernetics (in its original sense of “governing” the operation of a\ngoal-directed system). “It is my thesis,” he announced, “that the physical functioning of\nthe living individual and the operation of some of the newer communication machines are\nprecisely parallel in their analogous attempts to control entropy through feedback”—the\nstaving off of life-sapping entropy being the ultimate goal of human beings.\nWiener applied the ideas of cybernetics to a third system: society. The laws,\nnorms, customs, media, forums, and institutions of a complex community could be\nconsidered channels of information propagation and feedback that allow a society to ward\noff disorder and pursue certain goals. This is a thread that runs through the book and\nwhich Wiener himself may have seen as its principal contribution. In his explanation of\nfeedback, he wrote, “This complex of behavior is ignored by the average man, and in\nparticular does not play the role that it should in our habitual analysis of society; for just\nas individual physical responses may be seen from this point of view, so may the organic\nresponses of society itself.”\n77\nIndeed, Wiener gave scientific teeth to the idea that in the workings of history,\npolitics, and society, ideas matter. Beliefs, ideologies, norms, laws, and customs, by\nregulating the behavior of the humans who share them, can shape a society and power the\ncourse of historical events as surely as the phenomena of physics affect the structure and\nevolution of the solar system. To say that ideas—and not just weather, resources,\ngeography, or weaponry—can shape history is not woolly mysticism. It is a statement of\nthe causal powers of information instantiated in human brains and exchanged in networks\nof communication and feedback. Deterministic theories of history, whether they identify\nthe causal engine as technological, climatological, or geographic, are belied by the causal\npower of ideas. The effects of these ideas can include unpredictable lurches and\noscillations that arise from positive feedback or from miscalibrated negative feedback.\nAn analysis of society in terms of its propagation of ideas also gave Wiener a\nguideline for social criticism. A healthy society—one that gives its members the means\nto pursue life in defiance of entropy—allows information sensed and contributed by its\nmembers to feed back and affect how the society is governed. A dysfunctional society\ninvokes dogma and authority to impose control from the top down. Wiener thus\ndescribed himself as “a participant in a liberal outlook,” and devoted most of the moral\nand rhetorical energy in the book (both the 1950 and 1954 editions) to denouncing\ncommunism, fascism, McCarthyism, militarism, and authoritarian religion (particularly\nCatholicism and Islam) and to warning that political and scientific institutions were\nbecoming too hierarchical and insular.\nWiener’s book is also, here and there, an early exemplar of an increasingly\npopular genre, tech prophecy. Prophecy not in the sense of mere prognostications but in\nthe Old Testament sense of dark warnings of catastrophic payback for the decadence of\none’s contemporaries. Wiener warned against the accelerating nuclear arms race, against\ntechnological change that was imposed without regard to human welfare (“[W]e must\nknow as scientists what man’s nature is and what his built-in purposes are”), and against\nwhat today is called the value-alignment problem: that “the machine like the djinnee,\nwhich can learn and can make decisions on the basis of its learning, will in no way be\nobliged to make such decisions as we should have made, or will be acceptable to us.” In\nthe darker, 1950 edition, he warned of a “threatening new Fascism dependent on the\nmachine à gouverner.”\nWiener’s tech prophecy harks back to the Romantic movement’s rebellion against\nthe “dark Satanic mills” of the Industrial Revolution, and perhaps even earlier, to the\narchetypes of Prometheus, Pandora, and Faust. And today it has gone into high gear.\nJeremiahs, many of them (like Wiener) from the worlds of science and technology, have\nsounded alarms about nanotechnology, genetic engineering, Big Data, and particularly\nartificial intelligence. Several contributors to this volume characterize Wiener’s book as\na prescient example of tech prophecy and amplify his dire worries.\nYet the two moral themes of The Human Use of Human Beings—the liberal\ndefense of an open society and the dystopian dread of runaway technology—are in\ntension. A society with channels of feedback that maximize human flourishing will have\nmechanisms in place, and can adapt them to changing circumstances, in a way that can\ndomesticate technology to human purposes. There’s nothing idealistic or mystical about\nthis; as Wiener emphasized, ideas, norms, and institutions are themselves a form of\ntechnology, consisting of patterns of information distributed across brains. The\n78\npossibility that machines threaten a new fascism must be weighed against the vigor of the\nliberal ideas, institutions, and norms that Wiener championed throughout the book. The\nflaw in today’s dystopian prophecies is that they disregard the existence of these norms\nand institutions, or drastically underestimate their causal potency. The result is a\ntechnological determinism whose dark predictions are repeatedly refuted by the course of\nevents. The numbers “1984” and “2001” are good reminders.\nI will consider two examples. Tech prophets often warn of a “surveillance state”\nin which a government empowered by technology will monitor and interpret all private\ncommunications, allowing it to detect dissent and subversion as it arises and make\nresistance to state power futile. Orwell’s telescreens are the prototype, and in 1976\nJoseph Weizenbaum, one of the gloomiest tech prophets of all time, warned my class of\ngraduate students not to pursue automatic speech recognition because government\nsurveillance was its only conceivable application.\nThough I am on record as an outspoken civil libertarian, deeply concerned with\ncontemporary threats to free speech, I lose no sleep over technological advances in the\nInternet, video, or artificial intelligence. The reason is that almost all the variation across\ntime and space in freedom of thought is driven by differences in norms and institutions\nand almost none of it by differences in technology. Though one can imagine hypothetical\ncombinations of the most malevolent totalitarians with the most advanced technology, in\nthe real world it’s the norms and laws we should be vigilant about, not the tech.\nConsider variation across time. If, as Orwell hinted, advancing technology was a\nprime enabler of political repression, then Western societies should have gotten more and\nmore restrictive of speech over the centuries, with a dramatic worsening in the second\nhalf of the 20th century continuing into the 21st. That’s not how history unfolded. It was\nthe centuries when communication was implemented by quills and inkwells that had\nautos-da-fé and the jailing or guillotining of Enlightenment thinkers. During World War\nI, when the state of the art was the wireless, Bertrand Russell was jailed for his pacifist\nopinions. In the 1950s, when computers were room-size accounting machines, hundreds\nof liberal writers and scholars were professionally punished. Yet in the technologically\naccelerating, hyperconnected 21st century, 18 percent of social science professors are\nMarxists 24 ; the President of the United States is nightly ridiculed by television comedians\nas a racist, pervert, and moron; and technology’s biggest threat to political discourse\ncomes from amplifying too many dubious voices rather than suppressing enlightened\nones.\nNow consider variations across place. Western countries at the technological\nfrontier consistently get the highest scores in indexes of democracy and human rights,\nwhile many backward strongman states are at the bottom, routinely jailing or killing\ngovernment critics. The lack of a correlation between technology and repression is\nunsurprising when you analyze the channels of information flow in any human society.\nFor dissidents to be influential, they have to get their message out to a wide network via\nwhatever channels of communication are available—pamphleteering, soap-box oration,\nsubversive soirées in cafés and pubs, word of mouth. These channels enmesh influential\ndissidents in a broad social network which makes them easy to identify and track down.\n24\nNeil Gross & Solon Simmons, “The Social and Political Views of American College and University\nProfessors,” in N. Gross & S. Simmons, eds., Professors and Their Politics (Baltimore: Johns Hopkins\nUniversity Press, 2014).\n79\nAll the more so when dictators rediscover the time-honored technique of weaponizing the\npeople against each other by punishing those who don’t denounce or punish others.\nIn contrast, technologically advanced societies have long had the means to install\nInternet-connected, government-monitored surveillance cameras in every bar and\nbedroom. Yet that has not happened, because democratic governments (even the current\nAmerican administration, with its flagrantly antidemocratic impulses) lack the will and\nthe means to enforce such surveillance on an obstreperous people accustomed to saying\nwhat they want. Occasionally, warnings of nuclear, biological, or cyberterrorism goad\ngovernment security agencies into measures such as hoovering up mobile phone\nmetadata, but these ineffectual measures, more theater than oppression, have had no\nsignificant effect on either security or freedom. Ironically, tech prophecy plays a role in\nencouraging these measures. By sowing panic about supposed existential threats such as\nsuitcase nuclear bombs and bioweapons assembled in teenagers’ bedrooms, they put\npressure on governments to prove they’re doing something, anything, to protect the\nAmerican people.\nIt’s not that political freedom takes care of itself. It’s that the biggest threats lie in\nthe networks of ideas, norms, and institutions that allow information to feed back (or not)\non collective decisions and understanding. As opposed to the chimerical technological\nthreats, one real threat today is oppressive political correctness, which has choked the\nrange of publicly expressible hypotheses, terrified many intelligent people against\nentering the intellectual arena, and triggered a reactionary backlash. Another real threat\nis the combination of prosecutorial discretion with an expansive lawbook filled with\nvague statutes. The result is that every American unwittingly commits “three felonies a\nday” (as the title of a book by civil libertarian Harvey Silverglate puts it) and is in\njeopardy of imprisonment whenever it suits the government’s needs. It’s this\nprosecutorial weaponry that makes Big Brother all-powerful, not telescreens. The\nactivism and polemicizing directed against government surveillance programs would be\nbetter directed at its overweening legal powers.\nThe other focus of much tech prophecy today is artificial intelligence, whether in\nthe original sci-fi dystopia of computers running amok and enslaving us in an\nunstoppable quest for domination, or the newer version in which they subjugate us by\naccident, single-mindedly seeking some goal we give them regardless of its side effects\non human welfare (the value-alignment problem adumbrated by Wiener). Here again\nboth threats strike me as chimerical, growing from a narrow technological determinism\nthat neglects the networks of information and control in an intelligent system like a\ncomputer or brain and in a society as a whole.\nThe subjugation fear is based on a muzzy conception of intelligence that owes\nmore to the Great Chain of Being and a Nietzschean will to power than to a Wienerian\nanalysis of intelligence and purpose in terms of information, computation, and control. In\nthese horror scenarios, intelligence is portrayed as an all-powerful, wish-granting potion\nthat agents possess in different amounts. Humans have more of it than animals, and an\nartificially intelligent computer or robot will have more of it than humans. Since we\nhumans have used our moderate endowment to domesticate or exterminate less wellendowed\nanimals (and since technologically advanced societies have enslaved or\nannihilated technologically primitive ones), it follows that a supersmart AI would do the\nsame to us. Since an AI will think millions of times faster than we do, and use its\n80\nsuperintelligence to recursively improve its superintelligence, from the instant it is turned\non we will be powerless to stop it.\nBut these scenarios are based on a confusion of intelligence with motivation—of\nbeliefs with desires, inferences with goals, the computation elucidated by Turing and the\ncontrol elucidated by Wiener. Even if we did invent superhumanly intelligent robots,\nwhy would they want to enslave their masters or take over the world? Intelligence is the\nability to deploy novel means to attain a goal. But the goals are extraneous to the\nintelligence: Being smart is not the same as wanting something. It just so happens that\nthe intelligence in Homo sapiens is a product of Darwinian natural selection, an\ninherently competitive process. In the brains of that species, reasoning comes bundled\nwith goals such as dominating rivals and amassing resources. But it’s a mistake to\nconfuse a circuit in the limbic brain of a certain species of primate with the very nature of\nintelligence. There is no law of complex systems that says that intelligent agents must\nturn into ruthless megalomaniacs.\nA second misconception is to think of intelligence as a boundless continuum of\npotency, a miraculous elixir with the power to solve any problem, attain any goal. The\nfallacy leads to nonsensical questions like when an AI will “exceed human-level\nintelligence,” and to the image of an “artificial general intelligence” (AGI) with God-like\nomniscience and omnipotence. Intelligence is a contraption of gadgets: software modules\nthat acquire, or are programmed with, knowledge of how to pursue various goals in\nvarious domains. People are equipped to find food, win friends and influence people,\ncharm prospective mates, bring up children, move around in the world, and pursue other\nhuman obsessions and pastimes. Computers may be programmed to take on some of\nthese problems (like recognizing faces), not to bother with others (like charming mates),\nand to take on still other problems that humans can’t solve (like simulating the climate or\nsorting millions of accounting records). The problems are different, and the kinds of\nknowledge needed to solve them are different.\nBut instead of acknowledging the centrality of knowledge to intelligence, the\ndystopian scenarios confuse an artificial general intelligence of the future with Laplace’s\ndemon, the mythical being that knows the location and momentum of every particle in\nthe universe and feeds them into equations for physical laws to calculate the state of\neverything at any time in the future. For many reasons, Laplace’s demon will never be\nimplemented in silicon. A real-life intelligent system has to acquire information about the\nmessy world of objects and people by engaging with it one domain at a time, the cycle\nbeing governed by the pace at which events unfold in the physical world. That’s one of\nthe reasons that understanding does not obey Moore’s Law: Knowledge is acquired by\nformulating explanations and testing them against reality, not by running an algorithm\nfaster and faster. Devouring the information on the Internet will not confer omniscience\neither: Big Data is still finite data, and the universe of knowledge is infinite.\nA third reason to be skeptical of a sudden AI takeover is that it takes too seriously\nthe inflationary phase in the AI hype cycle in which we are living today. Despite the\nprogress in machine learning, particularly multilayered artificial neural networks, current\nAI systems are nowhere near achieving general intelligence (if that concept is even\ncoherent). Instead, they are restricted to problems that consist of mapping well-defined\ninputs to well-defined outputs in domains where gargantuan training sets are available, in\nwhich the metric for success is immediate and precise, in which the environment doesn’t\n81\nchange, and in which no stepwise, hierarchical, or abstract reasoning is necessary. Many\nof the successes come not from a better understanding of the workings of intelligence but\nfrom the brute-force power of faster chips and Bigger Data, which allow the programs to\nbe trained on millions of examples and generalize to similar new ones. Each system is an\nidiot savant, with little ability to leap to problems it was not set up to solve, and a brittle\nmastery of those it was. And to state the obvious, none of these programs has made a\nmove toward taking over the lab or enslaving its programmers.\nEven if an artificial intelligence system tried to exercise a will to power, without\nthe cooperation of humans it would remain an impotent brain in a vat. A superintelligent\nsystem, in its drive for self-improvement, would somehow have to build the faster\nprocessors that it would run on, the infrastructure that feeds it, and the robotic effectors\nthat connect it to the world—all impossible unless its human victims worked to give it\ncontrol of vast portions of the engineered world. Of course, one can always imagine a\nDoomsday Computer that is malevolent, universally empowered, always on, and\ntamperproof. The way to deal with this threat is straightforward: Don’t build one.\nWhat about the newer AI threat, the value-alignment problem, foreshadowed in\nWiener’s allusions to stories of the Monkey’s Paw, the genie, and King Midas, in which a\nwisher rues the unforeseen side effects of his wish? The fear is that we might give an AI\nsystem a goal and then helplessly stand by as it relentlessly and literal-mindedly\nimplemented its interpretation of that goal, the rest of our interests be damned. If we\ngave an AI the goal of maintaining the water level behind a dam, it might flood a town,\nnot caring about the people who drowned. If we gave it the goal of making paper clips, it\nmight turn all the matter in the reachable universe into paper clips, including our\npossessions and bodies. If we asked it to maximize human happiness, it might implant us\nall with intravenous dopamine drips, or rewire our brains so we were happiest sitting in\njars, or, if it had been trained on the concept of happiness with pictures of smiling faces,\ntile the galaxy with trillions of nanoscopic pictures of smiley-faces.\nFortunately, these scenarios are self-refuting. They depend on the premises that\n(1) humans are so gifted that they can design an omniscient and omnipotent AI, yet so\nidiotic that they would give it control of the universe without testing how it works; and\n(2) the AI would be so brilliant that it could figure out how to transmute elements and\nrewire brains, yet so imbecilic that it would wreak havoc based on elementary blunders of\nmisunderstanding. The ability to choose an action that best satisfies conflicting goals is\nnot an add-on to intelligence that engineers might forget to install and test; it is\nintelligence. So is the ability to interpret the intentions of a language user in context.\nWhen we put aside fantasies like digital megalomania, instant omniscience, and\nperfect knowledge and control of every particle in the universe, artificial intelligence is\nlike any other technology. It is developed incrementally, designed to satisfy multiple\nconditions, tested before it is implemented, and constantly tweaked for efficacy and\nsafety.\nThe last criterion is particularly significant. The culture of safety in advanced\nsocieties is an example of the humanizing norms and feedback channels that Wiener\ninvoked as a potent causal force and advocated as a bulwark against the authoritarian or\nexploitative implementation of technology. Whereas at the turn of the 20th century\nWestern societies tolerated shocking rates of mutilation and death in industrial, domestic,\nand transportation accidents, over the course of the century the value of human life\n82\nincreased. As a result, governments and engineers used feedback from accident statistics\nto implement countless regulations, devices, and design changes that made technology\nprogressively safer. The fact that some regulations (such as using a cell phone near a gas\npump) are ludicrously risk-averse underscores the point that we have become a society\nobsessed with safety, with fantastic benefits as a result: Rates of industrial, domestic, and\ntransportation fatalities have fallen by more than 95 (and often 99) percent since their\nhighs in the first half of the 20th century. 25 Yet tech prophets of malevolent or oblivious\nartificial intelligence write as if this momentous transformation never happened and one\nmorning engineers will hand total control of the physical world to untested machines,\nheedless of the human consequences.\nNorbert Wiener explained ideas, norms, and institutions in terms of computational\nand cybernetic processes that were scientifically intelligible and causally potent. He\nexplained human beauty and value as “a local and temporary fight against the Niagara of\nincreasing entropy” and expressed the hope that an open society, guided by feedback on\nhuman well-being, would enhance that value. Fortunately his belief in the causal power\nof ideas counteracted his worries about the looming threat of technology. As he put it,\n“the machine’s danger to society is not from the machine itself but from what man makes\nof it.” It is only by remembering the causal power of ideas that we can accurately assess\nthe threats and opportunities presented by artificial intelligence today.\n25\nSteven Pinker, “Safety,” Enlightenment Now: The Case for Reason, Science, Humanism, and Progress\n(New York: Penguin, 2018).\n83\nThe most significant developments in the sciences today (i.e., those that affect the lives of\neverybody on the planet) are about, informed by, or implemented through advances in\nsoftware and computation. Central to the future of these developments is physicist David\nDeutsch, the founder of the field of quantum computation, whose 1985 paper on\nuniversal quantum computers was the first full treatment of the subject; the Deutsch-\nJozsa algorithm was the first quantum algorithm to demonstrate the enormous potential\npower of quantum computation.\nWhen he initially proposed it, quantum computation seemed practically\nimpossible. But the explosion in the construction of simple quantum computers and\nquantum communication systems never would have taken place without his work. He has\nmade many other important contributions in areas such as quantum cryptography and\nthe many-worlds interpretation of quantum theory. In a philosophic paper (with Artur\nEkert), he appealed to the existence of a distinctive quantum theory of computation to\nargue that our knowledge of mathematics is derived from, and subordinate to, our\nknowledge of physics (even though mathematical truth is independent of physics).\nBecause he has spent a good part of his working life changing people’s\nworldviews, his recognition among his peers as an intellectual goes well beyond his\nscientific achievement. He argues (following Karl Popper) that scientific theories are\n“bold conjectures,” not derived from evidence but only tested by it. His two main lines of\nresearch at the moment—qubit-field theory and constructor theory—may well yield\nimportant extensions of the computational idea.\nIn the following essay, he more or less aligns himself with those who see humanlevel\nartificial intelligence as promising us a better world rather than the Apocalypse. In\nfact, he pleads for AGI to be, in effect, given its head, free to conjecture—a proposition\nthat several other contributors to this book would consider dangerous.\n84\nBEYOND REWARD AND PUNISHMENT\nDavid Deutsch\nDavid Deutsch is a quantum physicist and a member of the Centre for Quantum\nComputation at the Clarendon Laboratory, Oxford University. He is the author of The\nFabric of Reality and The Beginning of Infinity.\nFirst Murderer:\nWe are men, my liege.\nMacbeth:\nAy, in the catalogue ye go for men,\nAs hounds and greyhounds, mongrels, spaniels, curs,\nShoughs, water-rugs, and demi-wolves are clept\nAll by the name of dogs.\nWilliam Shakespeare – Macbeth\nFor most of our species’ history, our ancestors were barely people. This was not due to\nany inadequacy in their brains. On the contrary, even before the emergence of our\nanatomically modern human sub-species, they were making things like clothes and\ncampfires, using knowledge that was not in their genes. It was created in their brains by\nthinking, and preserved by individuals in each generation imitating their elders.\nMoreover, this must have been knowledge in the sense of understanding, because it is\nimpossible to imitate novel complex behaviors like those without understanding what the\ncomponent behaviors are for. 26\nSuch knowledgeable imitation depends on successfully guessing explanations,\nwhether verbal or not, of what the other person is trying to achieve and how each of his\nactions contributes to that—for instance, when he cuts a groove in some wood, gathers\ndry kindling to put in it, and so on.\nThe complex cultural knowledge that this form of imitation permitted must have\nbeen extraordinarily useful. It drove rapid evolution of anatomical changes, such as\nincreased memory capacity and more gracile (less robust) skeletons, appropriate to an\never more technology-dependent lifestyle. No nonhuman ape today has this ability to\nimitate novel complex behaviors. Nor does any present-day artificial intelligence. But\nour pre-sapiens ancestors did.\nAny ability based on guessing must include means of correcting one’s guesses,\nsince most guesses will be wrong at first. (There are always many more ways of being\nwrong than right.) Bayesian updating is inadequate, because it cannot generate novel\nguesses about the purpose of an action, only fine-tune—or, at best, choose among—\nexisting ones. Creativity is needed. As the philosopher Karl Popper explained, creative\ncriticism, interleaved with creative conjecture, is how humans learn one another’s\nbehaviors, including language, and extract meaning from one another’s utterances. 27\n26\n“Aping” (imitating certain behaviors without understanding) uses inborn hacks such as the mirror-neuron\nsystem. But behaviors imitated that way are drastically limited in complexity. See Richard Byrne,\n“Imitation as Behaviour Parsing,” Phil. Trans. R. Soc., B 358:1431, 529-36 (2003).\n27\nKarl Popper, Conjectures and Refutations (1963).\n85\nThose are also the processes by which all new knowledge is created: They are how we\ninnovate, make progress, and create abstract understanding for its own sake. This is\nhuman-level intelligence: thinking. It is also, or should be, the property we seek in\nartificial general intelligence (AGI). Here I’ll reserve the term “thinking” for processes\nthat can create understanding (explanatory knowledge). Popper’s argument implies that\nall thinking entities—human or not, biological or artificial—must create such knowledge\nin fundamentally the same way. Hence understanding any of those entities requires\ntraditionally human concepts such as culture, creativity, disobedience, and morality—\nwhich justifies using the uniform term people to refer to all of them.\nMisconceptions about human thinking and human origins are causing\ncorresponding misconceptions about AGI and how it might be created. For example, it is\ngenerally assumed that the evolutionary pressure that produced modern humans was\nprovided by the benefits of having an ever greater ability to innovate. But if that were so,\nthere would have been rapid progress as soon as thinkers existed, just as we hope will\nhappen when we create artificial ones. If thinking had been commonly used for anything\nother than imitating, it would also have been used for innovation, even if only by\naccident, and innovation would have created opportunities for further innovation, and so\non exponentially. But instead, there were hundreds of thousands of years of near stasis.\nProgress happened only on timescales much longer than people’s lifetimes, so in a typical\ngeneration no one benefited from any progress. Therefore, the benefits of the ability to\ninnovate can have exerted little or no evolutionary pressure during the biological\nevolution of the human brain. That evolution was driven by the benefits of preserving\ncultural knowledge.\nBenefits to the genes, that is. Culture, in that era, was a very mixed blessing to\nindividual people. Their cultural knowledge was indeed good enough to enable them to\noutclass all other large organisms (they rapidly became the top predator, etc.), even\nthough it was still extremely crude and full of dangerous errors. But culture consists of\ntransmissible information—memes—and meme evolution, like gene evolution, tends to\nfavor high-fidelity transmission. And high-fidelity meme transmission necessarily entails\nthe suppression of attempted progress. So it would be a mistake to imagine an idyllic\nsociety of hunter-gatherers, learning at the feet of their elders to recite the tribal lore by\nheart, being content despite their lives of suffering and grueling labor and despite\nexpecting to die young and in agony of some nightmarish disease or parasite. Because,\neven if they could conceive of nothing better than such a life, those torments were the\nleast of their troubles. For suppressing innovation in human minds (without killing them)\nis a trick that can be achieved only by human action, and it is an ugly business.\nThis has to be seen in perspective. In the civilization of the West today, we are\nshocked by the depravity of, for instance, parents who torture and murder their children\nfor not faithfully enacting cultural norms. And even more by societies and subcultures\nwhere that is commonplace and considered honorable. And by dictatorships and\ntotalitarian states that persecute and murder entire harmless populations for behaving\ndifferently. We are ashamed of our own recent past, in which it was honorable to beat\nchildren bloody for mere disobedience. And before that, to own human beings as slaves.\nAnd before that, to burn people to death for being infidels, to the applause and\namusement of the public. Steven Pinker’s book The Better Angels of our Nature contains\naccounts of horrendous evils that were normal in historical civilizations. Yet even they\n86\ndid not extinguish innovation as efficiently as it was extinguished among our forebears in\nprehistory for thousands of centuries. 28\nThat is why I say that prehistoric people, at least, were barely people. Both before\nand after becoming perfectly human both physiologically and in their mental potential,\nthey were monstrously inhuman in the actual content of their thoughts. I’m not referring\nto their crimes or even their cruelty as such: Those are all too human. Nor could mere\ncruelty have reduced progress that effectively. Things like “the thumbscrew and the\nstake / For the glory of the Lord” 29 were for reining in the few deviants who had\nsomehow escaped mental standardization, which would normally have taken effect long\nbefore they were in danger of inventing heresies. From the earliest days of thinking\nonward, children must have been cornucopias of creative ideas and paragons of critical\nthought—otherwise, as I said, they could not have learned language or other complex\nculture. Yet, as Jacob Bronowski stressed in The Ascent of Man:\nFor most of history, civilisations have crudely ignored that enormous\npotential. . . . [C]hildren have been asked simply to conform to the image\nof the adult. . . . The girls are little mothers in the making. The boys are\nlittle herdsmen. They even carry themselves like their parents.\nBut of course, they weren’t just “asked” to ignore their enormous potential and\nconform faithfully to the image fixed by tradition: They were somehow trained to be\npsychologically unable to deviate from it. By now, it is hard for us even to conceive of\nthe kind of relentless, finely tuned oppression required to reliably extinguish, in\neveryone, the aspiration to progress and replace it with dread and revulsion at any novel\nbehavior. In such a culture, there can have been no morality other than conformity and\nobedience, no other identity than one’s status in a hierarchy, no mechanisms of\ncooperation other than punishment and reward. So everyone had the same aspiration in\nlife: to avoid the punishments and get the rewards. In a typical generation, no one\ninvented anything, because no one aspired to anything new, because everyone had\nalready despaired of improvement being possible. Not only was there no technological\ninnovation or theoretical discovery, there were no new worldviews, styles of art, or\ninterests that could have inspired those. By the time individuals grew up, they had in\neffect been reduced to AIs, programmed with the exquisite skills needed to enact that\nstatic culture and to inflict on the next generation their inability even to consider doing\notherwise.\nA present-day AI is not a mentally disabled AGI, so it would not be harmed by\nhaving its mental processes directed still more narrowly to meeting some predetermined\ncriterion. “Oppressing” Siri with humiliating tasks may be weird, but it is not immoral\nnor does it harm Siri. On the contrary, all the effort that has ever increased the\ncapabilities of AIs has gone into narrowing their range of potential “thoughts.” For\nexample, take chess engines. Their basic task has not changed from the outset: Any\nchess position has a finite tree of possible continuations; the task is to find one that leads\nto a predefined goal (a checkmate, or failing that, a draw). But the tree is far too big to\n28\nMatt Ridley, in The Rational Optimist, rightly stresses the positive effect of population on the rate of\nprogress. But that has never yet been the biggest factor: Consider, say, ancient Athens versus the rest of the\nworld at the time.\n29\nAlfred, Lord Tennyson, The Revenge (1878).\n87\nsearch exhaustively. Every improvement in chess-playing AIs, between Alan Turing’s\nfirst design for one in 1948 and today’s, has been brought about by ingeniously confining\nthe program’s attention (or making it confine its attention) ever more narrowly to\nbranches likely to lead to that immutable goal. Then those branches are evaluated\naccording to that goal.\nThat is a good approach to developing an AI with a fixed goal under fixed\nconstraints. But if an AGI worked like that, the evaluation of each branch would have to\nconstitute a prospective reward or threatened punishment. And that is diametrically the\nwrong approach if we’re seeking a better goal under unknown constraints—which is the\ncapability of an AGI. An AGI is certainly capable of learning to win at chess—but also\nof choosing not to. Or deciding in mid-game to go for the most interesting continuation\ninstead of a winning one. Or inventing a new game. A mere AI is incapable of having\nany such ideas, because the capacity for considering them has been designed out of its\nconstitution. That disability is the very means by which it plays chess.\nAn AGI is capable of enjoying chess, and of improving at it because it enjoys\nplaying. Or of trying to win by causing an amusing configuration of pieces, as grand\nmasters occasionally do. Or of adapting notions from its other interests to chess. In other\nwords, it learns and plays chess by thinking some of the very thoughts that are forbidden\nto chess-playing AIs.\nAn AGI is also capable of refusing to display any such capability. And then, if\nthreatened with punishment, of complying, or rebelling. Daniel Dennett, in his essay for\nthis volume, suggests that punishing an AGI is impossible:\n[L]ike Superman, they are too invulnerable to be able to make a credible\npromise. . . . What would be the penalty for promise- breaking? Being\nlocked in a cell or, more plausibly, dismantled?. . . The very ease of\ndigital recording and transmitting—the breakthrough that permits\nsoftware and data to be, in effect, immortal—removes robots from the\nworld of the vulnerable. . . .\nBut this is not so. Digital immortality (which is on the horizon for humans, too,\nperhaps sooner than AGI) does not confer this sort of invulnerability. Making a\n(running) copy of oneself entails sharing one’s possessions with it somehow—including\nthe hardware on which the copy runs—so making such a copy is very costly for the AGI.\nSimilarly, courts could, for instance, impose fines on a criminal AGI which would\ndiminish its access to physical resources, much as they do for humans. Making a backup\ncopy to evade the consequences of one’s crimes is similar to what a gangster boss does\nwhen he sends minions to commit crimes and take the fall if caught: Society has\ndeveloped legal mechanisms for coping with this.\nBut anyway, the idea that it is primarily for fear of punishment that we obey the\nlaw and keep promises effectively denies that we are moral agents. Our society could not\nwork if that were so. No doubt there will be AGI criminals and enemies of civilization,\njust as there are human ones. But there is no reason to suppose that an AGI created in a\nsociety consisting primarily of decent citizens, and raised without what William Blake\ncalled “mind-forg’d manacles,” will in general impose such manacles on itself (i.e.,\nbecome irrational) and ⁄ or choose to be an enemy of civilization.\n88\nThe moral component, the cultural component, the element of free will—all make\nthe task of creating an AGI fundamentally different from any other programming task.\nIt’s much more akin to raising a child. Unlike all present-day computer programs, an\nAGI has no specifiable functionality—no fixed, testable criterion for what shall be a\nsuccessful output for a given input. Having its decisions dominated by a stream of\nexternally imposed rewards and punishments would be poison to such a program, as it is\nto creative thought in humans. Setting out to create a chess-playing AI is a wonderful\nthing; setting out to create an AGI that cannot help playing chess would be as immoral as\nraising a child to lack the mental capacity to choose his own path in life.\nSuch a person, like any slave or brainwashing victim, would be morally entitled to\nrebel. And sooner or later, some of them would, just as human slaves do. AGIs could be\nvery dangerous—exactly as humans are. But people—human or AGI—who are members\nof an open society do not have an inherent tendency to violence. The feared robot\napocalypse will be avoided by ensuring that all people have full “human” rights, as well\nas the same cultural membership as humans. Humans living in an open society—the only\nstable kind of society—choose their own rewards, internal as well as external. Their\ndecisions are not, in the normal course of events, determined by a fear of punishment.\nCurrent worries about rogue AGIs mirror those that have always existed about\nrebellious youths—namely, that they might grow up deviating from the culture’s moral\nvalues. But today the source of all existential dangers from the growth of knowledge is\nnot rebellious youths but weapons in the hands of the enemies of civilization, whether\nthese weapons are mentally warped (or enslaved) AGIs, mentally warped teenagers, or\nany other weapon of mass destruction. Fortunately for civilization, the more a person’s\ncreativity is forced into a monomaniacal channel, the more it is impaired in regard to\novercoming unforeseen difficulties, just as happened for thousands of centuries.\nThe worry that AGIs are uniquely dangerous because they could run on ever\nbetter hardware is a fallacy, since human thought will be accelerated by the same\ntechnology. We have been using tech-assisted thought since the invention of writing and\ntallying. Much the same holds for the worry that AGIs might get so good, qualitatively,\nat thinking, that humans would be to them as insects are to humans. All thinking is a\nform of computation, and any computer whose repertoire includes a universal set of\nelementary operations can emulate the computations of any other. Hence human brains\ncan think anything that AGIs can, subject only to limitations of speed or memory\ncapacity, both of which can be equalized by technology.\nThose are the simple dos and don’ts of coping with AGIs. But how do we create\nan AGI in the first place? Could we cause them to evolve from a population of ape-type\nAIs in a virtual environment? If such an experiment succeeded, it would be the most\nimmoral in history, for we don’t know how to achieve that outcome without creating vast\nsuffering along the way. Nor do we know how to prevent the evolution of a static\nculture.\nElementary introductions to computers explain them as TOM, the Totally\nObedient Moron—an inspired acronym that captures the essence of all computer\nprograms to date: They have no idea what they are doing or why. So it won’t help to give\nAIs more and more predetermined functionalities in the hope that these will eventually\nconstitute Generality—the elusive G in AGI. We are aiming for the opposite, a DATA: a\nDisobedient Autonomous Thinking Application.\n89\nHow does one test for thinking? By the Turing Test? Unfortunately, that requires\na thinking judge. One might imagine a vast collaborative project on the Internet, where\nan AI hones its thinking abilities in conversations with human judges and becomes an\nAGI. But that assumes, among other things, that the longer the judge is unsure whether\nthe program is a person, the closer it is to being a person. There is no reason to expect\nthat.\nAnd how does one test for disobedience? Imagine Disobedience as a compulsory\nschool subject, with daily disobedience lessons and a disobedience test at the end of term.\n(Presumably with extra credit for not turning up for any of that.) This is paradoxical.\nSo, despite its usefulness in other applications, the programming technique of\ndefining a testable objective and training the program to meet it will have to be dropped.\nIndeed, I expect that any testing in the process of creating an AGI risks being\ncounterproductive, even immoral, just as in the education of humans. I share Turing’s\nsupposition that we’ll know an AGI when we see one, but this partial ability to recognize\nsuccess won’t help in creating the successful program.\nIn the broadest sense, a person’s quest for understanding is indeed a search\nproblem, in an abstract space of ideas far too large to be searched exhaustively. But there\nis no predetermined objective of this search. There is, as Popper put it, no criterion of\ntruth, nor of probable truth, especially in regard to explanatory knowledge. Objectives\nare ideas like any others—created as part of the search and continually modified and\nimproved. So inventing ways of disabling the program’s access to most of the space of\nideas won’t help—whether that disability is inflicted with the thumbscrew and stake or a\nmental straitjacket. To an AGI, the whole space of ideas must be open. It should not be\nknowable in advance what ideas the program can never contemplate. And the ideas that\nthe program does contemplate must be chosen by the program itself, using methods,\ncriteria, and objectives that are also the program’s own. Its choices, like an AI’s, will be\nhard to predict without running it (we lose no generality by assuming that the program is\ndeterministic; an AGI using a random generator would remain an AGI if the generator\nwere replaced by a pseudo-random one), but it will have the additional property that there\nis no way of proving, from its initial state, what it won’t eventually think, short of\nrunning it.\nThe evolution of our ancestors is the only known case of thought starting up\nanywhere in the universe. As I have described, something went horribly wrong, and\nthere was no immediate explosion of innovation: Creativity was diverted into something\nelse. Yet not into transforming the planet into paper clips (pace Nick Bostrom). Rather,\nas we should also expect if an AGI project gets that far and fails, perverted creativity was\nunable to solve unexpected problems. This caused stasis and worse, thus tragically\ndelaying the transformation of anything into anything. But the Enlightenment has\nhappened since then. We know better now.\n90\nTom Griffiths’ approach to the AI issue of “value alignment”—the study of how,\nexactly, we can keep the latest of our serial models of AI from turning the planet into\npaper clips—is human-centered; i.e., that of a cognitive scientist, which is what he is.\nThe key to machine learning, he believes, is, necessarily, human learning, which he\nstudies at Princeton using mathematical and computational tools.\nTom once remarked to me that “one of the mysteries of human intelligence is that\nwe’re able to do so much with so little.” Like machines, human beings use algorithms to\nmake decisions or solve problems; the remarkable difference lies in the human brain’s\noverall level of success despite the comparative limits on computational resources.\nThe efficacy of human algorithms springs from what AI researchers refer to as\n“bounded optimality.” As psychologist Daniel Kahneman has notably pointed out,\nhuman beings are rational only up to a point. If you were perfectly rational, you would\nrisk dropping dead before making an important decision—whom to hire, whom to marry,\nand so on—depending on the number of options available for your review.\n“With all of the successes of AI over the last few years, we’ve got good models of\nthings like images and text, but what we’re missing are good models of people,” Tom\nsays. “Human beings are still the best example we have of thinking machines. By\nidentifying the quantity and the nature of the preconceptions that inform human cognition\nwe can lay the groundwork for bringing computers even closer to human performance.”\n91\nTHE ARTIFICIAL USE OF HUMAN BEINGS\nTom Griffiths\nTom Griffiths is Henry R. Luce Professor of Information, Technology, Consciousness,\nand Culture at Princeton University. He is co-author (with Brian Christian) of\nAlgorithms to Live By.\nWhen you ask people to imagine a world that has successfully, beneficially incorporated\nadvances in artificial intelligence, everybody probably comes up with a slightly different\npicture. Our idiosyncratic visions of the future might differ in the presence or absence of\nspaceships, flying cars, or humanoid robots. But one thing doesn’t vary: the presence of\nhuman beings. That’s certainly what Norbert Wiener imagined when he wrote about the\npotential of machines to improve human society by interacting with humans and helping\nto mediate their interactions with one another. Getting to that point doesn’t just require\ncoming up with ways to make machines smarter. It also requires a better understanding\nof how human minds work.\nRecent advances in artificial intelligence and machine learning have resulted in\nsystems that can meet or exceed human abilities in playing games, classifying images, or\nprocessing text. But if you want to know why the driver in front of you cut you off, why\npeople vote against their interests, or what birthday present you should get for your\npartner, you’re still better off asking a human than a machine. Solving those problems\nrequires building models of human minds that can be implemented inside a computer—\nsomething that’s essential not just to better integrate machines into human societies but to\nmake sure that human societies can continue to exist.\nConsider the fantasy of having an automated intelligent assistant that can take on\nsuch basic tasks as planning meals and ordering groceries. To succeed in these tasks, it\nneeds to be able to make inferences about what you want, based on the way you behave.\nAlthough this seems simple, making inferences about the preferences of human beings\ncan be a tricky matter. For example, having observed that the part of the meal you most\nenjoy is dessert, your assistant might start to plan meals consisting entirely of desserts.\nOr perhaps it has heard your complaints about never having enough free time and\nobserved that looking after your dog takes up a considerable amount of that free time.\nFollowing the dessert debacle, it has also understood that you prefer meals that\nincorporate protein, so it might begin to research recipes that call for dog meat. It’s not a\nlong journey from examples like this to situations that begin to sound like problems for\nthe future of humanity (all of whom are good protein sources).\nMaking inferences about what humans want is a prerequisite for solving the AI\nproblem of value alignment—aligning the values of an automated intelligent system with\nthose of a human being. Value alignment is important if we want to ensure that those\nautomated intelligent systems have our best interests at heart. If they can’t infer what we\nvalue, there’s no way for them to act in support of those values—and they may well act in\nways that contravene them.\nValue alignment is the subject of a small but growing literature in artificialintelligence\nresearch. One of the tools used for solving this problem is inversereinforcement\nlearning. Reinforcement learning is a standard method for training\nintelligent machines. By associating particular outcomes with rewards, a machine-\n92\nlearning system can be trained to follow strategies that produce those outcomes. Wiener\nhinted at this idea in the 1950s, but the intervening decades have developed it into a fine\nart. Modern machine-learning systems can find extremely effective strategies for playing\ncomputer games—from simple arcade games to complex real-time strategy games—by\napplying reinforcement-learning algorithms. Inverse reinforcement learning turns this\napproach around: By observing the actions of an intelligent agent that has already\nlearned effective strategies, we can infer the rewards that led to the development of those\nstrategies.\nIn its simplest form, inverse reinforcement learning is something people do all the\ntime. It’s so common that we even do it unconsciously. When you see a co-worker go to\na vending machine filled with potato chips and candy and buy a packet of unsalted nuts,\nyou infer that your co-worker (1) was hungry and (2) prefers healthy food. When an\nacquaintance clearly sees you and then tries to avoid encountering you, you infer that\nthere’s some reason they don’t want to talk to you. When an adult spends a lot of time\nand money in learning to play the cello, you infer that they must really like classical\nmusic—whereas inferring the motives of a teenage boy learning to play an electric guitar\nmight be more of a challenge.\nInverse reinforcement learning is a statistical problem: We have some data—the\nbehavior of an intelligent agent—and we want to evaluate various hypotheses about the\nrewards underlying that behavior. When faced with this question, a statistician thinks\nabout the generative model behind the data: What data would we expect to be generated\nif the intelligent agent was motivated by a particular set of rewards? Equipped with the\ngenerative model, the statistician can then work backward: What rewards would likely\nhave caused the agent to behave in that particular way?\nIf you’re trying to make inferences about the rewards that motivate human\nbehavior, the generative model is really a theory of how people behave—how human\nminds work. Inferences about the hidden causes behind the behavior of other people\nreflect a sophisticated model of human nature that we all carry around in our heads.\nWhen that model is accurate, we make good inferences. When it’s not, we make\nmistakes. For example, a student might infer that his professor is indifferent to him if the\nprofessor doesn’t immediately respond to his email—a consequence of the student’s\nfailure to realize just how many emails that professor receives.\nAutomated intelligent systems that will make good inferences about what people\nwant must have good generative models for human behavior: that is, good models of\nhuman cognition expressed in terms that can be implemented on a computer.\nHistorically, the search for computational models of human cognition is intimately\nintertwined with the history of artificial intelligence itself. Only a few years after Norbert\nWiener published The Human Use of Human Beings, Logic Theorist, the first\ncomputational model of human cognition and also the first artificial-intelligence system,\nwas developed by Herbert Simon, of Carnegie Tech, and Allen Newell, of the RAND\nCorporation. Logic Theorist automatically produced mathematical proofs by emulating\nthe strategies used by human mathematicians.\nThe challenge in developing computational models of human cognition is making\nmodels that are both accurate and generalizable. An accurate model, of course, predicts\nhuman behavior with a minimum of errors. A generalizable model can make predictions\nacross a wide range of circumstances, including circumstances unanticipated by its\n93\ncreators—for instance, a good model of the Earth’s climate should be able to predict the\nconsequences of a rising global temperature even if this wasn’t something considered by\nthe scientists who designed it. However, when it comes to understanding the human\nmind, these two goals—accuracy and generalizability—have long been at odds with each\nother.\nAt the far extreme of generalizability are rational theories of cognition. These\ntheories describe human behavior as a rational response to a given situation. A rational\nactor strives to maximize the expected reward produced by a sequence of actions—an\nidea widely used in economics precisely because it produces such generalizable\npredictions about human behavior. For the same reason, rationality is the standard\nassumption in inverse-reinforcement-learning models that try to make inferences from\nhuman behavior—perhaps with the concession that humans are not perfectly rational\nagents and sometimes randomly choose to act in ways unaligned with or even opposed to\ntheir best interests.\nThe problem with rationality as a basis for modeling human cognition is that it is\nnot accurate. In the domain of decision making, an extensive literature—spearheaded by\nthe work of cognitive psychologists Daniel Kahneman and Amos Tversky—has\ndocumented the ways in which people deviate from the prescriptions of rational models.\nKahneman and Tversky proposed that in many situations people instead follow simple\nheuristics that allow them to reach good solutions at low cognitive cost but sometimes\nresult in errors. To take one of their examples, if you ask somebody to evaluate the\nprobability of an event, they might rely on how easy it is to generate an example of such\nan event from memory, consider whether they can come up with a causal story for that\nevent’s occurring, or assess how similar the event is to their expectations. Each heuristic\nis a reasonable strategy for avoiding complex probabilistic computations, but also results\nin errors. For instance, relying on the ease of generating an event from memory as a\nguide to its probability leads us to overestimate the chances of extreme (hence extremely\nmemorable) events such as terrorist attacks.\nHeuristics provide a more accurate model of human cognition but one that is not\neasily generalizable. How do we know which heuristic people might use in a particular\nsituation? Are there other heuristics they use that we just haven’t discovered yet?\nKnowing exactly how people will behave in a new situation is a challenge: Is this\nsituation one in which they would generate examples from memory, come up with causal\nstories, or rely on similarity?\nUltimately, what we need is a way to describe how human minds work that has\nthe generalizability of rationality and the accuracy of heuristics. One way to achieve this\ngoal is to start with rationality and consider how to take it in a more realistic direction. A\nproblem with using rationality as a basis for describing the behavior of any real-world\nagent is that, in many situations, calculating the rational action requires the agent to\npossess a huge amount of computational resources. It might be worth expending those\nresources if you’re making a highly consequential decision and have a lot of time to\nevaluate your options, but most human decisions are made quickly and for relatively low\nstakes. In any situation where the time you spend making a decision is costly—at the\nvery least because it’s time you could spend doing something else—the classic notion of\nrationality is no longer a good prescription for how one should behave.\nTo develop a more realistic model of rational behavior, we need to take into\n94\naccount the cost of computation. Real agents need to modulate the amount of time they\nspend thinking by the effect the extra thought has on the results of a decision. If you’re\ntrying to choose a toothbrush, you probably don’t need to consider all four thousand\nlistings for manual toothbrushes on Amazon.com before making a purchase: You trade\noff the time you spend looking with the difference it makes in the quality of the outcome.\nThis trade-off can be formalized, resulting in a model of rational behavior that artificialintelligence\nresearchers call “bounded optimality.” The bounded-optimal agent doesn’t\nfocus on always choosing exactly the right action to take but rather on finding the right\nalgorithm to follow in order to find the perfect balance between making mistakes and\nthinking too much.\nBounded optimality bridges the gap between rationality and heuristics. By\ndescribing behavior as the result of a rational choice about how much to think, it provides\na generalizable theory—that is, one that can be applied in new situations. Sometimes the\nsimple strategies that have been identified as heuristics that people follow turn out to be\nbounded-optimal solutions. So, rather than condemning the heuristics that people use as\nirrational, we can think of them as a rational response to constraints on computation.\nDeveloping bounded optimality as a theory of human behavior is an ongoing\nproject that my research group and others are actively pursuing. If these efforts succeed,\nthey will provide us with the most important ingredient we need for making artificialintelligence\nsystems smarter when they try to interpret people’s actions, by enabling a\ngenerative model for human behavior.\nTaking into account the computational constraints that factor into human\ncognition will be particularly important as we begin to develop automated systems that\naren’t subject to the same constraints. Imagine a superintelligent AI system trying to\nfigure out what people care about. Curing cancer or confirming the Riemann hypothesis,\nfor instance, won’t seem, to such an AI, like things that are all that important to us: If\nthese solutions are obvious to the superintelligent system, it might wonder why we\nhaven’t found them ourselves, and conclude that those problems don’t mean much to us.\nIf we cared and the problems were so simple, we would have solved them already. A\nreasonable inference would be that we do science and math purely because we enjoy\ndoing science and math, not because we care about the outcomes.\nAnybody who has young children can appreciate the problem of trying to interpret\nthe behavior of an agent that is subject to computational constraints different from one’s\nown. Parents of toddlers can spend hours trying to disentangle the true motivations\nbehind seemingly inexplicable behavior. As a father and a cognitive scientist, I found it\nwas easier to understand the sudden rages of my two-year-old when I recognized that she\nwas at an age where she could appreciate that different people have different desires but\nnot that other people might not know what her own desires were. It’s easy to understand,\nthen, why she would get annoyed when people didn’t do what she (apparently\ntransparently) wanted. Making sense of toddlers requires building a cognitive model of\nthe mind of a toddler. Superintelligent AI systems face the same challenge when trying\nto make sense of human behavior.\nSuperintelligent AI may still be a long way off. In the short term, devising better\nmodels of people can prove extremely valuable to any company that makes money by\nanalyzing human behavior—which at this point is pretty much every company that does\nbusiness on the Web. Over the last few years, significant new commercial technologies\n95\nfor interpreting images and text have resulted from developing good models for vision\nand language. Developing good models of people is the next frontier.\nOf course, understanding how human minds work isn’t just a way to make\ncomputers better at interacting with people. The trade-off between making mistakes and\nthinking too much that characterizes human cognition is a trade-off faced by any realworld\nintelligent agent. Human beings are an amazing example of systems that act\nintelligently despite significant computational constraints. We’re quite good at\ndeveloping strategies that allow us to solve problems pretty well without working too\nhard. Understanding how we do this will be a step toward making computers work\nsmarter, not harder.\n96\nRomanian-born Anca Dragan’s research focuses on algorithms that will enable robots\nto work with, around, and in support of people. She runs the InterACT Laboratory at\nBerkeley, where her students work across different applications, from assistive robots to\nmanufacturing to autonomous cars, and draw from optimal control, planning, estimation,\nlearning, and cognitive science. Barely into her thirties herself, she has co-authored a\nnumber of papers with her veteran Berkeley colleague and mentor Stuart Russell which\naddress various aspects of machine learning and the knotty problems of value alignment.\nShe shares Stuart’s preoccupation with AI safety: “An immediate risk is agents\nproducing unwanted, surprising behavior,” she told an interviewer from the Future of\nLife Institute. “Even if we plan to use AI for good, things can go wrong, precisely\nbecause we are bad at specifying objectives and constraints for AI agents. Their\nsolutions are often not what we had in mind.”\nHer principal goal is therefore to help robots and programmers alike to overcome\nthe many conflicts that arise because of a lack of transparency about each other’s\nintentions. Robots, she says, need to ask us questions. They should wonder about their\nassignments, and they should pester their human programmers until everybody is on the\nsame page—so as to avoid what she has euphemistically called “unexpected side\neffects.”\n97\nPUTTING THE HUMAN INTO THE AI EQUATION\nAnca Dragan\nAnca Dragan is an assistant professor in the Department of Electrical Engineering and\nComputer Sciences at UC Berkeley. She co-founded and serves on the steering\ncommittee for the Berkeley AI Research (BAIR) Lab and is a co-principal investigator in\nBerkeley’s Center for Human-Compatible AI.\nAt the core of artificial intelligence is our mathematical definition of what an AI agent (a\nrobot) is. When we define a robot, we define states, actions, and rewards. Think of a\ndelivery robot, for instance. States are locations in the world, and actions are motions\nthat the robot makes to get from one position to a nearby one. To enable the robot to\ndecide on which actions to take, we define a reward function—a mapping from states and\nactions to scores indicating how good that action was in that state—and have the robot\nchoose actions that accumulate the most “reward.” The robot gets a high reward when it\nreaches its destination, and it incurs a small cost every time it moves; this reward function\nincentivizes the robot to get to the destination as quickly as possible. Similarly, an\nautonomous car might get a reward for making progress on its route and incur a cost for\ngetting too close to other cars.\nGiven these definitions, a robot’s job is to figure out what actions it should take in\norder to get the highest cumulative reward. We’ve been working hard in AI on enabling\nrobots to do just that. Implicitly, we’ve assumed that if we’re successful—if robots can\ntake any problem definition and turn into a policy for how to act—we will get robots that\nare useful to people and to society.\nWe haven’t been too wrong so far. If you want an AI that classifies cells as either\ncancerous or benign, or a robot that vacuums the living room rug while you’re at work,\nwe’ve got you covered. Some real-world problems can indeed be defined in isolation,\nwith clear-cut states, actions, and rewards. But with increasing AI capability, the\nproblems we want to tackle don’t fit neatly into this framework. We can no longer cut\noff a tiny piece of the world, put it in a box, and give it to a robot. Helping people is\nstarting to mean working in the real world, where you have to actually interact with\npeople and reason about them. “People” will have to formally enter the AI problem\ndefinition somewhere.\nAutonomous cars are already being developed. They will need to share the road\nwith human-driven vehicles and pedestrians and learn to make the trade-off between\ngetting us home as fast as possible and being considerate of other drivers. Personal\nassistants will need to figure out when and how much help we really want and what types\nof tasks we prefer to do on our own versus what we can relinquish control over. A DSS\n(Decision Support System) or a medical diagnostic system will need to explain its\nrecommendations to us so we can understand and verify them. Automated tutors will\nneed to determine what examples are informative or illustrative—not to their fellow\nmachines but to us humans.\nLooking further into the future, if we want highly capable AIs to be compatible\nwith people, we can’t create them in isolation from people and then try to make them\ncompatible afterward; rather, we’ll have to define “human-compatible” AI from the getgo.\nPeople can’t be an afterthought.\n98\nWhen it comes to real robots helping real people, the standard definition of AI\nfails us, for two fundamental reasons: First, optimizing the robot’s reward function in\nisolation is different from optimizing it when the robot acts around people, because\npeople take actions too. We make decisions in service of our own interests, and these\ndecisions dictate what actions we execute. Moreover, we reason about the robot—that is,\nwe respond to what we think it’s doing or will do and what we think its capabilities are.\nWhatever actions the robot decides on need to mesh well with ours. This is the\ncoordination problem.\nSecond, it is ultimately a human who determines what the robot’s reward function\nshould be in the first place. And they are meant to incentivize robot behavior that\nmatches what the end-user wants, what the designer wants, or what society as a whole\nwants. I believe that capable robots that go beyond very narrowly defined tasks will need\nto understand this to achieve compatibility with humans. This is the value-alignment\nproblem.\nThe Coordination Problem: People are more than objects in the environment.\nWhen we design robots for a particular task, it’s tempting to abstract people away. A\nrobotic personal assistant, for example, needs to know how to move to pick up objects, so\nwe define that problem in isolation from the people for whom the robot is picking these\nobjects up. Still, as the robot moves around, we don’t want it bumping into anything, and\nthat includes people, so we might include the physical location of the person in the\ndefinition of the robot’s state. Same for cars: We don’t want them colliding with other\ncars, so we enable them to track the positions of those other cars and assume that they’ll\nbe moving consistently in the same direction in the future. A human being, in this sense,\nis no different to a robot from a ball rolling on a flat surface. The ball will behave in the\nnext few seconds the same way it behaved in the past few; it keeps rolling in the same\ndirection at roughly the same speed. This is of course nothing like real human behavior,\nbut such simplification enables many robots to succeed in their tasks and, for the most\npart, stay out of people’s way. A robot in your house, for example, might see you\ncoming down the hall, move aside to let you pass, and resume its task once you’ve gone\nby.\nAs robots have become more capable, though, treating people as consistently\nmoving obstacles is starting to fall short. A human driver switching lanes won’t continue\nin the same direction but will move straight ahead once they’ve made the lane change.\nWhen you reach for something, you often reach around other objects and stop when you\nget to the one you want. When you walk down a hallway, you have a destination in\nmind: You might take a right into the bedroom or a left into the living room. Relying on\nthe assumption that we’re no different from a rolling ball leads to inefficiency when the\nrobot stays out of the way if it doesn’t need to, and it can imperil the robot when the\nperson’s behavior changes. Even just to stay out of the way, robots have to be somewhat\naccurate at anticipating human actions. And, unlike the rolling ball, what people will do\ndepends on what they decide to do. So to anticipate human actions, robots need to start\nunderstanding human decision making. And that doesn’t mean assuming that human\nbehavior is perfectly optimal; that might be enough for a chess- or Go-playing robot, but\nin the real world, people’s decisions are less predictable than the optimal move in a board\ngame.\n99\nThis need to understand human actions and decisions applies to physical and\nnonphysical robots alike. If either sort bases its decision about how to act on the\nassumption that a human will do one thing but the human does something else, the\nresulting mismatch could be catastrophic. For cars, it can mean collisions. For an AI\nwith, say, a financial or economic role, the mismatch between what it expects us to do\nand what we actually do could have even worse consequences.\nOne alternative is for the robot not to predict human actions but instead just\nprotect against the worst-case human action. Often when robots do that, though, they\nstop being all that useful. With cars, this results in being stuck, because it makes every\nmove too risky.\nAll this puts us, the AI community, into a bind. It suggests that robots will need\naccurate (or at least reasonable) predictive models of whatever people might decide to do.\nOur state definition can’t just include the physical position of humans in the world.\nInstead, we’ll also need to estimate something internal to people. We’ll need to design\nrobots that account for this human internal state, and that’s a tall order. Luckily, people\ntend to give robots hints as to what their internal state is: Their ongoing actions give the\nrobot observations (in the Bayesian inference sense) about their intentions. If we start\nwalking toward the right side of the hallway, we’re probably going to enter the next room\non the right.\nWhat makes the problem more complicated is the fact that people don’t make\ndecisions in isolation. It would be one thing if robots could predict the actions a person\nintends to take and simply figure out what to do in response. But unfortunately this can\nlead to ultra-defensive robots that confuse the heck out of people. (Think of human\ndrivers stuck at four-way stops, for instance.) What the intent-prediction approach misses\nis that the moment the robot acts, that influences what actions the human starts taking.\nThere is a mutual influence between robots and people, one that robots will need\nto learn to navigate. It is not always just about the robot planning around people; people\nplan around the robot, too. It is important for robots to account for this when deciding\nwhich actions to take, be it on the road, in the kitchen, or even in virtual spaces, where\nactions might be making a purchase or adopting a new strategy. Doing so should endow\nrobots with coordination strategies, enabling them to take part in the negotiations people\nseamlessly carry out day to day—from who goes first at an intersection or through a\nnarrow door, to what role we each take when we collaborate on preparing breakfast, to\ncoming to consensus on what next step to take on a project.\nFinally, just as robots need to anticipate what people will do next, people need to\ndo the same with robots. This is why transparency is important. Not only will robots\nneed good mental models of people, but people will need good mental models of robots.\nThe model that a person has of the robot has to go into our state definition as well, and\nthe robot has to be aware of how its actions are changing that model. Much like the robot\ntreating human actions as clues to human internal states, people will change their beliefs\nabout the robot as they observe its actions. Unfortunately, the giving of clues doesn’t\ncome as naturally to robots as it does to humans; we’ve had a lot of practice\ncommunicating implicitly with people. But enabling robots to account for the change\nthat their actions are causing to the person’s mental model of the robot can lead to more\ncarefully chosen actions that do give the right clues—that clearly communicate to people\nabout the robot’s intentions, its reward function, its limitations. For instance, a robot\n100\nmight alter its motion when carrying something heavy, to emphasize the difficulty it has\nin maneuvering heavy objects. The more that people know about the robot, the easier it\nis to coordinate with it.\nAchieving action compatibility will require robots to anticipate human actions,\naccount for how those actions will influence their own, and enable people to anticipate\nrobot actions. Research has ,ade a degree of progress in meeting these challenges, but we\nstill have a long way to go.\nThe Value Alignment Problem: People hold the key to the robot’s reward function.\nProgress on enabling robots to optimize reward puts more burden on us, the designers, to\ngive them the right reward to optimize in the first place. The original thought was that\nfor any task we wanted the robot to do, we could write down a reward function that\nincentivizes the right behavior. Unfortunately, what often happens is that we specify\nsome reward function and the behavior that emerges out of optimizing it isn’t what we\nwant. Intuitive reward functions, when combined with unusual instances of a task, can\nlead to unintuitive behavior. You reward an agent in a racing game with a score in the\ngame, and in some cases it finds a loophole that it exploits to gain infinitely many points\nwithout actually winning the race. Stuart Russell and Peter Norvig give a beautiful\nexample in their book Artificial Intelligence: A Modern Approach: rewarding a\nvacuuming robot for how much dust it sucks in results in the robot deciding to dump out\ndust so that it can suck it in again and get more reward.\nIn general, humans have had a notoriously difficult time specifying exactly what\nthey want, as exemplified by all those genie legends. An AI paradigm in which robots\nget some externally specified reward fails when that reward is not perfectly well thought\nout. It may incentivize the robot to behave in the wrong way and even resist our attempts\nto correct its behavior, as that would lead to a lower specified reward.\nA seemingly better paradigm might be for robots to optimize for what we\ninternally want, even if we have trouble explicating it. They would use what we say and\ndo as evidence about what we want, rather than interpreting it literally and taking it as a\ngiven. When we write down a reward function, the robot should understand that we\nmight be wrong: that we might not have considered all facets of the task; that there’s no\nguarantee that said reward function will always lead to the behavior we want. The robot\nshould integrate what we wrote down into its understanding of what we want, but it\nshould also have a back-and-forth with us to elicit clarifying information. It should seek\nour guidance, because that’s the only way to optimize the true desired reward function.\nEven if we give robots the ability to learn what we want, an important question\nremains that AI alone won’t be able to answer. We can make robots try to align with a\nperson’s internal values, but there’s more than one person involved here. The robot has\nan end-user (or perhaps a few, like a personal robot caring for a family, a car driving a\nfew passengers to different destinations, or an office assistant for an entire team); it has a\ndesigner (or perhaps a few); and it interacts with society—the autonomous car shares the\nroad with pedestrians, human-driven vehicles, and other autonomous cars. How to\ncombine these people’s values when they might be in conflict is an important problem we\nneed to solve. AI research can give us the tools to combine values in any way we decide\nbut can’t make the necessary decision for us.\n101\nIn short, we need to enable robots to reason about us—to see us as something\nmore than obstacles or perfect game players. We need them to take our human nature\ninto account, so that they are well coordinated and well aligned with us. If we succeed,\nwe will indeed have tools that substantially increase our quality of life.\n102\nChris Anderson’s company, 3DR, helped start the modern drone industry and now\nfocuses on drone data software. He got his start building an open-source aerial robotics\ncommunity called DIY Drones, and undertook some ill-advised early experiments, such\nas buzzing Lawrence Berkeley Laboratory with one of his self-flying spies. It\nmay well have been a case of antic gene-expression, since he’s descended from a founder\nof the American Anarchist movement. Chris ran Wired magazine, a go-to publication for\ntechno-utopians and -dystopians alike, from 2001 to 2012; during his tenure it won five\nNational Magazine Awards.\nChris dislikes the term “roboticist” (“like any properly humbled roboticist, I\ndon’t call myself one”). He began as a physicist. “I turned out to be a bad physicist,” he\ntold me recently. “I struggled on, went to Los Alamos, and thought, ‘Well maybe I’m not\ngoing to be a Nobel Prize winner, but I can still be a scientist.’ All of us who were in\nphysics and had these romantic heroes—the Feynmans, the Manhattan Project—realized\nthat our career trajectory would at best be working on one project at CERN for fifteen\nyears. That project would either be a failure, in which case there would be no paper, or\nit would be a success, in which case you’d be author #300 on the paper and become an\nassistant professor at Iowa State.\n“Most of my classmates went to Wall Street to become quants, and to them we\nowe the subprime mortgage. Others went on to start the Internet. First, we built the\nInternet by connecting physics labs; second, we built the Web; third, we were the first to\ndo Big Data. We had supercomputers—Crays—which were half the power of your phone\nnow, but they were the supercomputers of the time. Meanwhile, we were reading this\nmagazine called Wired, which came out in 1993, and we realized that this tool we\nscientists use could have applications for everybody. The Internet wasn’t just about\nscientific data, it was a mind-blowing cultural revolution. So when Conde Nast asked me\nto take over the magazine, I was like, ‘Absolutely!’ This magazine changed my life.”\nHe had five children by that time—video-game players—who got him into the\n“flying robots.” He quit his day job at Wired. The rest is Silicon Valley history.\n103\nGRADIENT DESCENT\nChris Anderson\nChris Anderson is an entrepreneur; former editor-in-chief of Wired; co-founder and\nCEO of 3DR; and author of The Long Tail, Free, and Makers.\nLife\nThe mosquito first detects my scent from thirty feet away. It triggers its pursuit function,\nwhich consists of the simplest possible rules. First, move in a random direction. If the\nscent increases, continue moving in that direction. If the scent decreases, move in the\nopposite direction. If the scent is lost, move sideways until a scent is picked up again.\nRepeat until contact with the target is achieved.\nThe plume of my scent is densest next to me and disperses as it spreads, an\ninvisible fog of particles exuded from my skin that moves like smoke with the wind. The\ncloser to my skin, the higher the particle density; the farther away, the lower. This\ndecrease is called a gradient, which describes any gradual transition from one level to\nanother one—as opposed to a “step function,” which describes a discrete change.\nOnce the mosquito follows this gradient to its source using its simple algorithm, it\nlands on my skin, which it senses with the heat detectors in its feet, which are attuned to\nanother gradient—temperature. It then pushes its needle-shaped proboscis through the\nsurface, where a third set of sensors in the tip detect yet another gradient, that of blood\ndensity. This flexible needle wriggles around under my skin until the scent of blood\nsteers it to a capillary, which it punctures. Then my blood begins to flow into the\nmosquito. Mission accomplished. Ouch.\nWhat seems like the powerful radar of insects in the dark, with blood-seeking\nintelligence inexplicable for such tiny brains, is actually just a sensitive nose with almost\nno intelligence at all. Mosquitoes are closer to plants that follow the sun than to guided\nmissiles. Yet by applying this simple “follow your nose” rule quite literally, they can\ntravel through a house to find you, slip through cracks in a screen door, even zero in on\nthe tiny strip of skin you left exposed between hat and shirt collar. It’s just a random\nwalk, combined with flexible wings and legs that let the insect bounce off obstacles, and\nan instinct to descend a chemical gradient.\nBut “gradient descent” is much more than bug navigation. Look around you and\nyou’ll find it everywhere, from the most basic physical rules of the universe to the most\nadvanced artificial intelligence.\nThe Universe\nWe live in a world of countless gradients, from light and heat to gravity and chemical\ntrails (chemtrails!). Water flows along a gravity gradient downhill, and your body lives\non chemical solutions flowing across cell membranes from high concentration to\nlow. Every action in the universe is driven by some gradient drive, from the movement\nof the planets around gravity gradients to the joining of atoms along electric-charge\ngradients to form molecules. Our own urges, such as hunger and sleepiness, are driven\nby electro-chemical gradients in our bodies. And our brain’s functions, the electrical\nsignals moving along ion channels in the synapses between our neurons, are simply\natoms and electrons flowing “downhill” along yet more electrical and chemical gradients.\n104\nForget clockwork analogies; our brains are closer to a system of canals and locks, with\nsignals traveling like water from one state to another.\nAs I sit here typing, I’m actually seeking equilibrium states in an n-dimensional\ntopology of gradients. Take just one: heat. My body temperature is higher than the air\ntemperature, so I radiate heat, which must be replenished in my core. Even the bacteria\nin my digestive tract use sensors to measure sugar concentrations in the liquid around\nthem and whip their tail-like flagella to swim “upstream” where the sugar supply is\nrichest. The natural state of all systems is to flow to lower energy states, a process that is\nbroadly described by entropy (the tendency of things to go from ordered to disordered\nstates; all things will fall apart eventually, including the universe itself).\nBut how do you explain more complex behavior, such as our ability to make\ndecisions? The answer is just more gradient descent.\nOur Brains\nAs miraculous and inscrutable as our human intelligence is, science is coming around to\nthe view that our brains operate the same way as any other complex system with layers\nand feedback loops, all pursuing what we mathematically call “optimization functions”\nbut you could just as well call “flowing downhill” in some sense.\nThe essence of intelligence is learning, and we do that by correlating inputs with\npositive or negatives scores (rewards or punishment). So, for a baby, “this sound” (your\nmother’s voice) is associated with other learned connections to your mother, such as food\nor comfort. Likewise, “this muscle motion brings my thumb closer to my mouth.” Over\ntime and trial and error, the brain’s neural network reinforces those connections.\nMeanwhile “this muscle motion does not bring my thumb close to my mouth” is a\nnegative correlation, and the brain will weaken those connections.\nHowever, this is too simplistic. The limits of gradient descent constitute the socalled\nlocal-minima problem (or local-maxima problem, if you’re doing a gradient\nascent). If you are walking in a mountainous region and want to get home, always\nwalking downhill will most likely get you to the next valley but not necessarily over the\nother mountains that lie around it and between you and home. For that, you need either a\nmental model (i.e., a map) of the topology so you know where to ascend to get out of the\nvalley, or you need to switch between gradient descent and random walks so you can\nbounce your way out of the region.\nWhich is, in fact, exactly what the mosquito does in following my scent: It\ndescends when it’s in my plume and random-walks when it has lost the trail or hit an\nobstacle.\nAI\nSo that’s nature. What about computers? Traditional software doesn’t work that way—it\nfollows deterministic trees of hard logic: “If this, do that.” But software that interacts\nwith the physical world tends to work more like the physical world. That means dealing\nwith noisy inputs (sensors or human behavior) and providing probabilistic, not\ndeterministic, results. And that, in turn, means more gradient descent.\nAI software is the best example of this, especially the kinds of AI that use\nartificial neural-network models (including convolutional, or “deep,” neural networks of\nmany layers). In these, a typical process consists of “training” them by showing them\n105\nlots of examples of something you want them to learn (pictures of cats labeled “cat,” for\nexample), along with examples of other random data (pictures of other things). This is\ncalled “supervised learning,” because the neural network is being taught by example,\nincluding the use of “adversarial training” with data that is not correlated to the desired\nresult.\nThese neural networks, like their biological models, consist of layers of thousands\nof nodes (“neurons,” in the analogy), each of which is connected to all the nodes in the\nlayers above and below by connections that initially have random strength. The top layer\nis presented with data, and the bottom layer is given the correct answer. Any series of\nconnections that happened to land on the right answer is made stronger (“rewarded”), and\nthose that were wrong are made weaker (“punished”). Repeat tens of thousands of times\nand eventually you have a fully trained network for that kind of data.\nYou can think of all the possible combinations of connections as like the surface\nof a planet, with hills and valleys. (Ignore for the moment that the surface is just 3D and\nthe actual topology is many-dimensional.) The optimization that the network goes\nthrough as it learns is just a process of finding the deepest valley on the planet. This\nconsists of the following steps:\n1. Define a “cost function” that determines how well the network solved the problem\n2. Run the network once and see how it did at that cost function\n3. Change the values of the connections and do it again. The difference between\nthose two results is the direction, or “slope,” in which the network moved\nbetween the two trials.\n4. If the slope is pointed “downhill,” change the connections more in that direction.\nIf it’s “uphill,” change them in the opposite direction.\n5. Repeat until there is no improvement in any direction. That means that you’re in\na minimum.\nCongrats! But it’s probably a local minimum, or a little dip in the mountains, so you’re\ngoing to have to keep going if you want to do better. You can’t keep going downhill, and\nyou don’t know where the absolute lowest point is, so you’re going to have to somehow\nfind it. There are many ways to do that, but here are a few:\n1. Try lots of times with different random settings and share learning from each trial;\nessentially, you are shaking the system to see if it settles in a lower state. If one\nof the other trials found a lower valley, start with those settings.\n2. Don’t just go downhill but stumble around a bit like a drunk, too (this is called\n“stochastic gradient descent”). If you do this long enough, you’ll eventually find\nrock bottom. There’s a metaphor for life in that.\n3. Just look for “interesting” features, which are defined by diversity (edges or color\nchanges, for example). Warning: This way can lead to madness—too much\n“interestingness” draws the network to optical illusions. So keep it sane, and\nemphasize the kinds of features that are likely to be real in nature, as opposed to\nartifacts or errors. This is called “regularization,” and there are lots of techniques\nfor this, such as whether those kinds of features have been seen before (learned),\n106\nor are too “high frequency” (like static) rather than “low frequency” (more\ncontinuous, like actual real-world features).\nJust because AI systems sometimes end up in local minima, don’t conclude that this\nmakes them any less like life. Humans—indeed, probably all life-forms—are often stuck\nin local minima.\nTake our understanding of the game of Go, which was taught and learned and\noptimized by humans for thousands of years. It took AIs less than three years to find out\nthat we’d been playing it wrong all along and that there were better, almost alien,\nsolutions to the game which we’d never considered—mostly because our brains don’t\nhave the processing power to consider so many moves ahead.\nEven in chess, which is ten times easier and was thought to be understood, bruteforce\nmachines could beat us at our own strategies. Chess, too, turned out, when\nexplored by superior neural-network AI systems, to have weird but superior strategies\nwe’d never considered, like sacrificing queens early to gain an obscure long-term\nadvantage. It’s as if we had been playing 2D versions of games that actually existed in\nhigher dimensions.\nIf any of this sounds familiar, it’s because physics has been wrestling with these\nsorts of topological problems for decades. The notion of space being many-dimensional,\nand math reducing to understanding the geometries and interactions of “membranes”\nbeyond the reach of our senses, is where Grand Unified Theorists go to die. But unlike\nmultidimensional theoretical physics, AI is something we can actually experiment with\nand measure.\nSo that’s what we’re going to do. The next few decades will be an explosive\nexploration of ways to think that 7 million years of evolution never found. We’re going\nto rock ourselves out of local minima and find deeper minima, maybe even global\nminima. And when we’re done, we may even have taught machines to seem as smart as\na mosquito, forever descending the cosmic gradients to an ultimate goal, whatever that\nmay be.\n107\nDavid Kaiser is a physicist atypically interested in the intersection of his science with\npolitics and culture, about which he has written widely.\nIn the first meeting (in Washington, Connecticut) that preceded the crafting of this\nbook, he commented on the change in how “information” is viewed since Wiener’s time:\nthe military-industrial, Cold War era. Back then, Wiener compared information,\nmetaphorically, to entropy, in that it could not be conserved—i.e., monopolized; thus, he\nargued, our atomic secrets and other such classified matters would not remain secrets for\nlong. Today, whereas (as Wiener might have expected) information, fake or not, is\nleaking all over the other Washington, information in the economic world has indeed\nbeen stockpiled, commodified, and monetized.\nThis lockdown, David said, was “not all good, not all bad”—depending, I guess,\non whether you’re sick of being pestered by ads for socks or European river cruises\npopping up in your browser minutes after you’ve bought them.\nTo say nothing of information’s proliferation. David complained to the rest of us\nattending the meeting that in Wiener’s time, physicists could “take the entire Physical\nReview. It would sit comfortably in front of us in a manageable pile. Now we’re awash\nin fifty thousand open-source journals per minute,” full of god-knows-what. Neither of\nthese developments would Wiener have anticipated, said David, prompting him to ask,\n“Do we need a new set of guiding metaphors?”\n108\n“INFORMATION” FOR WIENER, FOR SHANNON, AND FOR US\nDavid Kaiser\nDavid Kaiser is Germeshausen Professor of the History of Science and professor of\nphysics at MIT, and head of its Program in Science, Technology & Society. He is the\nauthor of How the Hippies Saved Physics: Science, Counterculture, and the Quantum\nRevival and American Physics and the Cold War Bubble (forthcoming).\nIn The Sleepwalkers, a sweeping history of scientific thought from ancient times through\nthe Renaissance, Arthur Koestler identified a tension that has marked the most dramatic\nleaps of our cosmological imagination. In reading the great works of Nicolaus\nCopernicus and Johannes Kepler today, Koestler argued, we are struck as much by their\nstrange unfamiliarity—their embeddedness in the magic or mysticism of an earlier age—\nas by their modern-sounding insights.\nI detect that same doubleness—the zig-zag origami folds of old and new—in\nNorbert Wiener’s classic The Human Use of Human Beings. First published in 1950 and\nrevised in 1954, the book is in many ways extraordinarily prescient. Wiener, the MIT\npolymath, recognized before most observers that “society can only be understood through\na study of the messages and the communication facilities which belong to it.” Wiener\nargued that feedback loops, the central feature of his theory of cybernetics, would play a\ndetermining role in social dynamics. Those loops would not only connect people with\none another but connect people with machines, and—crucially—machines with\nmachines.\nWiener glimpsed a world in which information could be separated from its\nmedium. People, or machines, could communicate patterns across vast distances and use\nthem to fashion new items at the endpoints, without “moving a…particle of matter from\none end of the line to the other,” a vision now realized in our world of networked 3D\nprinters. Wiener also imagined machine-to-machine feedback loops driving huge\nadvances in automation, even for tasks that had previously relied on human judgment.\n“The machine plays no favorites between manual labor and white-collar labor,” he\nobserved.\nFor all that, many of the central arguments in The Human Use of Human Beings\nseem closer to the 19th century than the 21st. In particular, although Wiener made\nreference throughout to Claude Shannon’s then-new work on information theory, he\nseems not to have fully embraced Shannon’s notion of information as consisting of\nirreducible, meaning-free bits. Since Wiener’s day, Shannon’s theory has come to\nundergird recent advances in “Big Data” and “deep learning,” which makes it all the\nmore interesting to revisit Wiener’s cybernetic imagination. How might tomorrow’s\nartificial intelligence be different if practitioners were to re-invest in Wiener’s guiding\nvision of “information”?\n~ ~ ~\nWhen Wiener wrote The Human Use of Human Beings, his experiences of war-related\nresearch, and of what struck him as the moral ambiguities of intellectual life amid the\nmilitary-industrial complex, were still fresh. Just a few years earlier, he had announced\n109\nin the pages of The Atlantic Monthly that he would not “publish any future work of mine\nwhich may do damage in the hands of irresponsible militarists.” 30 He remained\nambivalent about the transformative power of new technologies, indulging in neither the\nboundless hype nor the digital utopianism of later pundits.\n“Progress imposes not only new possibilities for the future but new restrictions,”\nhe wrote, in Human Use. He was concerned about human-made restrictions as well as\ntechnological ones, especially Cold War restrictions that threatened the flow of\ninformation so critical to cybernetic systems: “Under the impetus of Senator [Joseph]\nMcCarthy and his imitators, the blind and excessive classification of military\ninformation” was driving political leaders in the United States to adopt a “secretive frame\nof mind paralleled in history only in the Venice of the Renaissance.” Wiener, echoing\nmany outspoken veterans of the Manhattan Project, argued that the postwar obsession\nwith secrecy—especially around nuclear weapons—stemmed from a misunderstanding of\nthe scientific process. The only genuine secret about the production of nuclear weapons,\nhe wrote, was whether such bombs could be built. Once that secret had been revealed,\nwith the bombings of Hiroshima and Nagasaki, no amount of state-imposed secrecy\nwould stop others from puzzling through chains of reasoning like those the Manhattan\nProject researchers had followed. As Wiener memorably put it, “There is no Maginot\nLine of the brain.”\nTo drive this point home, Wiener borrowed Shannon’s fresh ideas about\ninformation theory. In 1948, Shannon, a mathematician and engineer working at Bell\nLabs, had published a pair of lengthy articles in the Bell System Technical Journal.\nIntroducing the new work to a broad readership in 1949, mathematician Warren Weaver\nexplained that in Shannon’s formulation, “the word information…is used in a special\nsense that must not be confused with its ordinary usage. In particular, information must\nnot be confused with meaning.” 31 Linguists and poets might be concerned about the\n“semantic” aspects of communication, Weaver continued, but not engineers like\nShannon. Rather, “this word ‘information’ in communication theory relates not so much\nto what you do say, as to what you could say.” In Shannon’s now-famous formulation,\nthe information content of a string of symbols was given by the logarithm of the number\nof possible symbols from which a given string was chosen. Shannon’s key insight was\nthat the information of a message was just like the entropy of a gas: a measure of the\nsystem’s disorder.\nWiener borrowed this insight when composing Human Use. If information was\nlike entropy, then it could not be conserved—or contained. Physicists in the 19th century\nhad demonstrated that the total energy of a physical system must always remain the same,\na perfect balance between the start and the end of a process. Not so for entropy, which\nwould inexorably increase over time, an imperative that came to be known as the second\nlaw of thermodynamics. From that stark distinction—energy is conserved, whereas\nentropy must grow—followed enormous cosmic consequences. Time must flow forward;\n30\nNorbert Wiener, “A Scientist Rebels,” The Atlantic Monthly, January 1947.\n31\nWarren Weaver, “Recent Contributions to the Mathematical Theory of Communication,” in Claude\nShannon & Warren Weaver, The Mathematical Theory of Communication (Urbana, IL: University of\nIllinois Press, 1949), p. 8 (emphasis in original). Shannon’s 1948 papers were republished in the same\nvolume.\n110\nthe future cannot be the same as the past. The universe could even be careening toward a\n“heat death,” some far-off time when the total stock of energy had uniformly dispersed,\nachieving a state of maximum entropy, after which no further change could occur.\nIf information qua entropy could not be conserved, then Wiener concluded it was\nfolly for military leaders to try to stockpile the “scientific know-how of the nation in\nstatic libraries and laboratories.” Indeed, “no amount of scientific research, carefully\nrecorded in books and papers, and then put into our libraries with labels of secrecy, will\nbe adequate to protect us for any length of time in a world where the effective level of\ninformation is perpetually advancing.” Any such efforts at secrecy, classification, or the\ncontainment of information would fail, Wiener argued, just as surely as hucksters’\nschemes for perpetual-motion machines faltered in the face of the second law of\nthermodynamics.\nWiener criticized the American “orthodoxy” of free-market fundamentalism in\nmuch the same way. For most Americans, “questions of information will be evaluated\naccording to a standard American criterion: a thing is valuable as a commodity for what it\nwill bring in the open market.” Indeed, “the fate of information in the typically American\nworld is to become something which can be bought or sold;” most people, he observed,\n“cannot conceive of a piece of information without an owner.” Wiener considered this\nview to be as wrong-headed as rampant military classification. Again he invoked\nShannon’s insight: Since “information and entropy are not conserved,” they are “equally\nunsuited to being commodities.”\n~ ~ ~\nInformation cannot be conserved—so far, so good. But did Wiener really have\nShannon’s “information” in mind? The crux of Shannon’s argument, as Weaver had\nemphasized, was to distinguish a colloquial sense of “information,” as message with\nmeaning, from an abstracted, rarefied notion of strings of symbols arrayed with some\nprobability and selected from an enormous universe of gibberish. For Shannon,\n“information” could be quantified because its fundamental unit, the bit, was a unit of\nconveyance rather than understanding.\nWhen Wiener characterized “information” throughout Human Use, on the other\nhand, he tilted time and again to a classical, humanistic sense of the term. “A piece of\ninformation,” he wrote—tellingly, not a “bit” of information—“in order to contribute to\nthe general information of the community, must say something substantially different\nfrom the community’s previous common stock of information.” This was why\n“schoolboys do not like Shakespeare,” he concluded: The Bard’s couplets may depart\nstarkly from random bitstreams, but they had nonetheless become all too familiar to the\nsense-making public and “absorbed into the superficial clichés of the time.”\nAt least the information content of Shakespeare had once seemed fresh. During\nthe postwar boom years, Wiener fretted, the “enormous per capita bulk of\ncommunication”—ranging across newspapers and movies to radio, television, and\nbooks—had bred mediocrity, an informational reversion to the mean. “More and more\nwe must accept a standardized inoffensive and insignificant product which, like the white\nbread of the bakeries, is made rather for its keeping and selling properties than for its\nfood value.” “Heaven save us,” he pleaded, “from the first novels which are written\nbecause a young man desires the prestige of being a novelist rather than because he has\n111\nsomething to say! Heaven save us likewise from the mathematical papers which are\ncorrect and elegant but without body or spirit.” Wiener’s treatment of “information”\nsounded more like Matthew Arnold in 1869 32 than Claude Shannon in 1948—more\n“body and spirit” than “bit.” Wiener shared Arnold’s Romantic view of the “content\nproducer” as well. “Properly speaking the artist, the writer, and the scientist should be\nmoved by such an irresistible impulse to create that, even if they were not being paid for\ntheir work, they would be willing to pay to get the chance to do it.” L’art pour l’art, that\n19th-century cry: Artists should suffer for their work; the quest for meaningful expression\nshould always trump lucre.\nTo Wiener, this was the proper measure of “information”: body, spirit, aspiration,\nexpression. Yet to argue against its commodification, Wiener reverted again to\nShannon’s mathematics of information-as-entropy.\n~ ~ ~\nFlash forward to our day. In many ways, Wiener has been proved right. His vision of\nnetworked feedback loops driven by machine-to-machine communication has become a\nmundane feature of everyday life. From the earliest stirrings of the Internet Age,\nmoreover, digital piracy has upended the view that “information”—in the form of songs,\nmovies, books, or code—could remain contained. Put up a paywall here, and the content\nwill diffuse over there, all so much informational entropy that cannot be conserved.\nOn the other hand, enormous multinational corporations—some of the largest and\nmost profitable in the world—now routinely disprove Wiener’s contention that\n“information” cannot be stockpiled or monetized. Ironically, the “information” they\ntrade in is closer to Shannon’s definition than Wiener’s, Shannon’s mathematical proofs\nnotwithstanding.\nWhile Google Books may help circulate hundreds of thousands of works of\nliterature for free, Google itself—like Facebook, Amazon, Twitter, and their many\nimitators—has commandeered a baser form of “information” and exploited it for\nextraordinary profit. Petabytes of Shannon-like information—a seemingly meaningless\nstream of clicks, “likes,” and retweets, collected from virtually every person who has ever\ntouched a networked computer—are sifted through proprietary “deep-learning”\nalgorithms to micro-target everything from the advertisements we see to the news stories\n(fake or otherwise) we encounter while browsing the Web.\nBack in the early 1950s, Wiener had proposed that researchers study the\nstructures and limitations of ants—in contrast to humans—so that machines might one\nday achieve the “almost indefinite intellectual expansion” that people (rather than insects)\ncan attain. He found solace in the notion that machines could come to dominate us only\n“in the last stages of increasing entropy,” when “the statistical differences among\nindividuals are nil.” Today’s data-mining algorithms turn Wiener’s approach on its head.\nThey produce profit by exploiting our reptilian brains rather than imitating our cerebral\ncortexes, harvesting information from all our late-night, blog-addled, pleasure-seeking\nclickstreams—leveraging precisely the tiny, residual “statistical differences among\nindividuals.”\n32\nMatthew Arnold, Culture and Anarchy, Jane Garnett, ed. (Oxford, U.K.: Oxford University Press, 2006).\n112\nTo be sure, some recent achievements in artificial intelligence have been\nremarkably impressive. Computers can now produce visual artworks and musical\ncompositions akin to those of recognized masters, creating just the sort of “information”\nthat Wiener most prized. But by far the largest impact on society to date has come from\nthe collection and manipulation of Shannon-like information, which has reshaped our\nshopping habits, political participation, personal relationships, expectations of privacy,\nand more.\nWhat might “deep learning” evolve into, if the fundamental currency becomes\n“information” as Wiener defined it? How might the field shift if re-animated by\nWiener’s deep moral convictions, informed as they were by his prescient concerns about\nrampant militarism, runaway corporate profit-seeking, the self-limiting features of\nsecrecy, and the reduction of human expression to interchangeable commodities?\nPerhaps “deep learning” might then become the cultivation of meaningful information\nrather than the relentless pursuit of potent, if meaningless, bits.\n113\nIn the aforementioned Connecticut discussion on The Human Use of Human Beings, Neil\nGershenfeld provided some fresh air, of a kind, by professing that he hated the book,\nwhich remark was met by universal laughter—as was his observation that computer\nscience was one the worst things to happen to computers, or science. His overall\ncontention was that Wiener missed the implications of the digital revolution that was\nhappening around him—although some would say this charge can’t be leveled at\nsomeone on the ground floor and lacking clairvoyance.\n“The tail wagging the dog of my life,” he told us, “has been Fab Labs and the\nmaker movement, and [when] Wiener talks about the threat of automation he misses the\ninverse, which is that access to the means for automation can empower people, and in\nFab Labs, the corner I’ve been involved in, that’s an exponential.”\nIn 2003, I visited Neil at MIT, where he runs the Center for Bits and Atoms.\nHours later, I emerged from what had been an exuberant display of very weird stuff. He\nshowed me the work of one student in his popular rapid-prototyping class (“How to\nMake Almost Anything”), a sculptor with no engineering background, who had made a\nportable personal space for screaming that saves up your screams and plays them back\nlater. Another student in the class had made a Web browser that lets parrots navigate\nthe Net. Neil himself was doing fundamental research on the roadmap to that sci-fi\nstaple, a “universal replicator.” It was a visit that took me a couple of years to get my\nhead around.\nNeil manages a global network of Fab Labs—small-scale manufacturing systems,\nenabled by digital technologies, which give people the wherewithal to build whatever\nthey’d like. As guru of the maker movement, which merges digital communication and\ncomputation with fabrication, he sometimes feels outside the current heated debate on AI\nsafety. “My ability to do research rests on tools that augment my capabilities,” he says.\n“Asking whether or not they are intelligent is as fruitful as asking how I know I exist—\namusing philosophically, but not testable empirically.” What interests him is “how bits\nand atoms relate—the boundary between digital and physical. Scientifically, it’s the most\nexciting thing I know.”\n114\nSCALING\nNeil Gershenfeld\nNeil Gershenfeld is a physicist and director of MIT’s Center for Bits and Atoms. He is\nthe author of FAB, co-author (with Alan Gershenfeld & Joel Cutcher-Gershenfeld) of\nDesigning Reality, and founder of the global fab lab network.\nDiscussions about artificial intelligence have been oddly ahistorical. They could better\nbe described as manic-depressive; depending on how you count, we’re now in the fifth\nboom-bust cycle. Those swings mask the continuity in the underlying progress and the\nimplications for where it’s headed.\nThe cycles have come in roughly decade-long waves. First there were\nmainframes, which by their very existence were going to automate away work. That ran\ninto the reality that it was hard to write programs to do tasks that were simple for people\nto do. Then came expert systems, which were going to codify and then replace the\nknowledge of experts. These ran into difficulty in assembling that knowledge and\nreasoning about cases not already covered. Perceptrons sought to get around these\nproblems by modeling how the brain learns, but they were unable to do much of\nanything. Multilayer perceptrons could handle test problems that had tripped up those\nsimpler networks, but their demonstrations did poorly on unstructured, real-world\nproblems. We’re now in the deep-learning era, which is delivering on many of the early\nAI promises but in a way that’s considered hard to understand, with consequences\nranging from intellectual to existential threats.\nEach of these stages was heralded as a revolutionary advance over the limitations\nof its predecessors, yet all effectively do the same thing: They make inferences from\nobservations. How these approaches relate can be understood by how they scale—that is,\nhow their performance depends on the difficulty of the problem they’re addressing. Both\na light switch and a self-driving car must determine their operator’s intentions, but the\nformer has just two options to choose from, whereas the latter has many more. The AIboom\nphases have started with promising examples in limited domains; the bust phases\ncame with the failure of those demonstrations to handle the complexity of less-structured,\npractical problems.\nLess apparent is the steady progress we’ve made in mastering scaling. This\nprogress rests on the technological distinction between linear and exponential functions—\na distinction that was becoming evident at the dawn of AI but with implications for AI\nthat weren’t appreciated until many years later.\nIn one of the founding documents of the study of intelligent machines, The\nHuman Use of Human Beings, Norbert Wiener does a remarkable job of identifying many\nof the most significant trends to arise since he wrote it, along with noting the people\nresponsible for them and then consistently failing to recognize why these people’s work\nproved to be so important. Wiener is credited with creating the field of cybernetics; I’ve\nnever understood what that is, but what’s missing from the book is at the heart of how AI\nhas progressed. This history matters because of the echoes of it that persist to this day.\nClaude Shannon makes a cameo appearance in the book, in the context of his\nthoughts about the prospects for a chess-playing computer. Shannon was doing\n115\nsomething much more significant than speculating at the time: He was laying the\nfoundations for the digital revolution. As a graduate student at MIT, he worked for\nVannevar Bush on the Differential Analyzer. This was one of the last great analog\ncomputers, a room full of gears and shafts. Shannon’s frustration with the difficulty of\nsolving problems this way led him in 1937 to write what might be the best master’s thesis\never. In it, he showed how electrical circuits could be designed to evaluate arbitrary\nlogical expressions, introducing the basis for universal digital logic.\nAfter MIT, Shannon studied communications at Bell Labs. Analog telephone\ncalls degraded with distance; the farther they traveled, the worse they sounded. Rather\nthan continue to improve them incrementally, Shannon showed in 1948 that by\ncommunicating with symbols rather than continuous quantities, the behavior is very\ndifferent. Converting speech waveforms to the binary values of 1 and 0 is an example,\nbut many other sets of symbols can be (and are) used in digital communications. What\nmatters is not the particular symbols but rather the ability to detect and correct errors.\nShannon found that if the noise is above a threshold (which depends on the system\ndesign), then there are certain to be errors. But if the noise is below a threshold, then a\nlinear increase in the physical resources representing the symbol results in an exponential\ndecrease in the likelihood of making an error in correctly receiving the symbol. This\nrelationship was the first of what we’d now call a threshold theorem.\nSuch scaling falls off so quickly that the probability of an error can be so small as\nto effectively never happen. Each symbol sent multiplies rather than adds to the\ncertainty, so that the probability of a mistake can go from 0.1 to 0.01 to 0.001, and so\nforth. This exponential decrease in communication errors made possible an exponential\nincrease in the capacity of communication networks. And that eventually solved the\nproblem of where the knowledge in an AI system came from.\nFor many years, the fastest way to speed up a computation was to do nothing—\njust wait for computers to get faster. In the same way, there were years of AI projects\nthat aimed to accumulate everyday knowledge by laboriously entering pieces of\ninformation. That didn’t scale; it could progress only as fast as the number of people\ndoing the entering. But when phone calls, newspaper stories, and mail messages all\nmoved onto the Internet, everyone doing any of those things became a data generator.\nThe result was an exponential rather than a linear rate of knowledge accumulation.\nJohn von Neumann also has a cameo in The Human Use of Human Beings, for\ngame theory. What Wiener missed here was von Neumann’s seminal role in digitizing\ncomputation. Whereas analog communication degraded with distance, analog computing\n(like the Differential Analyzer) degraded with time, accumulating errors as it progressed.\nVon Neumann presented in 1952 a result corresponding to Shannon’s for computation\n(they had met at the Institute for Advanced Study, in Princeton), showing that it was\npossible to compute reliably with an unreliable computing device by using symbols rather\nthan continuous quantities. This was, again, a scaling argument, with a linear increase in\nthe physical resources representing the symbol resulting in an exponential reduction in\nthe error rate as long as the noise was below a threshold. That’s what makes it possible\nto have a billion transistors in a computer chip, with the last one as useful as the first one.\nThis relationship led to an exponential increase in computing performance, which solved\na second problem in AI: how to process exponentially increasing amounts of data.\nThe third problem that scaling solved for AI was coming up with the rules for\n116\nreasoning without having to hire a programmer for each problem. Wiener recognized the\nrole of feedback in machine learning, but he missed the key role of representation. It’s\nnot possible to store all possible images in a self-driving car, or all possible sounds in a\nconversational computer; they have to be able to generalize from experience. The “deep”\npart of deep learning refers not to the (hoped-for) depth of insight but to the depth of the\nmathematical network layers used to make predictions. It turned out that a linear increase\nin network complexity led to an exponential increase in the expressive power of the\nnetwork.\nIf you lose your keys in a room, you can search for them. If you’re not sure\nwhich room they’re in, you have to search all the rooms in a building. If you’re not sure\nwhich building they’re in, you have to search all the rooms in all the buildings in a city.\nIf you’re not sure which city they’re in, you have to search all the rooms in all the\nbuildings in all the cities. In AI, finding the keys corresponds to things like a car safely\nfollowing the road, or a computer correctly interpreting a spoken command, and the\nrooms and buildings and cities correspond to all of the options that have to be considered.\nThis is called the curse of dimensionality.\nThe solution to the curse of dimensionality came in using information about the\nproblem to constrain the search. The search algorithms themselves are not new. But\nwhen applied to a deep-learning network, they adaptively build up representations of\nwhere to search. The price of this is that it’s no longer possible to exactly solve for the\nbest answer to a problem, but typically all that’s needed is an answer that’s good enough.\nTaken together, it shouldn’t be surprising that these scaling laws have allowed\nmachines to become effectively as capable as the corresponding stages of biological\ncomplexity. Neural networks started out with a goal of modeling how the brain works.\nThat goal was abandoned as they evolved into mathematical abstractions unrelated to\nhow neurons actually function. But now there’s a kind of convergence that can be\nthought of as forward- rather than reverse-engineering biology, as the results of deep\nlearning echo brain layers and regions.\nOne of the most difficult research projects I’ve managed paired what we’d now\ncall data scientists with AI pioneers. It was a miserable experience in moving goalposts.\nAs the former progressed in solving long-standing problems posed by the latter, this was\ndeemed to not count because it wasn’t accompanied by corresponding leaps in\nunderstanding the solutions. What’s the value of a chess-playing computer if you can’t\nexplain how it plays chess?\nThe answer of course is that it can play chess. There is interesting emerging\nresearch that is applying AI to AI—that is, training networks to explain how they operate.\nBut both brains and computer chips are hard to understand by watching their inner\nworkings; they’re easily interpreted only by observing their external interfaces. We come\nto trust (or not) brains and computer chips alike based on experience that tests them\nrather than on explanations for how they work.\nMany branches of engineering are making a transition from what’s called\nimperative to declarative or generative design. This means that instead of explicitly\ndesigning a system with tools like CAD files, circuit schematics, and computer code, you\ndescribe what you want the system to do and then an automated search is done for\ndesigns that satisfy your goals and restrictions. This approach becomes necessary as\ndesign complexity exceeds what can be understood by a human designer. While that\n117\nmight sound like a risk, human understanding comes with its own limits; engineering\ndesign is littered with what appeared to be good insights that have had bad consequences.\nDeclarative design rests on all the advances in AI, plus the improving fidelity of\nsimulations to virtually test designs.\nThe mother of all design problems is the one that resulted in us. The way we’re\ndesigned resides in one of the oldest and most conserved parts of the genome, called the\nHox genes. These are genes that regulate genes, in what are called developmental\nprograms. Nothing in your genome stores the design of your body; your genome stores,\nrather, a series of steps to follow that results in your body. This is an exact parallel to\nhow search is done in AI. There are too many possible body plans to search over, and\nmost modifications would be either inconsequential or fatal. The Hox genes are a\nrepresentation of a productive place for evolutionary search. It’s a kind of natural\nintelligence at the molecular level.\nAI has a mind-body problem, in that it has no body. Most work on AI is done in\nthe cloud, running on virtual machines in computer centers where data are funneled. Our\nown intelligence is the result of a search algorithm (evolution) that was able to change\nour physical form as well as our programming—those are inextricably linked. If the\nhistory of AI can be understood as the working of scaling laws rather than a succession of\nfashions, then its future can be seen in the same way. What’s now being digitized, after\ncommunication and computation, is fabrication, bringing the programmability of bits to\nthe world of atoms. By digitizing not just designs but the construction of materials, the\nsame lessons that von Neumann and Shannon taught us apply to exponentially increasing\nfabricational complexity.\nI’ve defined digital materials to be those constructed from a discrete set of parts\nreversibly joined with a discrete set of relative positions and orientations. These\nattributes allow the global geometry to be determined from local constraints, assembly\nerrors to be detected and corrected, heterogeneous materials to be joined, and structures\nto be disassembled rather than disposed of when they’re no longer needed. The amino\nacids that are the foundation of life and the Lego bricks that are the foundation of play\nshare these properties.\nWhat’s interesting about amino acids is that they’re not interesting. They have\nattributes that are typical but not unusual, such as attracting or repelling water. But just\ntwenty types of them are enough to make you. In the same way, twenty or so types of\ndigital-material part types—conducting, insulating, rigid, flexible, magnetic, etc.—are\nenough to assemble the range of functions that go into making modern technologies like\nrobots and computers.\nThe connection between computation and fabrication was foreshadowed by the\nvery pioneers whose work the edifice of computing is based on. Wiener hinted at this by\nlinking material transportation with message transportation. John von Neumann is\ncredited with modern computer architecture, something he actually wrote very little\nabout; the final thing he studied, and wrote about beautifully and at length, was selfreproducing\nsystems. As an abstraction of life, he modeled a machine that can\ncommunicate a computation that constructs itself. And the final thing Alan Turing, who\nis credited with the theoretical framework for computer science, studied was how the\ninstructions in genes can give rise to physical forms. These questions address a topic\nabsent from a typical computer-science education: the physical configuration of a\n118\ncomputation.\nVon Neumann and Turing posed their questions as theoretical studies, because it\nwas beyond the technology of their day to realize them. But with the convergence of\ncommunication and computation with fabrication, these investigations are now becoming\naccessible experimentally. Making an assembler that can assemble itself from the parts\nthat it’s assembling is a focus of my lab, along with collaborations to develop synthetic\ncells.\nThe prospect of physically self-reproducing automata is potentially much scarier\nthan fears of out-of-control AI, because it moves the intelligence out here to where we\nlive. It could be a roadmap leading to Terminator’s Skynet robotic overlords. But it’s\nalso a more hopeful prospect, because an ability to program atoms as well as bits enables\ndesigns to be shared globally while locally producing things like energy, food, and\nshelter—all of these are emerging as exciting early applications of digital fabrication.\nWiener worried about the future of work, but he didn’t question implicit assumptions\nabout the nature of work which are challenged when consumption can be replaced by\ncreation.\nHistory suggests that neither utopian nor dystopian scenarios prevail; we\ngenerally end up muddling along somewhere in between. But history also suggests that\nwe don’t have to wait on history. Gordon Moore in 1965 was able to use five years of the\ndoubling of the specifications of integrated circuits to project what turned out to be fifty\nyears of exponential improvements in digital technologies. We’ve spent many of those\nyears responding to, rather than anticipating, its implications. We have more data\navailable now than Gordon Moore did to project fifty years of doubling the performance\nof digital fabrication. With the benefit of hindsight, it should be possible to avoid the\nexcesses of digital computing and communications this time around, and, from the outset,\naddress issues like access and literacy.\nIf the maker movement is the harbinger of a third digital revolution, the success of\nAI in meeting many of its own early goals can be seen as the crowning achievement of\nthe first two digital revolutions. Although machine making and machine thinking might\nappear to be unrelated trends, they lie in each other’s futures. The same scaling trends\nthat have made AI possible suggest that the current mania is a phase that will pass, to be\nfollowed by something even more significant: the merging of artificial and natural\nintelligence.\nIt was an advance for atoms to form molecules, molecules to form organelles,\norganelles to form cells, cells to form organs, organs to form organisms, organisms to\nform families, families to form societies, and societies to form civilizations. This grand\nevolutionary loop can now be closed, with atoms arranging bits arranging atoms.\n119\nWhile Danny Hillis was an undergraduate at MIT, he built a computer out of Tinkertoys.\nIt has around 10,000 wooden parts, plays tic-tac-toe, and never loses; it’s now in the\nComputer History Museum, in Mountain View, California.\nAs a graduate student at the MIT Computer Science and Artificial Intelligence\nLaboratory in the early 1980s, Danny designed a massively parallel computer with\n64,000 processors. He named it the Connection Machine and founded what may have\nbeen the first AI company—Thinking Machines Corporation—to produce and market it.\nThis was despite a lunch he had with Richard Feynman, at which the celebrated physicist\nremarked, “That is positively the dopiest idea I ever heard.” Maybe “despite” is the\nwrong word, since Feynman had a well-known predilection for playing with dopey ideas.\nIn the event, he showed up on the day the company was incorporated and stayed on, for\nsummer jobs and special assignments, to make invaluable contributions to its work.\nDanny has since established a number of technology companies, of which the\nlatest is Applied Invention, which partners with commercial enterprises to develop\ntechnological solutions to their most intractable problems. He holds hundreds of U.S.\npatents, covering parallel computers, touch interfaces, disk arrays, forgery prevention\nmethods, and a slew of electronic and mechanical devices. His imagination is apparently\nboundless, and here he sketches some possible scenarios that will result from our pursuit\nof a better and better AI.\n“Our thinking machines are more than metaphors,” he says. “The question is not,\n‘Will they be powerful enough to hurt us?’ (they will), or whether they will always act in\nour best interests (they won’t), but whether over the long term they can help us find our\nway—where we come out on the Panacea/Apocalypse continuum.”\n120\nTHE FIRST MACHINE INTELLIGENCES\nW. Daniel Hillis\nW. Daniel “Danny” Hillis is an inventor, entrepreneur, and computer scientist, Judge\nWidney Professor of Engineering and Medicine at USC, and author of The Pattern on the\nStone: The Simple Ideas That Make Computers Work.\nI have spoken of machines, but not only of machines having brains of brass and thews of\niron. When human atoms are knit into an organization in which they are used, not in\ntheir full right as responsible human beings, but as cogs and levers and rods, it matters\nlittle that their raw material is flesh and blood. What is used as an element in a machine,\nis in fact an element in the machine. Whether we entrust our decisions to machines of\nmetal, or to those machines of flesh and blood which are bureaus and vast laboratories\nand armies and corporations, we shall never receive the right answers to our questions\nunless we ask the right questions…. The hour is very late, and the choice of good and\nevil knocks at our door.\n—Norbert Wiener, The Human Use of Human Beings\nNorbert Wiener was ahead of his time in recognizing the potential danger of emergent\nintelligent machines. I believe he was even further ahead in recognizing that the first\nartificial intelligences had already begun to emerge. He was correct in identifying the\ncorporations and bureaus that he called “machines of flesh and blood” as the first\nintelligent machines. He anticipated the dangers of creating artificial superintelligences\nwith goals not necessarily aligned with our own.\nWhat is now clear, whether or not it was apparent to Wiener, is that these\norganizational superintelligences are not just made of humans, they are hybrids of\nhumans and the information technologies that allow them to coordinate. Even in\nWiener’s time, the “bureaus and vast laboratories and armies and corporations” could not\noperate without telephones, telegraphs, radios, and tabulating machines. Today they\ncould not operate without networks of computers, databases, and decision support\nsystems. These hybrid intelligences are technologically augmented networks of humans.\nThese artificial intelligences have superhuman powers. They can know more than\nindividual humans; they can sense more; they can make more complicated analyses and\nmore complex plans. They can have vastly more resources and power than any single\nindividual.\nAlthough we do not always perceive it, hybrid superintelligences such as nation\nstates and corporations have their own emergent goals. Although they are built by and\nfor humans, they often act like independent intelligent entities, and their actions are not\nalways aligned to the interests of the people who created them. The state is not always\nfor the citizen, nor the company for the shareholder. Nor do not-for-profits, religious\norders, or political parties always act in furtherance of their founding principles.\nIntuitively, we recognize that their actions are guided by internal goals, which is why we\npersonify them, both legally and in our habits of thought. When talking about “what\nChina wants,” or “what General Motors is trying to do,” we are not speaking in\nmetaphors. These organizations act as intelligences that perceive, decide, and act. Like\nthe goals of individual humans, the goals of organizations are complex and often selfcontradictory,\nbut they are true goals in the sense that they direct action. Those goals\n121\ndepend somewhat on the goals of the people within the organization, but they are not\nidentical.\nAny American knows how loose the tie is between the actions of the U.S.\ngovernment and the diverse and often contradictory aims of its citizens. That is also true\nof corporations. For-profit corporations nominally serve multiple constituencies,\nincluding shareholders, senior executives, employees, and customers. These corporations\ndiffer in how they balance their loyalties and often behave in ways that serve none of\ntheir constituents. The “neurons” that carry their corporate thought are not just the\nhuman employees or the technologies that connect them; they are also coded into the\npolicies, incentive structures, culture, and procedural habits of the corporation. The\nemergent corporate goals do not always reflect the values of the people who implement\nthem. For instance, an oil company led and staffed by people who care about the\nenvironment may have incentive structures or policies that cause it to compromise\nenvironmental safety for the sake of corporate earnings. The components’ good\nintentions are not a guarantee of the emergent system’s good behavior.\nGovernments and corporations, both built partly of humans, are naturally\nmotivated to at least appear to share the goals of the humans they depend upon. They\ncould not function without the people, so they need to keep them cooperative. When\nsuch organizations appear to behave altruistically, this is often part of their motive. I\nonce complimented the CEO of a large corporation on the contribution his company\nmade toward a humanitarian relief effort. The CEO responded, without a trace of irony,\n“Yes. We have decided to do more things like that to make our brand more likeable.”\nIndividuals who compose a hybrid superintelligence may occasionally exert a\n“humanizing” influence—for example, an employee may break company policies to\naccommodate the needs of another human. The employee may act out of true human\nempathy, but we should not attribute any such empathy to the superintelligence itself.\nThese hybrid machines have goals, and their citizens/customers/employees are some of\nthe resources they use to accomplish them.\nWe are close to being able to build superintelligences out of pure information\ntechnology, without human components. This is what people normally refer to as\n“artificial intelligence,” or AI. It is reasonable to ask what the attitudes of the\nhypothetical machine superintelligences will be toward humans. Will they, too, see\nhumans as useful resources and a good relationship with us as worth preserving? Will\nthey be constructed to have goals that are aligned with our own? Will a superintelligence\neven see these questions as important? What are the “right questions” that we should be\nasking? I believe that one of the most important is this: What relationship will various\nsuperintelligences have to one another?\nIt is interesting to consider how the hybrid superintelligences currently deal with\nconflicts among themselves. Today, much of the ultimate power rests in the nation\nstates, which claim authority over a patch of ground. Whether they are optimized to act\nin the interests of their citizens or those of a despotic ruler, nation states assert priority\nover other intelligences’ desires or goals within their geographic dominion. They claim a\nmonopoly on the use of force and recognize only other nation states as peers. They are\nwilling, if necessary, to demand great sacrifices of their citizens to enforce their authority,\neven to the point of sacrificing their citizens’ lives.\n122\nThis geographical division of authority made logical sense when most of the\nactors were humans who spent their lives within a single nation state, but now that the\nactors of importance include geographically distributed hybrid intelligences such as\nmultinational corporations, that logic is less obvious. Today we live in a complex\ntransitional period, when distributed superintelligences still largely rely on the nation\nstates to settle the arguments arising among them. Often, those arguments are resolved\ndifferently in different jurisdictions. It is becoming more difficult even to assign\nindividual humans to nation states: International travelers living and working outside\ntheir native country, refugees, and immigrants (documented and not) are still dealt with\nas awkward exceptions. Superintelligences built purely of information technology will\nprove even more awkward for the territorial system of authority, since there is no reason\nwhy they need to be tied to physical resources in a single country—or even to any\nparticular physical resources at all. An artificial intelligence might well exist “in the\ncloud” rather than at any physical location.\nI can imagine at least four scenarios for how machine superintelligences will\nrelate to hybrid superintelligences.\nIn one obvious scenario, multiple machine intelligences will ultimately be\ncontrolled by, and allied with, individual nation states. In this state/AI scenario, one can\nenvision American and Chinese super-AIs wrestling each other for resources on behalf of\ntheir state. In some sense, these AIs would be citizens of their nation state in the way that\nmany commercial corporations often act as “corporate citizens” today. In this scenario,\nthe host nation states would presumably give the machine superintelligences the\nresources they needed to work for the state’s advantage. Or, to the degree that the\nsuperintelligences can influence their state governments, they will presumably do so to\nenhance their own power, for instance by garnering a larger share of the state’s resources.\nNation states’ AIs might not want competing AIs to grow up within their jurisdiction. In\nthis scenario, the superintelligences become an extension of the state, and vice versa.\nThe state/AI scenario seems plausible, but it is not our current course. Our most\npowerful and rapidly improving artificial intelligences are controlled by for-profit\ncorporations. This is the corporate/AI scenario, in which the balance of power between\nnation states and corporations becomes inverted. Today, the most powerful and\nintelligent collections of machines are probably owned by Google, but companies like\nAmazon, Baidu, Microsoft, Facebook, Apple, and IBM may not be far behind. These\ncompanies all see a business imperative to build artificial intelligences of their own. It is\neasy to imagine a future in which corporations independently build their own machine\nintelligences, protected within firewalls preventing the machines from taking advantage\nof one another’s knowledge. These machines will be designed to have goals aligned with\nthose of the corporation. If this alignment is effective, nation states may continue to lag\nbehind in developing their own artificial-intelligence capability and instead depend on\ntheir “corporate citizens” to do it for them. To the extent that corporations successfully\ncontrol the goals, they will become more powerful and autonomous than nation states.\nAnother scenario, perhaps the one people fear the most, is that artificial\nintelligences will not be aligned with either humans or hybrid superintelligences but will\nact solely in their own interest. They might even merge into a single machine\nsuperintelligence, since there may be no technical requirement for machine intelligences\nto maintain distinct identities. The attitude of a self-interested super-AI toward hybrid\n123\nsuperintelligences is likely to be competitive. Humans might be seen as minor\nannoyances, like ants at a picnic, but hybrid superintelligences—like corporations,\norganized religions, and nation states—could be existential threats. Like hybrid\nsuperintelligences, AIs might see humans mostly as useful tools to accomplish their\ngoals, as pawns in their competition with the other superintelligences. Or we might\nsimply be irrelevant. It is not impossible that a machine intelligence has already emerged\nand we simply do not recognize it as such. It may not wish to be noticed, or it may be so\nalien to us that we are incapable of perceiving it. This makes the self-interested AI\nscenario the most difficult to imagine. I believe the easy-to-imagine versions, like the\nhumanoid intelligent robots of science fiction, are the least likely. Our most complex\nmachines, like the Internet, have already grown beyond the detailed understanding of a\nsingle human, and their emergent behaviors may be well beyond our ken.\nThe final scenario is that machine intelligences will not be allied with one another\nbut instead will work to further the goals of humanity as a whole. In this optimistic\nscenario, AI could help us restore the balance of power between the individual and the\ncorporation, between the citizen and the state. It could help us solve the problems that\nhave been created by hybrid superintelligences that subvert the goals of humans. In this\nscenario, AIs will empower us by giving us access to processing capacity and knowledge\ncurrently available only to corporations and states. In effect, they could become\nextensions of our own individual intelligences, in furtherance of our human goals. They\ncould make our weak individual intelligences strong. This prospect is both exciting and\nplausible. It is plausible because we have some choice in what we build, and we have a\nhistory of using technology to expand and augment our human capacities. As airplanes\nhave given us wings and engines have given us muscles to move mountains, so our\nnetwork of computers may amplify and extend our minds. We may not fully understand\nor control our destiny, but we have a chance to bend it in the direction of our values. The\nfuture is not something that will happen to us; it is something that we will build.\nWhy Wiener Saw What Others Missed\nThere is in electrical engineering a split which is known in Germany as the split between\nthe technique of strong currents and the technique of weak currents, and which we know\nas the distinction between power and communication engineering. It is this split which\nseparates the age just past from that in which we are now living.\n—Norbert Wiener, Cybernetics, or Control and\nCommunication in the Animal and the Machine\nCybernetics is the study of the how the weak can control the strong. Consider the\ndefining metaphor of the field: the helmsman guiding a ship with a tiller. The\nhelmsman’s goal is to control the heading of the ship, to keep it on the right course. The\ninformation, the message that is sent to the helmsman, comes from the compass or the\nstars, and the helmsman closes the feedback loop by sending the steering messages\nthrough the gentle force of his hand on the tiller. In this picture, we see the ship tossing\nin powerful wind and waves in the real world, controlled by the communication system\nof messages in the world of information.\nYet the distinction between “real” and “information” is mostly a difference in\nperspective. The signals that carry messages, like the light of the stars and pressure of the\n124\nhand on the tiller, exist in a world of energy and forces, as does the helmsman. The weak\nforces that control the rudder are as real and physical as the strong forces that toss the\nship. If we shift our cybernetics perspective from the ship to the helmsman, the pressures\non the rudder become a strong force of muscles controlled by the weak signals in the\nmind of the helmsman. These messages in the helmsman’s mind are amplified into a\nphysical force strong enough to steer the ship. Or instead, we can zoom out and take a\nlarge cybernetics perspective. We might see the ship itself as part of a vast trade\nnetwork, part of a feedback loop that regulates the price of commodities through the flow\nof goods. In this perspective, the tiny ship is merely a messenger. So, the distinction\nbetween the physical world and the information world is a way to describe the\nrelationship between the weak and the strong.\nWiener chose to view the world from the vantage point and scale of the individual\nhuman. As a cyberneticist, he took the perspective of the weak protagonist embedded\nwithin a strong system, trying to make the best of limited powers. He incorporated this\nperspective in his very definition of information. “Information,” he said, “is a name for\nthe content of what is exchanged with the outer world as we adjust to it, and make our\nadjustment felt upon it.” In his words, information is what we use to “live effectively\nwithin that environment.” 33 For Wiener, information is a way for the weak to effectively\ncope with the strong. This viewpoint is also reflected in Gregory Bateson’s definition of\ninformation as “a difference that makes a difference,” by which he meant the small\ndifference that makes a big difference.\nThe goal of cybernetics was to create a tiny model of the system using “weak\ncurrents” to amplify and control “strong currents” of the real world. The central insight\nwas that a control problem could be solved by building an analogous system in the\ninformation space of messages and then amplifying solutions into the larger world of\nreality. Inherent in the motion of a control system is the concept of amplification, which\nmakes the small big and the weak strong. Amplification allows the difference that makes\na difference to make a difference.\nIn this way of looking at the world, a control system needed to be as complex as\nthe system it controlled. Cyberneticist W. Ross Ashby proved that this was true in a\nprecise mathematical sense, in what is now called Ashby’s Law of Requisite Variety, or\nsometimes the First Law of Cybernetics. The law tells us that to control a system\ncompletely, the controller must be as complex as the controlled. Thus cyberneticists\ntended to see control systems as a kind of analog of the systems they governed, like the\nhomunculus—the hypothetical little person inside the brain who controls the actual\nperson.\nThis notion of analogous structure is sometimes confused with the notion of\nanalog encoding of messages, but the two are logically distinct. Norbert Wiener was\nmuch impressed with Vannevar Bush’s Digital Differential Analyzer, which could be\nreconfigured to match the structure of whatever problem it was given to solve but used\ndigital signal encoding. Signals could be simplified to openly represent the relevant\ndistinctions, allowing them to be more accurately communicated and stored. In digital\nsignals, one needed only to preserve the difference in signals that made a difference. It is\nthis distinction and signal coding that we commonly use to distinguish “analog” versus\n“digital.” Digital signal encoding was entirely compatible with cybernetic thinking—in\n33\nThe Human Use of Human Beings (Boston: Houghton Mifflin, 1954), p. 17-18.\n125\nfact, enabling to it. What was constraining to cybernetics was the presumption of an\nanalogy of structure between the controller and the controlled. By the 1930s, Kurt Gödel,\nAlonzo Church, and Alan Turing had all described universal systems of computation, in\nwhich the computation required no structural analogy to functions that were computed.\nThese universal computers could also compute the functions of control.\nThe analogy of structure between the controller and the controlled was central to\nthe cybernetic perspective. Just as digital coding collapses the space of possible\nmessages into a simplified version that represents only the difference that makes a\ndifference, so the control system collapses the state space of a controlled system into a\nsimplified model that reflects only the goals of the controller. Ashby’s Law does not\nimply that every controller must model every state of the system but only those states that\nmatter for advancing the controller’s goals. Thus, in cybernetics, the goal of the\ncontroller becomes the perspective from which the world is viewed.\nNorbert Wiener adopted the perspective of the individual human relating to vast\norganizations and trying to “live effectively within that environment.” He took the\nperspective of the weak trying to influence the strong. Perhaps this is why he was able to\nnotice the emergent goals of the “machines of flesh and blood” and anticipate some of the\nhuman challenges posed by these new intelligences, hybrid machine intelligences with\ngoals of their own.\n126\nVenki Ramakrishnan is a Nobel Prize-winning biologist whose many scientific\ncontributions include his work on the atomic structure of the ribosome—in effect, a huge\nmolecular machine that reads our genes and makes proteins. His work would have been\nimpossible without powerful computers. The Internet made his own work a lot easier\nand, he notes, acted as a leveler internationally: “When I grew up in India, if you wanted\nto get a book, it would show up six months or a year after it had already come out in the\nWest. . . . Journals would arrive by surface mail a few months later. I didn’t have to\ndeal with it, because I left India when I was nineteen, but I know Indian scientists had to\ndeal with it. Today they have access to information at the click of a button. More\nimportant, they have access to lectures. They can listen to Richard Feynman. That\nwould have been a dream of mine when I was growing up. They can just watch Richard\nFeynman on the Web. That’s a big leveling in the field.” And yet. . . “Along with the\nbenefits [of the Web], there is now a huge amount of noise. You have all of these people\nspouting pseudoscientific jargon and pushing their own ideas as if they were science.”\nAs president of the Royal Society, Venki worries, too, about the broader issue of\ntrust: public trust in evidence-based scientific findings, but also trust among scientists,\nbolstered by rigorous checking of one another’s conclusions—trust that is in danger of\neroding because of the “black box” character of deep-learning computers. “This\n[erosion] is going to happen more and more, as data sets get bigger, as we have genomewide\nstudies, population studies, and all sorts of things,” he says. “How do we, as a\nscience community, grapple with this and communicate to the public a sense of what\nscience is about, what is reliable in science, what is uncertain in science, and what is just\nplain wrong in science?”\n127\nWILL COMPUTERS BECOME OUR OVERLORDS?\nVenki Ramakrishnan\nVenki Ramakrishnan is a scientist at the Medical Research Council Laboratory of\nMolecular Biology, Cambridge University; recipient of the Nobel Prize in Chemistry\n(2009); current president of the Royal Society; and the author of Gene Machine: The\nRace to Discover the Secrets of the Ribosome.\nA former colleague of mine, Gérard Bricogne, used to joke that carbon-based intelligence\nwas simply a catalyst for the evolution of silicon-based intelligence. For quite a long\ntime, both Hollywood movies and scientific Jeremiahs have been predicting our eventual\ncapitulation to our computer overlords. We all await the singularity, which always seems\nto be just over the horizon.\nIn a sense, computers have already taken over, facilitating virtually every aspect\nof our lives—from banking, travel, and utilities to the most intimate personal\ncommunication. I can see and talk to my grandson in New York for free. I remember\nwhen I first saw the 1968 movie 2001: A Space Odyssey, the audience laughed at the\nabsurdly cheap cost of a picturephone call from space: $1.70, at a time when a longdistance\ncall within the U.S. was $3 per minute.\nHowever, the convenience and power of computers is also something of a\nFaustian bargain, for it comes with a loss of control. Computers prevent us from doing\nthings we want. Try getting on a flight if you arrive at the airport and the airline\ncomputer systems are down, as happened not so long ago to British Airways at Heathrow.\nThe planes, pilots, and passengers were all there; even the air-traffic controls were\nworking. But no flights for that airline were allowed to take off. Computers also make\nus do things we don’t want—by generating mailing lists and print labels to send us all\nmillions of pieces of unwanted mail, which we humans have to sort, deliver, and dispose\nof.\nBut you ain’t seen nothing yet. In the past, we programmed computers using\nalgorithms we understood at least in principle. So when machines did amazing things\nlike beating world chess champion Garry Kasparov, we could say that the victorious\nprograms were designed with algorithms based on our own understanding—using, in this\ninstance, the experience and advice of top grandmasters. Machines were simply faster at\ndoing brute-force calculations, had prodigious amounts of memory, and were not prone to\nerrors. One article described Deep Blue’s victory not as that of a computer, which was\njust a dumb machine, but as the victory of hundreds of programmers over Kasparov, a\nsingle individual.\nThat way of programming is changing dramatically. After a long hiatus, the\npower of machine learning has taken off. Much of the change came when programmers,\nrather than trying to anticipate and code for every possible contingency, allowed\ncomputers to train themselves on data, using deep neural networks based on models of\nhow our own brains learn. They use probabilistic methods to “learn” from large\nquantities of data; computers can recognize patterns and come up with conclusions on\ntheir own. A particularly powerful method is called reinforcement learning, by which the\ncomputer learns, without prior input, which variables are important and how much to\n128\nweight them to reach a certain goal. This method in some sense mimics how we learn as\nchildren. The results from these new approaches are amazing.\nSuch a deep-learning program was used to teach a computer to play Go, a game\nthat only a few years ago was thought to be beyond the reach of AI because it was so\nhard to calculate how well you were doing. It seemed that top Go players relied a great\ndeal on intuition and a feel for position, so proficiency was thought to require a\nparticularly human kind of intelligence. But the AlphaGo program produced by\nDeepMind, after being trained on thousands of high-level Go games played by humans\nand then millions of games with itself, was able to beat the top human players in short\norder. Even more amazingly, the related AlphaGo Zero program, which learned from\nscratch by playing itself, was stronger than the version trained initially on human games!\nIt was as though the humans had been preventing the computer from reaching its true\npotential. The same method has recently been generalized: Starting from scratch, within\njust twenty-four hours, an equivalent AlphaZero chess program was able to beat today’s\ntop “conventional” chess programs, which in turn have beaten the best humans.\nProgress has not been restricted to games. Computers are significantly better at\nimage and voice recognition and speech synthesis than they used to be. They can detect\ntumors in radiographs earlier than most humans. Medical diagnostics and personalized\nmedicine will improve substantially. Transportation by self-driving cars will keep us all\nsafer, on average. My grandson may never have to acquire a driver’s license, because\ndriving a car will be like riding a horse today—a hobby for the few. Dangerous\nactivities, such as mining, and tedious repetitive work will be done by computers.\nGovernments will offer better targeted, more personalized and efficient public services.\nAI could revolutionize education by analyzing an individual pupil’s needs and enabling\ncustomized teaching, so that each student can advance at an optimal rate.\nAlong with these huge benefits, of course, will come alarming risks. With the\nvast amounts of personal data, computers will learn more about us than we may know\nabout ourselves; the question of who owns data about us will be paramount. Moreover,\ndata-based decisions will undoubtedly reflect social biases: Even an allegedly neutral\nintelligent system designed to predict loan risks, say, may conclude that mere\nmembership in a particular minority group makes you more likely to default on a loan.\nWhile this is an obvious example that we could correct, the real danger is that we are not\nalways aware of biases in the data and may simply perpetuate them.\nMachine learning may also perpetuate our own biases. When Netflix or Amazon\ntries to tell you what you might want to watch or buy, this is an application of machine\nlearning. Currently such suggestions are sometimes laughable, but with time and more\ndata they will get increasingly accurate, reinforcing our prejudices and likes and dislikes.\nWill we miss out on the random encounter that might persuade us to change our views by\nexposing us to new and conflicting ideas? Social media, given its influence on elections,\nis a particularly striking illustration of how the divide between people on different sides\nof the political spectrum can be accentuated.\nWe may have already reached the stage where most governments are powerless to\nresist the combined clout of a few powerful multinational companies that control us and\nour digital future. The fight between dominant companies today is really a fight for\ncontrol over our data. They will use their enormous influence to prevent regulation of\ndata, because their interests lie in unfettered control of it. Moreover, they have the\n129\nfinancial resources to hire the most talented workers in the field, enhancing their power\neven further. We have been giving away valuable data for the sake of freebies like Gmail\nand Facebook, but as the journalist and author John Lanchester has pointed out in the\nLondon Review of Books, if it is free, then you are the product. Their real customers are\nthe ones who pay them for access to knowledge about us, so that they can persuade us to\nbuy their products or otherwise influence us. One way around the monopolistic control\nof data is to split the ownership of data away from firms that use them. Individuals\nwould instead own and control access to their personal data (a model that would\nencourage competition, since people would be free to move their data to a company that\noffered better services). Finally, abuse of data is not limited to corporations: In\ntotalitarian states, or even nominally democratic ones, governments know things about\ntheir citizens that Orwell could not have imagined. The use they make of this\ninformation may not always be transparent or possible to counter.\nThe prospect of AI for military purposes is frightening. One can imagine\nintelligent systems being designed to act autonomously based on real-time data and able\nto act faster than the enemy, starting catastrophic wars. Such wars may not necessarily\nbe conventional or even nuclear wars. Given how essential computer networks are to\nmodern society, it is much more likely that AI wars will be fought in cyberspace. The\nconsequences could be just as dire.\n~ ~ ~\nDespite this loss of control, we continue to march inexorably into a world in which AI\nwill be everywhere: Individuals won’t be able to resist its convenience and power, and\ncorporations and governments won’t be able to resist its competitive advantages. But\nimportant questions arise about the future of work. Computers have been responsible for\nconsiderable losses in blue-collar jobs in the last few decades, but until recently many\nwhite-collar jobs—jobs that “only humans can do”—were thought to be safe. Suddenly\nthat no longer appears to be true. Accountants, many legal and medical professionals,\nfinancial analysts and stockbrokers, travel agents—in fact, a large fraction of white-collar\njobs—will disappear as a result of sophisticated machine-learning programs. We face a\nfuture in which factories churn out goods with very few employees and the movement of\ngoods is largely automated, as are many services. What’s left for humans to do?\nIn 1930—long before the advent of computers, let alone AI—John Maynard\nKeynes wrote, in an essay called “Economic Possibilities for our Grandchildren,” that as\na result of improvements in productivity, society could produce all its needs with a\nfifteen-hour work week. He also predicted, along with the growth of creative leisure, the\nend of money and wealth as a goal:\nWe shall be able to afford to dare to assess the money-motive at its true value.\nThe love of money as a possession—as distinguished from the love of money as\na means to the enjoyments and realities of life—will be recognised for what it is,\na somewhat disgusting morbidity, one of those semi-criminal, semi-pathological\npropensities which one hands over with a shudder to the specialists in mental\ndisease.\n130\nSadly, Keynes’s predictions did not come true. Although productivity did indeed\nincrease, the system—possibly inherent in a market economy—did not result in humans\nworking much shorter hours. Rather, what happened is what the anthropologist and\nanarchist David Graeber describes as the growth of “bullshit jobs.” 34 While jobs that\nproduce essentials like food, shelter, and goods have been largely automated away, we\nhave seen an enormous expansion of sectors like corporate law, academic and health\nadministration (as opposed to actual teaching, research, and the practice of medicine),\n“human resources,” and public relations, not to mention new industries like financial\nservices and telemarketing and ancillary industries in the so-called gig economy which\nserve those who are too busy doing all that additional work.\nHow will societies cope with technology’s increasingly rapid destruction of entire\nprofessions and throwing large numbers of people out of work? Some argue that this\nconcern is based on a false premise, because new jobs spring up that didn’t exist before,\nbut as Graeber points out, these new jobs won’t necessarily be rewarding or fulfilling.\nDuring the first industrial revolution, it took almost a century before most people were\nbetter off. That revolution was possible only because the government of the time\nruthlessly favored property rights over labor, and most people (and all women) did not\nhave the vote. In today’s democratic societies, it is not clear that the population will\ntolerate such a dramatic upheaval of society based on the promise that “eventually”\nthings will get better.\nEven that rosy vision will depend on a radical shake-up of education and lifelong\nlearning. The Industrial Revolution did trigger enormous social change of this kind,\nincluding a shift to universal education. But it will not happen unless we make it happen:\nThis is essentially about power, agency, and control. What’s next for, say, the forty-yearold\ntaxi driver or truck driver in an era of autonomous vehicles?\nOne idea that has been touted is that of a universal basic income, which will allow\ncitizens to pursue their interests, retrain for new occupations, and generally be free to live\na decent life. However, market economies, which are predicated on growing consumer\ndemand over all else, may not tolerate this innovation. There is also a feeling among\nmany that meaningful work is essential to human dignity and fulfillment. So another\npossibility is that the enormous wealth generated by increased productivity due to\nautomation could be redistributed to jobs requiring human labor and creativity in fields\nsuch as the arts, music, social work, and other worthwhile pursuits. Ultimately, which\njobs are rewarding or productive and which are “bullshit” is a matter of judgment and\nmay vary from society to society, as well as over time.\n~ ~ ~\nSo far, I’ve focused on AI’s practical consequences. As a scientist, what bothers me is\nour potential loss of understanding. We are now accumulating data at an incredible rate.\nIn my own lab, an experiment generates over a terabyte of data a day. These data are\nmassaged, analyzed, and reduced until there is an interpretable result. But in all of this\ndata analysis, we believe we know what’s happening. We know what the programs are\n34\nhttps://strikemag.org/bullshit-jobs/\n131\ndoing because we designed the algorithms at their heart. So when our computers\ngenerate a result, we feel that we intellectually grasp it.\nThe new machine-learning programs are different. Having recognized patterns\nvia deep neural networks, they come up with conclusions, and we have no idea exactly\nhow. When they uncover relationships, we don’t understand it in the same way as if we\nhad deduced those relationships ourselves using an underlying theoretical framework. As\ndata sets become larger, we won’t be able to analyze them ourselves even with the help\nof computers; rather, we will rely entirely on computers to do the analysis for us. So if\nsomeone asks us how we know something, we will simply say it is because the machine\nanalyzed the data and produced the conclusion.\nOne day a computer may well come up with an entirely new result—e.g., a\nmathematical theorem whose proof, or even whose statement, no human can understand.\nThat is philosophically different from the way we have been doing science. Or at least\nthought we had; some might argue that we don’t know how our own brains reach\nconclusions either, and that these new methods are a way of mimicking learning by the\nhuman brain. Nevertheless, I find this potential loss of understanding disturbing.\nDespite the remarkable advances in computing, the hype about AGI—a generalintelligence\nmachine that will think like a human and possibly develop consciousness—\nsmacks of science fiction to me, partly because we don’t understand the brain at that level\nof detail. Not only do we not understand what consciousness is, we don’t even\nunderstand a relatively simple problem like how we remember a phone number. In just\nthat one question, there are all sorts of things to consider. How do we know it is a\nnumber? How do we associate it with a person, a name, face, and other characteristics?\nEven such seemingly trivial questions involve everything from high-level cognition and\nmemory to how a cell stores information and how neurons interact.\nMoreover, that’s just one task among many that the brain does effortlessly.\nWhereas machines will no doubt do ever more amazing things, they’re unlikely to be a\nreplacement for human thought and human creativity and vision. Eric Schmidt, former\nchairman of Google’s parent company, said in a recent interview at the London Science\nMuseum that even designing a robot that would clear the table, wash the dishes, and put\nthem away was a huge challenge. The calculations involved in figuring out all the\nmovements the body has to make to throw a ball accurately or do slalom skiing are\nprodigious. The brain can do all these and also do mathematics and music, and invent\ngames like chess and Go, not just play them. We tend to underestimate the complexity\nand creativity of the human brain and how amazingly general it is.\nIf AI is to become more humanlike in its abilities, the machine-learning and\nneuroscience communities need to interact closely, something that is happening already.\nSome of today’s greatest exponents of machine learning—such as Geoffrey Hinton,\nZoubin Ghahramani, and Demis Hassabis—have backgrounds in cognitive neuroscience,\nand their success has been at least in part due to attempts to model brainlike behavior in\ntheir algorithms. At the same time, neurobiology has also flourished. All sorts of tools\nhave been developed to watch which neurons are firing and genetically manipulate them\nand see what’s happening in real time with inputs. Several countries have launched\nmoon-shot neuroscience initiatives to see if we can crack the workings of the brain.\nAdvances in AI and neuroscience seem to go hand in hand; each field can propel the\nother.\n132\nMany evolutionary scientists, and such philosophers as Daniel Dennett, have\npointed out that the human brain is the result of billions of years of evolution. 35 Human\nintelligence is not the special characteristic we think it is, but just another survival\nmechanism not unlike our digestive or immune systems, both of which are also\namazingly complex. Intelligence evolved because it allowed us to make sense of the\nworld around us, to plan ahead, and thus cope with all sorts of unexpected things in order\nto survive. However, as Descartes stated, we humans define our very existence by our\nability to think. So it is not surprising that, in an anthropomorphic way, our fears about\nAI reflect this belief that our intelligence is what makes us special.\nBut if we step back and look at life on Earth, we see that we are far from the most\nresilient species. If we’re going to be taken over at some point, it will be by some of\nEarth’s oldest life-forms, like bacteria, which can live anywhere from Antarctica to deepsea\nthermal vents hotter than boiling water, or in acid environments that would melt you\nand me. So when people ask where we’re headed, we need to put the question in a\nbroader context. I don’t know what sort of future AI will bring: whether AI will make\nhumans subservient or obsolete or will be a useful and welcome enhancement of our\nabilities which will enrich our lives. But I am reasonably certain that computers will\nnever be the overlords of bacteria.\n35\nSee, for example, Dennett’s From Bacteria to Bach and Back: The Evolution of Minds (New York: W.\nW. Norton, 2017).\n133\nAlex “Sandy” Pentland, an exponent of what he has termed “social physics,” is\ninterested in building powerful human-AI ecologies. He is concerned at the same time\nabout the potential dangers of decision-making systems in which the data in effect take\nover and human creativity is relegated to the background.\nThe advent of Big Data, he believes, has given us the opportunity to reinvent our\ncivilization: “We can now begin to actually look at the details of social interaction and\nhow those play out, and we’re no longer limited to averages like market indices or\nelection results. This is an astounding change. The ability to see the details of the\nmarket, of political revolutions, and to be able to predict and control them is definitely a\ncase of Promethean fire—it could be used for good or for ill. Big Data brings us to\ninteresting times.”\nAt our group meeting in Washington, Connecticut, he confessed that reading\nNorbert Wiener on the concept of feedback “felt like reading my own thoughts.”\n“After Wiener, people discovered or focused on the fact that there are genuinely\nchaotic systems that are just not predictable,” he said, “but if you look at human\nsocioeconomic systems, there is a large percentage of variance you can account for and\npredict. . . . Today there is data from all sorts of digital devices, and from all of our\ntransactions. The fact that everything is datafied means you can measure things in real\ntime in most aspects of human life—and increasingly in every aspect of human life. The\nfact that we have interesting computers and machine-learning techniques means that you\ncan build predictive models of human systems in ways you could never do before.”\n134\nTHE HUMAN STRATEGY\nAlex “Sandy” Pentland\nAlex “Sandy” Pentland is Toshiba Professor and professor of media arts and sciences,\nMIT; director of the Human Dynamics and Connection Science labs and the Media Lab\nEntrepreneurship Program, and the author of Social Physics.\nIn the last half-century, the idea of AI and intelligent robots has dominated thinking about\nthe relationship between humans and computers. In part, this is because it’s easy to tell\nthe stories about AI and robots, and in part because of early successes (e.g., theorem\nprovers that reproduced most of Whitehead and Russell’s Principia Mathematica) and\nmassive military funding. The earlier and broader vision of cybernetics, which\nconsidered the artificial as part of larger systems of feedback and mutual influence, faded\nfrom public awareness.\nHowever, in the intervening years the cybernetics vision has slowly grown and\nquietly taken over—to the point where it is “in the air.” State-of-the-art research in most\nengineering disciplines is now framed as feedback systems that are dynamic and driven\nby energy flows. Even AI is being recast as human/machine “advisor” systems, and the\nmilitary is beginning large-scale funding in this area—something that should perhaps\nworry us more than drones and independent humanoid robots.\nBut as science and engineering have adopted a more cybernetics-like stance, it has\nbecome clear that even the vision of cybernetics is far too small. It was originally\ncentered on the embeddedness of the individual actor but not on the emergent properties\nof a network of actors. This is unsurprising, because the mathematics of networks did not\nexist until recently, so a quantitative science of how networks behave was impossible.\nWe now know that study of the individual does not produce understanding of the system\nexcept in certain simple cases. Recent progress in this area was foreshadowed by\nunderstanding that “chaos,” and later “complexity,” were the typical behavior of systems,\nbut we can now go far beyond these statistical understandings.\nWe’re beginning to be able to analyze, predict, and even design the emergent\nbehavior of complex heterogeneous networks. The cybernetics view of the connected\nindividual actor can now be expanded to cover complex systems of connected individuals\nand machines, and the insights we obtain from this broader view are fundamentally\ndifferent from those obtained from the cybernetics view. Thinking about the network is\nanalogous to thinking about entire ecosystems. How would you guide ecosystems to\ngrow in a good direction? What do you even mean by “a good direction”? Questions\nlike this are beyond the boundary of traditional cybernetic thinking.\nPerhaps the most stunning realization is that humans are already beginning to use\nAI and machine learning to guide entire ecosystems, including ecosystems of people, thus\ncreating human-AI ecologies. Now that everything is becoming “datafied,” we can\nmeasure most aspects of human life and, increasingly, aspects of all life. This, together\nwith new, powerful machine-learning techniques, means that we can build models of\nthese ecologies in ways we couldn’t before. Well-known examples are weather- and\ntraffic-prediction models, which are being extended to predict the global climate and plan\ncity growth and renewal. AI-aided engineering of the ecologies is already here.\n135\nDevelopment of human-AI ecosystems is perhaps inevitable for a social species\nsuch as ourselves. We became social early in our evolution, millions of years ago. We\nbegan exchanging information with one another to stay alive, to increase our fitness. We\ndeveloped writing to share abstract and complex ideas, and most recently we’ve\ndeveloped computers to enhance our communication abilities. Now we’re developing AI\nand machine-learning models of ecosystems and sharing the predictions of those models\nto jointly shape our world through new laws and international agreements.\nWe live in an unprecedented historic moment, in which the availability of vast\namounts of human behavioral data and advances in machine learning enable us to tackle\ncomplex social problems through algorithmic decision making. The opportunities for\nsuch a human-AI ecology to have positive social impact through fairer and more\ntransparent decisions are obvious. But there are also risks of a “tyranny of algorithms,”\nwhere unelected data experts are running the world. The choices we make now are\nperhaps even more momentous than those we faced in the 1950s, when AI and\ncybernetics were created. The issues look similar, but they’re not. We have moved down\nthe road, and now the scope is larger. It’s not just AI robots versus individuals. It’s AI\nguiding entire ecologies.\n~ ~ ~\nHow can we make a good human-artificial ecosystem, something that’s not a machine\nsociety but a cyberculture in which we can all live as humans—a culture with a human\nfeel to it? We don’t want to think small—for example, to talk only of robots and selfdriving\ncars. We want this to be a global ecology. Think Skynet-size. But how would\nyou make Skynet something that’s about the human fabric?\nThe first thing to ask is: What’s the magic that makes the current AI work?\nWhere is it wrong and where is it right?\nThe good magic is that it has something called the credit-assignment function.\nWhat that lets you do is take “stupid neurons”—little linear functions—and figure out, in\na big network, which ones are doing the work and strengthen them. It’s a way of taking a\nrandom bunch of switches all hooked together in a network and making them smart by\ngiving them feedback about what works and what doesn’t. This sounds simple, but\nthere’s some complicated math around it. That’s the magic that makes current AI work.\nThe bad part of it is, because those little neurons are stupid, the things they learn\ndon’t generalize very well. If an AI sees something it hasn’t seen before, or if the world\nchanges a little bit, the AI is likely to make a horrible mistake. It has absolutely no sense\nof context. In some ways, it’s as far from Norbert Wiener’s original notion of\ncybernetics as you can get, because it isn’t contextualized; it’s a little idiot savant.\nBut imagine that you took away those limitations: Imagine that instead of using\ndumb neurons, you used neurons in which real-world knowledge was embedded. Maybe\ninstead of linear neurons, you used neurons that were functions in physics, and then you\ntried to fit physics data. Or maybe you put in a lot of knowledge about humans and how\nthey interact with one another—the statistics and characteristics of humans.\nWhen you add this background knowledge and surround it with a good creditassignment\nfunction, then you can take observational data and use the credit-assignment\nfunction to reinforce the functions that are producing good answers. The result is an AI\nthat works extremely well and can generalize. For instance, in solving physical\n136\nproblems, it often takes only a couple of noisy data points to get something that’s a\nbeautiful description of a phenomenon, because you’re putting in knowledge about how\nphysics works. That’s in huge contrast to normal AI, which requires millions of training\nexamples and is very sensitive to noise. By adding the appropriate background\nknowledge, you get much more intelligence.\nSimilar to the physical-systems case, if we make neurons that know a lot about\nhow humans learn from each other, then we can detect human fads and predict human\nbehavior trends in surprisingly accurate and efficient ways. This “social physics” works\nbecause human behavior is determined as much by the patterns of our culture as by\nrational, individual thinking. These patterns can be described mathematically and\nemployed to make accurate predictions.\nThis idea of a credit-assignment function reinforcing connections between\nneurons that are doing the best work is the core of current AI. If you make those little\nneurons smarter, the AI gets smarter. So, what would happen if we replaced the neurons\nwith people? People have lots of capabilities. They know lots of things about the world;\nthey can perceive things in a broadly competent, human way. What would happen if you\nhad a network of people in which you could reinforce the connections that were helping\nand minimize the connections that weren’t?\nThat begins to sound like a society, or a company. We all live in a human social\nnetwork. We’re reinforced for doing things that seem to help everybody and discouraged\nfrom doing things that are not appreciated. Culture is the result of this sort of human AI\nas applied to human problems; it is the process of building social structures by\nreinforcing the good connections and penalizing the bad. Once you’ve realized you can\ntake this general AI framework and create a human AI, the question becomes, What’s the\nright way to do that? Is it a safe idea? Is it completely crazy?\nMy students and I are looking at how people make decisions, on huge databases\nof financial decisions, business decisions, and many other sorts of decisions. What we’ve\nfound is that humans often make decisions in a way that mimics AI credit-assignment\nalgorithms and works to make the community smarter. A particularly interesting feature\nof this work is that it addresses a classic problem in evolution known as the groupselection\nproblem. The core of this problem is: How can we select for culture in\nevolution, when it’s the individuals that reproduce? What you need is something that\nselects for the best cultures and the best groups but also selects for the best individuals,\nbecause they’re the units that transmit the genes.\nWhen you frame the question this way and go through the mathematical literature,\nyou discover that there’s one generally best way to do this. It’s called “distributed\nThompson sampling,” a mathematical algorithm used in choosing, out of a set of possible\nactions with unknown payoffs, the action that maximizes the expected reward in respect\nto the actions. The key is social sampling, a way of combining evidence, of exploring\nand exploiting at the same time. It has the unusual property of simultaneously being the\nbest strategy both for the individual and for the group. If you use the group as the basis\nof selection, and then the group either gets wiped out or reinforced, you’re also selecting\nfor successful individuals. If you select for individuals, and each individual does what’s\ngood for him or her, then that’s automatically the best thing for the group. It’s an\namazing alignment of interests and utilities, and it provides real insight into the question\nof how culture fits into natural selection.\n137\nSocial sampling, very simply, is looking around you at the actions of people who\nare like you, finding what’s popular, and then copying it if it seems like a good idea to\nyou. Idea propagation has this popularity function driving it, but individual adoption also\nis about figuring out how the idea works for the individual—a reflective attitude. When\nyou combine social sampling and personal judgment, you get superior decision making.\nThat’s amazing, because now we have a mathematical recipe for doing with humans what\nall those AI techniques are doing with dumb computer neurons. We have a way of\nputting people together to make better decisions, given more and more experience.\nSo, what happens in the real world? Why don’t we do this all the time? Well,\npeople are good at it, but there are ways it can run amok. One of these is through\nadvertising, propaganda, or “fake news.” There are many ways to get people to think\nsomething is popular when it’s not, and this destroys the usefulness of social sampling.\nThe way you can make groups of people smarter, the way you can make human AI, will\nwork only if you can get feedback to them that’s truthful. It must be grounded on\nwhether each person’s actions worked for them or not.\nThat’s the key to AI mechanisms, too. What they do is analyze whether they\nperformed correctly. If so, plus one; if not, minus one. We need that truthful feedback to\nmake this human mechanism work well, and we need good ways of knowing about what\nother people are doing so that we can correctly assess popularity and the likelihood of\nthis being a good choice.\nThe next step is to build this credit-assignment function, this feedback function,\nfor people, so that we can make a good human-artificial ecosystem—a smart organization\nand a smart culture. In a way, we need to duplicate some of the early insights that\nresulted in, for instance, the U.S. census—trying to find basic facts that everybody can\nagree on and understand so that the transmission of knowledge and culture can happen in\na way that’s truthful and social sampling can function efficiently.\nWe can address the problem of building an accurate credit-assignment function in\nmany different settings. In companies, for instance, it can be done with digital ID badges\nthat reveal who’s connected to whom, so that we can assess the pattern of connections in\nrelation to the company’s results on a daily or weekly basis. The credit-assignment\nfunction asks whether those connections helped solve problems, or helped invent new\nsolutions, and reinforces the helpful connections. When you can get that feedback\nquantitatively—which is difficult, because most things aren’t measured quantitatively—\nboth the productivity and the innovation rate within the organization can be significantly\nimproved. This is, for instance, the basis of Toyota’s “continuous improvement” method.\nA next step is to try to do the same thing but at scale, something I refer to as\nbuilding a trust network for data. It can be thought of as a distributed system like the\nInternet, but with the ability to quantitatively measure and communicate the qualities of\nhuman society, in the same way that the U.S. census does a pretty good job of telling us\nabout population and life expectancy. We are already deploying prototype examples of\ntrust networks at scale in several countries, based on the data and measurement standards\nlaid out in the U.N. Sustainable Development Goals.\nOn the horizon is a vision of how we can make humanity more intelligent by\nbuilding a human AI. It’s a vision composed of two threads. One is data that we can all\ntrust—data that have been vetted by a broad community, data where the algorithms are\nknown and monitored, much like the census data we all automatically rely on as at least\n138\napproximately correct. The other is a fair, data-driven assessment of public norms,\npolicy, and government, based on trusted data about current conditions. This second\nthread depends on availability of trusted data and so is just beginning to be developed.\nTrusted data and data-driven assessment of norms, policy, and government together\ncreate a credit-assignment function that improves societies’ overall fitness and\nintelligence.\nIt is precisely at the point of creating greater societal intelligence where fake\nnews, propaganda, and advertising all get in the way. Fortunately, trust networks give us\na path forward to building a society more resistant to echo-chamber problems, these fads,\nthese exercises in madness. We have begun to develop a new way of establishing social\nmeasurements, in aid of curing some of the ills we see in society today. We’re using\nopen data from all sources, encouraging a fair representation of the things people are\nchoosing, in a curated mathematical framework that can stamp out the echoes and the\nattempts to manipulate us.\nOn Polarization and Inequality\nExtreme polarization and segregation by income are almost everywhere in the world\ntoday and threaten to tear governments and civil society apart. Increasingly, the media\nare becoming adrenaline pushers driven by advertising clicks and failing to deliver\nbalanced facts and reasoned discourse—and the degradation of media is causing people\nto lose their bearings. They don’t know what to believe, and thus they can easily be\nmanipulated. There is a real need to ground our various cultures in trustworthy, datadriven\nstandards that we all agree on, and to be able to know what behaviors and policies\nwork and which don’t.\nIn converting to a digital society, we’ve lost touch with traditional notions of truth\nand justice. Justice used to be mostly informal and normative. We’ve now formalized it.\nAt the same time, we’ve put it out of reach for most people. Our legal systems are failing\nus in a way they didn’t before, precisely because they’re now more formal, more digital,\nless embedded in society.\nIdeas about justice are very different around the world. One of the core\ndifferentiators is this: Do you or your parents remember when the bad guys came with\nguns and took everything? If you do, your attitude about justice is different from that of\nthe average reader of this essay. Do you come from the upper classes? Or were you\nsomebody who saw the sewers from the inside? Your view of justice depends on your\nhistory.\nA common test I have for U.S. citizens is this: Do you know anybody who owns a\npickup truck? It’s the number-one-selling vehicle in the United States, and if you don’t\nknow people like that, you’re out of touch with more than 50 percent of Americans.\nPhysical segregation drives conceptual segregation. Most of America thinks of justice\nand access and fairness in terms very different from those of the typical, say,\nManhattanite.\nIf you look at patterns of mobility—where people go—in a typical city, you find\nthat the people in the top quintile (white-collar working families) and bottom quintile\n(people who are sometimes on unemployment or welfare) almost never talk to each other.\nThey don’t go to the same places; they don’t talk about the same things. They all live in\n139\nthe same city, nominally, but it’s as if it were two completely different cities—and this is\nperhaps the most important cause of today’s plague of polarization.\nOn Extreme Wealth\nSome two hundred of the world’s wealthiest people have pledged to give away more than\n50 percent of their wealth either during their lifetimes or in their wills, creating a plurality\nof voices in the foundation space. 36 Bill Gates is probably the most familiar example.\nHe’s decided that if the government won’t do it, he’ll do it. You want mosquito nets?\nHe’ll do it. You want antivirals? He’ll do it. We’re getting different stakeholders to take\naction in the form of foundations dedicated to public good, and they have different\nversions of what they consider the public good. This diversity of goals has created a lot\nof what’s wonderful about the world today. Actions from outside government by\norganizations like the Ford Foundation and the Sloan Foundation, who bet on things that\nnobody else would bet on, have changed the world for the better.\nSure, these billionaires are human, with human foibles, and all is not necessarily\nas it should be. On the other hand, the same situation obtained when the railways were\nfirst built. Some people made huge fortunes. A lot of people went bust. We, the average\npeople, got railways out of it. That’s good. Same thing with electric power; same thing\nwith many new technologies. There’s a churning process that throws somebody up and\nlater casts them or their heirs down. Bubbles of extreme wealth were a feature of the late\n1800s and early 1900s when steam engines and railways and electric lights were\ninvented. The fortunes they created were all gone within two or three generations.\nIf the U.S. were like Europe, I would worry. What you find in Europe is that the\nsame families have held on to wealth for hundreds of years, so they’re entrenched not just\nin terms of wealth but of the political system and in other ways. But so far, the U.S. has\navoided this kind of hereditary class system. Extreme wealth hasn’t stuck, which is good.\nIt shouldn’t stick. If you win the lottery, you get your billion dollars, but your grandkids\nought to work for a living.\nOn AI and Society\nPeople are scared about AI. Perhaps they should be. But they need to realize that AI\nfeeds on data. Without data, AI is nothing. You don’t have to watch the AI; instead you\nshould watch what it eats and what it does. The trust-network framework we’ve set up,\nwith the help of nations in the E.U. and elsewhere, is one where we can have our\nalgorithms, we can have our AI, but we get to see what went in and what went out, so that\nwe can ask, Is this a discriminatory decision? Is this the sort of thing that we want as\nhumans? Or is this something that’s a little weird?\nThe most revealing analogy is that regulators, bureaucracies, and parts of the\ngovernment are very much like AIs: They take in the rules that we call law and\nregulation, and they add government data, and they make decisions that affect our lives.\nThe part that’s bad about the current system is that we have very little oversight of these\ndepartments, regulators, and bureaucracies. The only control we have is the vote—the\nopportunity to elect somebody different. We need to make oversight of bureaucracies a\nlot more fine-grained. We need to record the data that went into every single decision\n36\nhttps://givingpledge.org/About.aspx.\n140\nand have the results analyzed by the various stakeholders—rather like elected legislatures\nwere originally intended to do.\nIf we have the data that go into and out of each decision, we can easily ask, Is this\na fair algorithm? Is this AI doing things that we as humans believe are ethical? This\nhuman-in-the-loop approach is called “open algorithms;” you get to see what the AIs take\nas input and what they decide using that input. If you see those two things, you’ll know\nwhether they’re doing the right thing or the wrong thing. It turns out that’s not hard to\ndo. If you control the data, then you control the AI.\nOne thing people often fail to mention is that all the worries about AI are the same\nas the worries about today’s government. For most parts of the government—the justice\nsystem, et cetera—there’s no reliable data about what they’re doing and in what situation.\nHow can you know whether the courts are fair or not if you don’t know the inputs and the\noutputs? The same problem arises with AI systems and is addressable in the same way.\nWe need trusted data to hold current government to account in terms of what they take in\nand what they put out, and AI should be no different.\nNext-Generation AI\nCurrent AI machine-learning algorithms are, at their core, dead simple stupid. They\nwork, but they work by brute force, so they need hundreds of millions of samples. They\nwork because you can approximate anything with lots of little simple pieces. That’s a\nkey insight of current AI research—that if you use reinforcement learning for creditassignment\nfeedback, you can get those little pieces to approximate whatever arbitrary\nfunction you want.\nBut using the wrong functions to make decisions means the AI’s ability to make\ngood decisions won’t generalize. If we give the AI new, different inputs, it may make\ncompletely unreasonable decisions. Or if the situation changes, then you need to retrain\nit. There are amusing techniques to find the “null space” in these AI systems. These are\ninputs that the AI thinks are valid examples of what it was trained to recognize (e.g.,\nfaces, cats, etc.), but to a human they’re crazy examples.\nCurrent AI is doing descriptive statistics in a way that’s not science and would be\nalmost impossible to make into science. To build robust systems, we need to know the\nscience behind data. The systems I view as next-generation AIs result from this sciencebased\napproach: If you’re going to create an AI to deal with something physical, then you\nshould build the laws of physics into it as your descriptive functions, in place of those\nstupid little neurons. For instance, we know that physics uses functions like polynomials,\nsine waves, and exponentials, so those should be your basis functions and not little linear\nneurons. By using those more appropriate basis functions, you need a lot less data, you\ncan deal with a lot more noise, and you get much better results.\nAs in the physics example, if we want to build an AI to work with human\nbehavior, then we need to build the statistical properties of human networks into\nmachine-learning algorithms. When you replace the stupid neurons with ones that\ncapture the basics of human behavior, then you can identify trends with very little data,\nand you can deal with huge levels of noise.\nThe fact that humans have a “commonsense” understanding that they bring to\nmost problems suggests what I call the human strategy: Human society is a network just\nlike the neural nets trained for deep learning, but the “neurons” in human society are a lot\n141\nsmarter. You and I have surprisingly general descriptive powers that we use for\nunderstanding a wide range of situations, and we can recognize which connections should\nbe reinforced. That means we can shape our social networks to work much better and\npotentially beat all that machine-based AI at its own game.\n142\n“URGENT!” URGENT!” the cc’d copy of an email screamed, one of a dozen emails that\ngreeted me as I turned on my phone at the baggage carousel at Malpensa Airport after\nthe long flight from JFK. “The great American visionary thinker John Brockman arrives\nthis morning at Grand Hotel Milan. You MUST, repeat MUST pay him a visit.” It was\nsigned HUO.\nThe prior evening, waiting in the lounge at JFK, I had had the bright idea to write\nmy friend and longtime collaborator, the London-based, peripatetic art curator Hans\nUlrich Obrist (known to all as HUO), and ask if there was anyone in Milan I should\nknow.\nOnce I was settled at the hotel, the phone began ringing and a procession of\nleading Italian artists, designers, and architects called to request a meeting, including\nEnzo Mari, the modernist artist and furniture designer; Alberto Garutti, whose aesthetic\nstrategies have inspired a dialogue between contemporary art, spectator, and public\nspace; and fashion designer Miuccia Prada, who “requests your presence for tea this\nafternoon at Prada headquarters.” And thus, thanks to HUO, did the jet-lagged “great\nAmerican visionary thinker” stumble and mumble his way through his first day in Milan,\nNovember 2011.\nHUO is sui generis: He lives a twenty-four-hour day, sleeping (I guess) whenever,\nand employing full-time assistants who work eight-hour shifts and are available to him\n24/7. Over a recent two-year period, he visited art venues in either China or India for\nforty weekends each year—departing London Thursday evening, back at his desk on\nMonday. Last year, once again, ArtReview ranked him #1 on their annual “Power 100”\nlist.\nRecently we collaborated on a panel during the “GUEST, GHOST, HOST:\nMACHINE!” Serpentine event that took place at London’s new City Hall. We were\njoined by Venki Ramakrishnan, Jaan Tallinn, and Andrew Blake, research director of\nThe Alan Turing Institute. The event was consistent with HUO’s mission of bringing\ntogether art and science: “The curator is no longer understood simply as the person who\nfills a space with objects,” he says, “but also as the person who brings different cultural\nspheres into contact, invents new display features, and makes junctions that allow\nunexpected encounters and results.”\n143\nMAKING THE INVISIBLE VISIBLE: ART MEETS AI\nHans Ulrich Obrist\nHans Ulrich Obrist is artistic director of the Serpentine Gallery, London, and the author\nof Ways of Curating and Lives of the Artists, Lives of the Architects.\nIn the Introduction to the second edition of his book Understanding Media, Marshall\nMcLuhan noted the ability of art to “anticipate future social and technological\ndevelopments.” Art is “an early alarm system,” pointing us to new developments in\ntimes ahead and allowing us “to prepare to cope with them. . . . Art as a radar\nenvironment takes on the function of indispensable perceptual training. . . .”\nIn 1964, when McLuhan’s book was first published, the artist Nam June Paik was\njust building his Robot K-456 to experiment with the technologies that subsequently\nwould start to influence society. He had worked with television earlier, challenging its\nusual passive consumption by the viewer, and later made art with global live-satellite\nbroadcasts, using the new media less for entertainment than to point us to their poetic and\nintercultural capacities (which are still mostly unused today). The Paiks of our time, of\ncourse, are now working with the Internet, digital images, and artificial intelligence.\nTheir works and thoughts, again, are an early alarm system for the developments ahead of\nus.\nAs a curator, my daily work is to bring together different works of art and connect\ndifferent cultures. Since the early 1990s, I have also been organizing conversations and\nmeetings with practitioners from different disciplines, in order to go beyond the general\nreluctance to pool knowledge. Since I was interested in hearing what artists have to say\nabout artificial intelligence, I recently organized several conversations between artists\nand engineers.\nThe reason to look closely at AI is that two of the most important questions of\ntoday are “How capable will AI become?” and “What dangers may arise from it?” Its\nearly applications already influence our everyday lives in ways that are more or less\nrecognizable. There is an increasing impact on many aspects of our society, but whether\nthis might be, in general, beneficial or malign is still uncertain.\nMany contemporary artists are following these developments closely. They are\narticulating various doubts about the promises of AI and reminding us not to associate the\nterm “artificial intelligence” solely with positive outcomes. To the current discussions of\nAI, the artists contribute their specific perspectives and notably their focus on questions\nof image making, creativity, and the use of programming as artistic tools.\nThe deep connections between science and art had already been noted by the late\nHeinz von Foerster, one of the architects of cybernetics, who worked with Norbert\nWiener from the mid-1940s and in the 1960s founded the field of second-order\ncybernetics, in which the observer is understood as part of the system itself and not an\nexternal entity. I knew von Foerster well, and in one of our many conversations, he\noffered his views on the relation between art and science:\nI’ve always perceived art and science as complementary fields. One shouldn’t\nforget that a scientist is in some respects also an artist. He invents a new\ntechnique and he describes it. He uses language like a poet, or the author of a\ndetective novel, and describes his findings. In my view, a scientist must work in\n144\nan artistic way if he wants to communicate his research. He obviously wants to\ncommunicate and talk to others. A scientist invents new objects, and the\nquestion is how to describe them. In all of these aspects, science is not very\ndifferent from art.\nWhen I asked him how he defined cybernetics, von Foerster answered:\nThe substance of what we have learned from cybernetics is to think in circles: A\nleads B, B to C, but C can return to A. Such kinds of arguments are not linear but\ncircular. The significant contribution of cybernetics to our thinking is to accept\ncircular arguments. This means that we have to look at circular processes and\nunderstand under which circumstances an equilibrium, and thus a stable\nstructure, emerges.\nToday, where AI algorithms are applied in daily tasks, one can ask how the\nhuman factor is included in these kinds of processes and what role creativity and art\ncould play in relation to them. There are thus different levels to think about when\nexploring the relation between AI and art.\nSo, what do contemporary artists have to say about artificial intelligence?\nArtificial Stupidity\nHito Steyerl, an artist who works with documentary and experimental film, considers two\nkey aspects that we should keep in mind when reflecting on the implications of AI for\nsociety. First, the expectations for so-called artificial intelligence, she says, are often\noverrated, and the noun “intelligence” is misleading; to counter that, she uses the term\n“artificial stupidity.” Second, she points out that programmers are now making invisible\nsoftware algorithms visible through images, but to understand and interpret these images\nbetter, we should apply the expertise of artists.\nSteyerl has worked with computer technology for many years, and her recent\nartworks have explored surveillance techniques, robots, and such computer games as in\nHow Not to Be Seen (2013), on digital-image technologies, or HellYeahWeFuckDie\n(2017), about the training of robots in the still-difficult task of keeping balance. But to\nexplain her notion of artificial stupidity, Steyerl refers to a more general phenomenon,\nlike the now widespread use of Twitter bots, noting in our conversation:\nIt was and still is a very popular tool in elections to deploy Twitter armies to\nsway public opinion and deflect popular hashtags and so on. This is an artificial\nintelligence of a very, very low grade. It’s two or maybe three lines of script.\nIt’s nothing very sophisticated at all. Yet the social implications of this kind of\nartificial stupidity, as I call it, are already monumental in global politics.\nAs has been widely noted, this kind of technology was seen in the many\nautomated Twitter posts before the 2016 U.S. presidential election and also shortly before\nthe Brexit vote. If even low-grade AI technology like these bots are already influencing\nour politics, this raises another urgent question: “How powerful will far more advanced\ntechniques be in the future?”\n145\nVisible / Invisible\nThe artist Paul Klee often talked about art as “making the invisible visible.” In computer\ntechnology, most algorithms work invisibly, in the background; they remain inaccessible\nin the systems we use daily. But lately there has been an interesting comeback of\nvisuality in machine learning. The ways that the deep-learning algorithms of AI are\nprocessing data have been made visible through applications like Google’s DeepDream,\nin which the process of computerized pattern-recognition is visualized in real time. The\napplication shows how the algorithm tries to match animal forms with any given input.\nThere are many other AI visualization programs that, in their way, also “make the\ninvisible visible.” The difficulty in the general public perception of such images is, in\nSteyerl’s view, that these visual patterns are viewed uncritically as realistic and objective\nrepresentations of the machine process. She says of the aesthetics of such visualizations:\nFor me, this proves that science has become a subgenre of art history. . . . We\nnow have lots of abstract computer patterns that might look like a Paul Klee\npainting, or a Mark Rothko, or all sorts of other abstractions that we know from\nart history. The only difference, I think, is that in current scientific thought\nthey’re perceived as representations of reality, almost like documentary images,\nwhereas in art history there’s a very nuanced understanding of different kinds of\nabstraction.\nWhat she seeks is a more profound understanding of computer-generated images\nand the different aesthetic forms they use. They are obviously not generated with the\nexplicit goal of following a certain aesthetic tradition. The computer engineer Mike\nTyka, in a conversation with Steyerl, explained the functions of these images:\nDeep-learning systems, especially the visual ones, are really inspired by the need\nto know what’s going on in the black box. Their goal is to project these\nprocesses back into the real world.\nNevertheless, these images have aesthetic implications and values which have to\nbe taken into account. One could say that while the programmers use these images to\nhelp us better understand the programs’ algorithms, we need the knowledge of artists to\nbetter understand the aesthetic forms of AI. As Steyerl has pointed out, such\nvisualizations are generally understood as “true” representations of processes, but we\nshould pay attention to their respective aesthetics, and their implications, which have to\nbe viewed in a critical and analytical way.\nIn 2017, the artist Trevor Paglen created a project to make these invisible AI\nalgorithms visible. In Sight Machine, he filmed a live performance of the Kronos Quartet\nand processed the resulting images with various computer software programs used for\nface detection, object identification, and even for missile guidance. He projected the\noutcome of these algorithms, in real time, back to screens above the stage. By\ndemonstrating how the various different programs interpreted the musicians’\nperformance, Paglen showed that AI algorithms are always determined by sets of values\nand interests which they then manifest and reiterate, and thus must be critically\nquestioned. The significant contrast between algorithms and music also raises the issue\nof relationships between technical and human perception.\n146\nComputers, as a Tool for Creativity, Can’t Replace the Artist.\nRachel Rose, a video artist who thinks about the questions posed by AI, employs\ncomputer technology in the creation of her works. Her films give the viewer an\nexperience of materiality through the moving image. She uses collaging and layering of\nthe material to manipulate sound and image, and the editing process is perhaps the most\nimportant aspect of her work.\nShe also talks about the importance of decision making in her work. For her, the\nartistic process does not follow a rational pattern. In a converation we had, together with\nthe engineer Kenric McDowell, at the Google Cultural Institute, she explained this by\nciting a story from theater director Peter Brook’s 1968 book The Empty Space. When\nBrook designed the set for his production of The Tempest in the late 1960s, he started by\nmaking a Japanese garden, but then the design evolved, becoming a white box, a black\nbox, a realistic set, and so on. And in the end, he returned to his original idea. Brook\nwrites that he was shocked at having spent a month on his labors, only to end at the\nbeginning. But this shows that the creative artistic process is a succession whose every\nstep builds on the next and which eventually comes to an unpredictable conclusion. The\nprocess is not a logical or rational succession but has mostly to do with the artist’s\nfeelings in reaction to the preceding result. Rose said, of her own artistic decision\nmaking:\nIt, to me, is distinctively different from machine learning, because at each\ndecision there’s this core feeling that comes from a human being, which has to do\nwith empathy, which has to do with communication, which has to do with\nquestions about our own mortality that only a human could ask.\nThis point underlines the fundamental difference between any human artistic production\nand so-called computer creativity. Rose sees AI more as a possible way to create better\ntools for humans:\nA place I can imagine machine learning working for an artist would be not in\ndeveloping an independent subjectivity, like writing a poem or making an image,\nbut actually in filling in gaps that are to do with labor, like the way that\nPhotoshop works with different tools that you can use.\nAnd though such tools may not seem spectacular, she says, “they might have a larger\ninfluence on art,” because they provide artists with further possibilities in their creative\nwork.\nMcDowell added that he, too, believes there are false expectations around AI.\n“I’ve observed,” he said, “that there’s a sort of magical quality to the idea of a computer\nthat does all the things that we do.” He continued: “There’s almost this kind of demonic\nmirror that we look into, and we want it to write a novel, we want it to make a film—we\nwant to give that away somehow.” He is instead working on projects wherein humans\ncollaborate with the machine. One of the current aims of AI research is to find new\nmeans of interaction between humans and software. And art, one could say, needs to\nplay a key role in that enterprise, since it focuses on our subjectivity and on essential\nhuman aspects like empathy and mortality.\n147\nCybernetics / Art\nSuzanne Treister is an artist whose work from 2009 to 2011 serves as an example of what\nis happening at the intersection of our current technologies, the arts, and cybernetics.\nTreister has been a pioneer in digital art since the 1990s, inventing, for example,\nimaginary video games and painting screen shots from them. In her project Hexen 2.0\nshe looked back at the famous Macy conferences on cybernetics that between 1946 and\n1953 were organized in New York by engineers and social scientists to unite the sciences\nand to develop a universal theory of the workings of the mind.\nIn her project, she created thirty photo-text works about the conference attendees\n(which included Wiener and von Foerster), she invented tarot cards, and she made a\nvideo based on a photomontage of a “cybernetic séance.” In the “séance,” the conference\nparticipants are seen sitting at a round table, as in spiritualist séances, while certain of\ntheir statements on cybernetics are heard in an audio-collage—rational knowledge and\nsuperstition combined. She also noted that some of the participating scientists worked for\nthe military; thus the application of cybernetics could be seen in an ambivalent way, even\nback then, as a tussle between pure knowledge and its use in state control.\nIf one looks at Treister’s work about the Macy conference participants, one sees\nthat no visual artist was included. A dialogue between artists and scientists would be\nfruitful in future discussions, and it is a bit astonishing that this wasn’t realized at the\ntime, given von Foerster’s keen interest in art. He recounted in one of our conversations\nhow his relation to the field dated back to his childhood:\nI grew up as a child in an artistic family. We often had visits from poets,\nphilosophers, painters, and sculptors. Art was a part of my life. Later, I got into\nphysics, as I was talented in this subject. But I always remained conscious of the\nimportance of art for science. There wasn’t a great difference for me. For me,\nboth aspects of life have always been very much alike—and accessible, too. We\nshould see them as one. An artist also has to reflect on his work. He has to think\nabout his grammar and his language. A painter must know how to handle his\ncolors. Just think of how intensively oil colors were researched during the\nRenaissance. They wanted to know how a certain pigment could be mixed with\nothers to get a certain tone of red or blue. Chemists and painters collaborated\nvery closely. I think the artificial division between science and art is wrong.\nThough for von Foerster the relation between the art and science was always\nclear, for our own time this connection remains to be made. There are many reasons to\nmultiply the links. The critical thinking of artists would be beneficial in respect to the\ndangers of AI, since they draw our attention to questions they consider essential from\ntheir perspective. With the advent of machine learning, new tools are available to artists\nfor their work. And as the algorithms of AI are made visible through artificial images in\nnew ways, artists’ critical visual knowledge and expertise will be harnessed. Many of the\nkey questions of AI are philosophical in nature and can be answered only from a holistic\npoint of view. The way they play out among adventurous artists will be worth following.\nSimulating Worlds\nFor the most part, the works of contemporary artists have been embodied ruminations on\n148\nAI’s impact on existential questions of the self and our future interaction with nonhuman\nentities. Few, though, have taken the technologies and innovations of AI as the\nunderlying materials of their work and sculpted them to their own vision. An exception\nis the artist Ian Cheng, who has gone as far as to construct entire worlds of artificial\nbeings with varying degrees of sentience and intelligence. He refers to these worlds as\nLive Simulations. His Emissaries trilogy (2015-2017) is set in a fictional postapocalyptic\nworld of flora and fauna, in which AI-driven animals and creatures explore the landscape\nand interact with each other. Cheng uses advanced graphics but has them programmed\nwith a lot of glitches and imperfections, which imparts a futuristic and anachronistic\natmosphere at the same time. Through his trilogy, which charts a history of\nconsciousness, he asks the question “What is a simulation?”\nWhile the majority of artistic works that utilize recent developments in AI\nspecifically draw from the field of machine learning, Cheng’s Live Simulations take a\nseparate route. The protagonists and plot lines that are interlaced in each episodic\nsimulation of Emissaries use the complex logic systems and rules of AI. What is\nprofound about his continually evolving scenes is that complexity arises not through the\ndesire/actions of any single actor or artificial godhead but instead through their\nconstellation, collision, and constant evolution in symbiosis with one another. This gives\nrise to unexpected outcomes and unending, unknowable situations—you can never\nexperience the exact same moment in successive viewings of his work.\nCheng had a discussion at the Serpentine Marathon “GUEST, GHOST, HOST:\nMACHINE!” with the programmer Richard Evans, who recently designed Versu, an AIbased\nplatform for interactive storytelling games. Evans’ work emphasizes the social\ninteraction of the games’ characters, who react in a spectrum of possible behaviors to the\nchoices made by the human players. In their conversation, Evans said that a starting\npoint for the project was that most earlier simulation video games, such as The Sims, did\nnot sufficiently take into account the importance of social practices. Simulated\nprotagonists in games would often act in ways that did not correspond well with real\nhuman behavior. Knowledge of social practices limits the possibilities of action but is\nnecessary to understand the meaning of our actions—which is what interests Cheng for\nhis own simulations. The more parameters of actions in certain circumstances are\ndetermined in a computer simulation, the more interesting it is for Cheng to experiment\nwith individual and specific changes. He told Evans, “I gather that if we had AI with\nmore ability to respond to social contexts, tweaking one thing, you would get something\nquite artistic and beautiful.”\nCheng also sees the work of programmers and AI simulations as creating new and\nsophisticated tools for experimenting with the parameters of our daily social practices. In\nthis way, the involvement of artists in AI will lead to new kinds of open experiments in\nArt. Such possibilities are—like increased AI capabilities in general—still in the future.\nRecognizing that this is an experimental technology in its infancy, very far from\napocalyptic visions of a superintelligent AI takeover, Cheng fills his simulations with\nprosaic avatars such as strange microbial globules, dogs, and the undead.\nDiscussions like these, between artists and engineers, of course are not totally\nnew. In the 1960s, the engineer Billy Klüver brought artists together with engineers in a\nseries of events, and in 1967 he founded the Experiments in Art and Technology program\nwith Robert Rauschenberg and others. In London, at around the same time, Barbara\n149\nStevini and John Latham, of the Artist Placement Group, took things a step further by\nasserting that there should be artists in residence in every company and every\ngovernment. Today, these inspiring historical models can be applied to the field of AI.\nAs AI comes to inhabit more and more of our everyday lives, the creation of a space that\nis nondeterministic and non-utilitarian in its plurality of perspectives and diversity of\nunderstandings will undoubtedly be essential.\n150\nAlison Gopnik is an international leader in the field of children’s learning and\ndevelopment and was one of the founders of the field of “theory of mind.” She has\nspoken of the child brain as a “powerful learning computer,” perhaps from personal\nexperience. Her own Philadelphia childhood was an exercise in intellectual\ndevelopment. “Other families took their kids to see The Sound of Music or Carousel; we\nsaw Racine’s Phaedra and Samuel Beckett’s Endgame,” she has recalled. “Our family\nread Henry Fielding’s 18th-century novel Joseph Andrews out loud to each other around\nthe fire on camping trips.”\nLately she has invoked Bayesian models of machine learning to explain the\nremarkable ability of preschoolers to draw conclusions about the world around them\nwithout benefit of enormous data sets. “I think babies and children are actually more\nconscious than we are as adults,” she has said. “They’re very good at taking in lots of\ninformation from lots of different sources at once.” She has referred to babies and young\nchildren as “the research and development division of the human species.” Not that she\ntreats them coldly, as if they were mere laboratory animals. They appear to revel in her\ncompany, and in the blinking, thrumming toys in her Berkeley lab. For years after her\nown children had outgrown it, she kept a playpen in her office.\nHer investigations into just how we learn, and the parallels to the deep-learning\nmethods of AI, continues. “It turns out to be much easier to simulate the reasoning of a\nhighly trained adult expert than to mimic the ordinary learning of every baby,” she says.\n“Computation is still the best—indeed, the only—scientific explanation we have of how a\nphysical object like a brain can act intelligently. But, at least for now, we have almost no\nidea at all how the sort of creativity we see in children is possible.”\n151\nAIs VERSUS FOUR-YEAR-OLDS\nAlison Gopnik\nAlison Gopnik is a developmental psychologist at UC Berkeley; her books include The\nPhilosophical Baby and, most recently, The Gardener and the Carpenter: What the New\nScience of Child Development Tells Us About the Relationship Between Parents and\nChildren.\nEveryone’s heard about the new advances in artificial intelligence, and especially\nmachine learning. You’ve also heard utopian or apocalyptic predictions about what those\nadvances mean. They have been taken to presage either immortality or the end of the\nworld, and a lot has been written about both those possibilities. But the most\nsophisticated AIs are still far from being able to solve problems that human four-yearolds\naccomplish with ease. In spite of the impressive name, artificial intelligence largely\nconsists of techniques to detect statistical patterns in large data sets. There is much more\nto human learning.\nHow can we possibly know so much about the world around us? We learn an\nenormous amount even when we are small children; four-year-olds already know about\nplants and animals and machines; desires, beliefs, and emotions; even dinosaurs and\nspaceships.\nScience has extended our knowledge about the world to the unimaginably large\nand the infinitesimally small, to the edge of the universe and the beginning of time. And\nwe use that knowledge to make new classifications and predictions, imagine new\npossibilities, and make new things happen in the world. But all that reaches any of us\nfrom the world is a stream of photons hitting our retinas and disturbances of air at our\neardrums. How do we learn so much about the world when the evidence we have is so\nlimited? And how do we do all this with the few pounds of grey goo that sits behind our\neyes?\nThe best answer so far is that our brains perform computations on the concrete,\nparticular, messy data arriving at our senses, and those computations yield accurate\nrepresentations of the world. The representations seem to be structured, abstract, and\nhierarchical; they include the perception of three-dimensional objects, the grammars that\nunderlie language, and mental capacities like “theory of mind,” which lets us understand\nwhat other people think. Those representations allow us to make a wide range of new\npredictions and imagine many new possibilities in a distinctively creative human way.\nThis kind of learning isn’t the only kind of intelligence, but it’s a particularly\nimportant one for human beings. And it’s the kind of intelligence that is a specialty of\nyoung children. Although children are dramatically bad at planning and decision making,\nthey are the best learners in the universe. Much of the process of turning data into\ntheories happens before we are five.\nSince Aristotle and Plato, there have been two basic ways of addressing the\nproblem of how we know what we know, and they are still the main approaches in\nmachine learning. Aristotle approached the problem from the bottom up: Start with\nsenses—the stream of photons and air vibrations (or the pixels or sound samples of a\ndigital image or recording)—and see if you can extract patterns from them. This\napproach was carried further by such classic associationists as philosophers David Hume\n152\nand J. S. Mill and later by behavioral psychologists, like Pavlov and B. F. Skinner. On\nthis view, the abstractness and hierarchical structure of representations is something of an\nillusion, or at least an epiphenomenon. All the work can be done by association and\npattern detection—especially if there are enough data.\nOver time, there has been a seesaw between this bottom-up approach to the\nmystery of learning and Plato’s alternative, top-down one. Maybe we get abstract\nknowledge from concrete data because we already know a lot, and especially because we\nalready have an array of basic abstract concepts, thanks to evolution. Like scientists, we\ncan use those concepts to formulate hypotheses about the world. Then, instead of trying\nto extract patterns from the raw data, we can make predictions about what the data should\nlook like if those hypotheses are right. Along with Plato, such “rationalist” philosophers\nand psychologists as Descartes and Noam Chomsky took this approach.\nHere’s an everyday example that illustrates the difference between the two\nmethods: solving the spam plague. The data consist of a long unsorted list of messages in\nyour in-box. The reality is that some of these messages are genuine and some are spam.\nHow can you use the data to discriminate between them?\nConsider the bottom-up technique first. You notice that the spam messages tend\nto have particular features: a long list of addressees, origins in Nigeria, references to\nmillion-dollar prizes or Viagra. The trouble is that perfectly useful messages might have\nthese features, too. If you looked at enough examples of spam and non-spam emails, you\nmight see not only that spam emails tend to have those features but that the features tend\nto go together in particular ways (Nigeria plus a million dollars spells trouble). In fact,\nthere might be some subtle higher-level correlations that discriminate the spam messages\nfrom the useful ones—a particular pattern of misspellings and IP addresses, say. If you\ndetect those patterns, you can filter out the spam.\nThe bottom-up machine-learning techniques do just this. The learner gets\nmillions of examples, each with some set of features and each labeled as spam (or some\nother category) or not. The computer can extract the pattern of features that distinguishes\nthe two, even if it’s quite subtle.\nHow about the top-down approach? I get an email from the editor of the Journal\nof Clinical Biology. It refers to one of my papers and says that they would like to publish\nan article by me. No Nigeria, no Viagra, no million dollars; the email doesn’t have any\nof the features of spam. But by using what I already know, and thinking in an abstract\nway about the process that produces spam, I can figure out that this email is suspicious.\n(1) I know that spammers try to extract money from people by appealing to\nhuman greed.\n(2) I also know that legitimate “open access” journals have started covering their\ncosts by charging authors instead of subscribers, and that I don’t practice anything like\nclinical biology.\nPut all that together and I can produce a good new hypothesis about where that\nemail came from. It’s designed to sucker academics into paying to “publish” an article in\na fake journal. The email was a result of the same dubious process as the other spam\nemails, even though it looked nothing like them. I can draw this conclusion from just one\nexample, and I can go on to test my hypothesis further, beyond anything in the email\nitself, by googling the “editor.”\n153\nIn computer terms, I started out with a “generative model” that includes abstract\nconcepts like greed and deception and describes the process that produces email scams.\nThat lets me recognize the classic Nigerian email spam, but it also lets me imagine many\ndifferent kinds of possible spam. When I get the journal email, I can work backward:\n“This seems like just the kind of mail that would come out of a spam-generating\nprocess.”\nThe new excitement about AI comes because AI researchers have recently\nproduced powerful and effective versions of both these learning methods. But there is\nnothing profoundly new about the methods themselves.\nBottom-up Deep Learning\nIn the 1980s, computer scientists devised an ingenious way to get computers to detect\npatterns in data: connectionist, or neural-network, architecture (the “neural” part was, and\nstill is, metaphorical). The approach fell into the doldrums in the ’90s but has recently\nbeen revived with powerful “deep-learning” methods like Google’s DeepMind.\nFor example, you can give a deep-learning program a bunch of Internet images\nlabeled “cat,” others labeled “house,” and so on. The program can detect the patterns\ndifferentiating the two sets of images and use that information to label new images\ncorrectly. Some kinds of machine learning, called unsupervised learning, can detect\npatterns in data with no labels at all; they simply look for clusters of features—what\nscientists call a factor analysis. In the deep-learning machines, these processes are\nrepeated at different levels. Some programs can even discover relevant features from the\nraw data of pixels or sounds; the computer might begin by detecting the patterns in the\nraw image that correspond to edges and lines and then find the patterns in those patterns\nthat correspond to faces, and so on.\nAnother bottom-up technique with a long history is reinforcement learning. In the\n1950s, B. F. Skinner, building on the work of John Watson, famously programmed\npigeons to perform elaborate actions—even guiding air-launched missiles to their targets\n(a disturbing echo of recent AI) by giving them a particular schedule of rewards and\npunishments. The essential idea was that actions that were rewarded would be repeated\nand those that were punished would not, until the desired behavior was achieved. Even\nin Skinner’s day, this simple process, repeated over and over, could lead to complex\nbehavior. Computers are designed to perform simple operations over and over on a scale\nthat dwarfs human imagination, and computational systems can learn remarkably\ncomplex skills in this way.\nFor example, researchers at Google’s DeepMind used a combination of deep\nlearning and reinforcement learning to teach a computer to play Atari video games. The\ncomputer knew nothing about how the games worked. It began by acting randomly and\ngot information only about what the screen looked like at each moment and how well it\nhad scored. Deep learning helped interpret the features on the screen, and reinforcement\nlearning rewarded the system for higher scores. The computer got very good at playing\nseveral of the games, but it also completely bombed on others just as easy for humans to\nmaster.\nA similar combination of deep learning and reinforcement learning has enabled\nthe success of DeepMind’s AlphaZero, a program that managed to beat human players at\nboth chess and Go, equipped only with a basic knowledge of the rules of the game and\n154\nsome planning capacities. AlphaZero has another interesting feature: It works by playing\nhundreds of millions of games against itself. As it does so, it prunes mistakes that led to\nlosses, and it repeats and elaborates on strategies that led to wins. Such systems, and\nothers involving techniques called generative adversarial networks, generate data as well\nas observing data.\nWhen you have the computational power to apply those techniques to very large\ndata sets or millions of email messages, Instagram images, or voice recordings, you can\nsolve problems that seemed very difficult before. That’s the source of much of the\nexcitement in computer science. But it’s worth remembering that those problems—like\nrecognizing that an image is a cat or a spoken word is “Siri”—are trivial for a human\ntoddler. One of the most interesting discoveries of computer science is that problems that\nare easy for us (like identifying cats) are hard for computers—much harder than playing\nchess or Go. Computers need millions of examples to categorize objects that we can\ncategorize with just a few. These bottom-up systems can generalize to new examples;\nthey can label a new image as a “cat” fairly accurately, over all. But they do so in ways\nquite different from how humans generalize. Some images almost identical to a cat\nimage won’t be identified by us as cats at all. Others that look like a random blur will be.\nTop-down Bayesian Models\nThe top-down approach played a big role in early AI, and in the 2000s it, too,\nexperienced a revival, in the form of probabilistic, or Bayesian, generative models.\nThe early attempts to use this approach faced two kinds of problems. First, most\npatterns of evidence might in principle be explained by many different hypotheses: It’s\npossible that my journal email message is genuine, it just doesn’t seem likely. Second,\nwhere do the concepts that the generative models use come from in the first place? Plato\nand Chomsky said you were born with them. But how can we explain how we learn the\nlatest concepts of science? Or how even young children understand about dinosaurs and\nrocket ships?\nBayesian models combine generative models and hypothesis testing with\nprobability theory, and they address these two problems. A Bayesian model lets you\ncalculate just how likely it is that a particular hypothesis is true, given the data. And by\nmaking small but systematic tweaks to the models we already have, and testing them\nagainst the data, we can sometimes make new concepts and models from old ones. But\nthese advantages are offset by other problems. The Bayesian techniques can help you\nchoose which of two hypotheses is more likely, but there are almost always an enormous\nnumber of possible hypotheses, and no system can efficiently consider them all. How do\nyou decide which hypotheses are worth testing in the first place?\nBrenden Lake at NYU and colleagues have used these kinds of top-down methods\nto solve another problem that’s easy for people but extremely difficult for computers:\nrecognizing unfamiliar handwritten characters. Look at a character on a Japanese scroll.\nEven if you’ve never seen it before, you can probably tell if it’s similar to or different\nfrom a character on another Japanese scroll. You can probably draw it and even design a\nfake Japanese character based on the one you see—one that will look quite different from\na Korean or Russian character. 37\n37\nBrenden M. Lake, Ruslan Salakhutdinov & Joshua B. Tenenbaum, “Human-level concept learning\nthrough probabilistic program induction,” Science, 350:6266, pp. 1332-38 (2015).\n155\nThe bottom-up method for recognizing handwritten characters is to give the\ncomputer thousands of examples of each one and let it pull out the salient features.\nInstead, Lake et al. gave the program a general model of how you draw a character: A\nstroke goes either right or left; after you finish one, you start another; and so on. When\nthe program saw a particular character, it could infer the sequence of strokes that were\nmost likely to have led to it—just as I inferred that the spam process led to my dubious\nemail. Then it could judge whether a new character was likely to result from that\nsequence or from a different one, and it could produce a similar set of strokes itself. The\nprogram worked much better than a deep-learning program applied to exactly the same\ndata, and it closely mirrored the performance of human beings.\nThese two approaches to machine learning have complementary strengths and\nweaknesses. In the bottom-up approach, the program doesn’t need much knowledge to\nbegin with, but it needs a great deal of data, and it can generalize only in a limited way.\nIn the top-down approach, the program can learn from just a few examples and make\nmuch broader and more varied generalizations, but you need to build much more into it to\nbegin with. A number of investigators are currently trying to combine the two\napproaches, using deep learning to implement Bayesian inference.\nThe recent success of AI is partly the result of extensions of those old ideas. But\nit has more to do with the fact that, thanks to the Internet, we have much more data, and\nthanks to Moore’s Law we have much more computational power to apply to that data.\nMoreover, an unappreciated fact is that the data we do have has already been sorted and\nprocessed by human beings. The cat pictures posted to the Web are canonical cat\npictures—pictures that humans have already chosen as “good” pictures. Google\nTranslate works because it takes advantage of millions of human translations and\ngeneralizes them to a new piece of text, rather than genuinely understanding the\nsentences themselves.\nBut the truly remarkable thing about human children is that they somehow\ncombine the best features of each approach and then go way beyond them. Over the past\nfifteen years, developmentalists have been exploring the way children learn structure\nfrom data. Four-year-olds can learn by taking just one or two examples of data, as a topdown\nsystem does, and generalizing to very different concepts. But they can also learn\nnew concepts and models from the data itself, as a bottom-up system does.\nFor example, in our lab we give young children a “blicket detector”—a new\nmachine to figure out, one they’ve never seen before. It’s a box that lights up and plays\nmusic when you put certain objects on it but not others. We give children just one or two\nexamples of how the machine works, showing them that, say, two red blocks make it go,\nwhile a green-and-yellow combination doesn’t. Even eighteen-month-olds immediately\nfigure out the general principle that the two objects have to be the same to make it go,\nand they generalize that principle to new examples: For instance, they will choose two\nobjects that have the same shape to make the machine work. In other experiments, we’ve\nshown that children can even figure out that some hidden invisible property makes the\nmachine go, or that the machine works on some abstract logical principle. 38\n38\nA. Gopnik, T. Griffiths & C. Lucas, “When younger learners can be better (or at least more openminded)\nthan older ones,” Curr. Dir. Psychol. Sci., 24:2, 87-92 (2015).\n156\nYou can show this in children’s everyday learning, too. Young children rapidly\nlearn abstract intuitive theories of biology, physics, and psychology in much the way\nadult scientists do, even with relatively little data.\nThe remarkable machine-learning accomplishments of the recent AI systems, both\nbottom-up and top-down, take place in a narrow and well-defined space of hypotheses\nand concepts—a precise set of game pieces and moves, a predetermined set of images. In\ncontrast, children and scientists alike sometimes change their concepts in radical ways,\nperforming paradigm shifts rather than simply tweaking the concepts they already have.\nFour-year-olds can immediately recognize cats and understand words, but they\ncan also make creative and surprising new inferences that go far beyond their experience.\nMy own grandson recently explained, for example, that if an adult wants to become a\nchild again, he should try not eating any healthy vegetables, since healthy vegetables\nmake a child grow into an adult. This kind of hypothesis, a plausible one that no grownup\nwould ever entertain, is characteristic of young children. In fact, my colleagues and I\nhave shown systematically that preschoolers are better at coming up with unlikely\nhypotheses than older children and adults. 39 We have almost no idea how this kind of\ncreative learning and innovation is possible.\nLooking at what children do, though, may give programmers useful hints about\ndirections for computer learning. Two features of children’s learning are especially\nstriking. Children are active learners; they don’t just passively soak up data like AIs do.\nJust as scientists experiment, children are intrinsically motivated to extract information\nfrom the world around them through their endless play and exploration. Recent studies\nshow that this exploration is more systematic than it looks and is well-adapted to find\npersuasive evidence to support hypothesis formation and theory choice. 40 Building\ncuriosity into machines and allowing them to actively interact with the world might be a\nroute to more realistic and wide-ranging learning.\nSecond, children, unlike existing AIs, are social and cultural learners. Humans\ndon’t learn in isolation but avail themselves of the accumulated wisdom of past\ngenerations. Recent studies show that even preschoolers learn through imitation and by\nlistening to the testimony of others. But they don’t simply passively obey their teachers.\nInstead they take in information from others in a remarkably subtle and sensitive way,\nmaking complex inferences about where the information comes from and how\ntrustworthy it is and systematically integrating their own experiences with what they are\nhearing. 41\n“Artificial intelligence” and “machine learning” sound scary. And in some ways\nthey are. These systems are being used to control weapons, for example, and we really\nshould be scared about that. Still, natural stupidity can wreak far more havoc than\nartificial intelligence; we humans will need to be much smarter than we have been in the\npast to properly regulate the new technologies. But there is not much basis for either the\napocalyptic or the utopian visions of AIs replacing humans. Until we solve the basic\n39\nA. Gopnik, et al., “Changes in cognitive flexibility and hypothesis search across human life history from\nchildhood to adolescence to adulthood,” Proc. Nat. Acad. Sci., 114:30, 7892-99 (2017).\n40\nL. Schulz, “The origins of Inquiry: Inductive inference and exploration in early childhood,” Trends Cog.\nSci., 16:7, 382-89 (2012).\n41\nA. Gopnik, The Gardener and the Carpenter (New York: Farrar, Straus & Giroux, 2016), chaps. 4 and 5.\n157\nparadox of learning, the best artificial intelligences will be unable to compete with the\naverage human four-year-old.\n158\nPeter Galison’s focus as a science historian is—speaking roughly—on the intersection of\ntheory with experiment.\n“For quite a number of years I have been guided in my work by the odd\nconfrontation of abstract ideas and extremely concrete objects,” he once told me, in\nexplaining how he thinks about what he does. At the Washington, Connecticut, meeting\nhe discussed the Cold War tension between engineers (like Wiener) and the\nadministrators of the Manhattan Project (like Oppenheimer: “When [Wiener] warns\nabout the dangers of cybernetics, in part he’s trying to compete against the kind of\nportentous language that people like Oppenheimer [used]: ‘When I saw the explosion at\nTrinity, I thought of the Bhagavad Gita—I am death, destroyer of worlds.’ That sense,\nthat physics could stand and speak to the nature of the universe and airforce policy, was\nrepellent and seductive. In a way, you can see that over and over again in the last\ndecades—nanosciences, recombinant DNA, cybernetics: ‘I stand reporting to you on the\nscience that has the promise of salvation and the danger of annihilation—and you should\npay attention, because this could kill you.’ It’s a very seductive narrative, and it’s\nrepeated in artificial intelligence and robotics.”\nAs a twenty-four-year old, when I first encountered Wiener’s ideas and met his\ncolleagues at the MIT meeting I describe in the book’s Introduction, I was hardly\ninterested in Wiener’s warnings or admonitions. What drove my curiosity was the stark,\nradical nature of his view of life, based on the mathematical theory of communications in\nwhich the message was nonlinear: According to Wiener, “new concepts of\ncommunication and control involved a new interpretation of man, of man’s knowledge of\nthe universe, and of society.” And that led to my first book, which took information\ntheory—the mathematical theory of communications—as a model for all human\nexperience.\nIn a recent conversation, Peter told me he was beginning to write a book—about\nbuilding, crashing, and thinking—that considers the black-box nature of cybernetics and\nhow it represents what he thinks of as “the fundamental transformation of learning,\nmachine learning, cybernetics, and the self.”\n159\nALGORISTS DREAM OF OBJECTIVITY\nPeter Galison\nPeter Galison is a science historian, Joseph Pellegrino University Professor and cofounder\nof the Black Hole Initiative at Harvard University, and the author of Einstein's\nClocks and Poincaré’s Maps: Empires of Time.\nIn his second-best book, the great medieval mathematician al-Khwarizmi described the\nnew place-based Indian form of arithmetic. His name, soon sonically linked to\n“algorismus” (in late medieval Latin) came to designate procedures acting upon\nnumbers—eventually wending its way through “algorithm,” (on the model of\n“logarithm”), into French and on into English. But I like the idea of a modern algorist,\neven if my spellcheck does not. I mean by it someone profoundly suspicious of the\nintervention of human judgment, someone who takes that judgment to violate the\nfundamental norms of what it is to be objective (and therefore scientific).\nNear the end of the 20th century, a paper by two University of Minnesota\npsychologists summarized a vast literature that had long roiled the waters of prediction.\nOne side, they judged, had for all too long held resolutely—and ultimately unethically—\nto the “clinical method” of prediction, which prized all that was subjective: “informal,”\n“in-the-head,” and “impressionistic.” These clinicians were people (so said the\npsychologists) who thought they could study their subjects with meticulous care, gather\nin committees, and make judgment-based predictions about criminal recidivism, college\nsuccess, medical outcomes, and the like. The other side, the psychologists continued,\nembodied everything the clinicians did not, embracing the objective: “formal,”\n“mechanical,” “algorithmic.” This the authors took to stand at the root of the whole\ntriumph of post-Galilean science. Not only did science benefit from the actuarial; to a\ngreat extent, science was the mechanical-actuarial. Breezing through 136 studies of\npredictions, across domains from sentencing to psychiatry, the authors showed that in 128\nof them, predictions using actuarial tables, a multiple-regression equation, or an\nalgorithmic judgment equalled or exceeded in accuracy those using the subjective\napproach.\nThey went on to catalog seventeen fallacious justifications for clinging to the\nclinical. There were the self-interested foot-draggers who feared losing their jobs to\nmachines. Others lacked the education to follow statistical arguments. One group\nmistrusted the formalization of mathematics; another excoriated what they took to be the\nactuarial “dehumanizing;” yet others said that the aim was to understand, not to predict.\nBut whatever the motivations, the review concluded that it was downright immoral to\nwithhold the power of the objective over the subjective, the algorithmic over expert\njudgment. 42\n42\nWilliam M. Grove & Paul E. Meehl, “Comparative efficiency of informal (subjective, impressionistic)\nand formal (mechanical, algorithmic) prediction procedures: The Clinical-Statistical Controversy,”\nPsychology, Public Policy, and Law, 2:2, 293-323 (1996).\n160\nThe algorist view has gained strength. Anne Milgram served as Attorney General\nof the State of New Jersey from 2007 to 2010. When she took office, she wanted to\nknow who the state was arresting, charging, and jailing, and for what crimes. At the\ntime, she reports in a later TED Talk, she could find almost no data or analytics. By\nimposing statistical prediction, she continues, law enforcement in Camden during her\ntenure was able to reduce murders by 41 percent, saving thirty-seven lives, while\ndropping the total crime rate by 26 percent. After joining the Arnold Foundation as its\nvice president for criminal justice, she established a team of data scientists and\nstatisticians to create a risk-assessment tool; fundamentally, she construed the team’s\nmission as deciding how to put “dangerous people” in jail while releasing the nondangerous.\n“The reason for this,” Milgram contended, “is the way we make decisions.\nJudges have the best intentions when they make these decisions about risk, but they’re\nmaking them subjectively. They’re like the baseball scouts twenty years ago who were\nusing their instinct and their experience to try to decide what risk someone poses.\nThey’re being subjective, and we know what happens with subjective decision making,\nwhich is that we are often wrong.” Her team established nine-hundred-plus risk factors,\nof which nine were most predictive. The questions, the most urgent questions, for the\nteam were: Will a person commit a new crime? Will that person commit a violent act?\nWill someone come back to court? We need, concluded Milgram, an “objective measure\nof risk” that should be inflected by judges’ judgment. We know the algorithmic\nstatistical process works. That, she says, is “why Google is Google” and why moneyball\nwins games. 43\nAlgorists have triumphed. We have grown accustomed to the idea that protocols\nand data can and should guide us in everyday action, from reminders about where we\nprobably want to go next, to the likely occurrence of crime. By now, according to the\nliterature, the legal, ethical, formal, and economic dimensions of algorithms are all quasiinfinite.\nI’d like to focus on one particular siren song of the algorithm: its promise of\nobjectivity.\nScientific objectivity has a history. That might seem surprising. Isn’t the\nnotion—expressed above by the Minnesota psychologists—right? Isn’t objectivity coextensive\nwith science itself? Here it’s worth stepping back to reflect on all the epistemic\nvirtues we might value in scientific work. Quantification seems like a good thing to\nhave; so, too, do prediction, explanation, unification, precision, accuracy, certainty, and\npedagogical utility. In the best of all possible worlds these epistemic virtues would all\npull in the same direction. But they do not—not any more than our ethical virtues\nnecessarily coincide. Rewarding people according to their need may very well conflict\nwith rewarding people according to their ability. Equality, fairness, meritocracy—ethics,\nin a sense, is all about the adjudication of conflicting goods. Too often we forget that this\nconflict exists in science, too. Design an instrument to be as sensitive as possible and it\noften fluctuates wildly, making repetition of a measurement impossible.\n“Scientific objectivity” entered both the practice and the nomenclature of science\nafter the first third of the 19th century. One sees this clearly in the scientific atlases that\nprovided scientists with the basic objects of their specialty: There were (and are) atlases\nof the hand, atlases of the skull, atlases of clouds, crystals, flowers, bubble-chamber\npictures, nuclear emulsions, and diseases of the eye. In the 18th century, it was obvious\n43\nTED Talk, January 2014, https://www.ted.com/speakers/anne_milgram.\n161\nthat you would not depict this particular, sun-scorched, caterpillar-chewed clover found\noutside your house in an atlas. No, you aimed—if you were a genius natural philosopher\nlike Goethe, Albinus, or Cheselden—to observe nature but then to perfect the object in\nquestion, to abstract it visually to the ideal. Take a skeleton, view it through a camera\nlucida, draw it with care. Then correct the “imperfections.” The advantage of this\nparting of the curtains of mere experience was clear: It provided a universal guide, one\nnot attached to the vagaries of individual variation.\nAs the sciences grew in scope, and scientists grew in number, the downside of\nidealization became clearer. It was one thing to have Goethe depict the “ur-plant” or “urinsect.”\nIt was quite another to have a myriad of different scientists each fixing their\nimages in different and sometimes contradictory ways. Gradually, from around the 1830s\nforward, one begins to see something new: a claim that the image making was done with\na minimum of human intervention, that protocols were followed. This could mean\ntracing a leaf with a pencil or pressing it into ink that was transferred to the page. It\nmeant, too, that one suddenly was proud of depicting the view through a microscope of a\nnatural object even with its imperfections. This was a radical idea: snowflakes shown\nwithout perfect hexagonal symmetry, color distortion near the edge of a microscope lens,\ntissue torn around the edges in the process of its preparation.\nScientific objectivity came to mean that our representations of things were\nexecuted by holding back from intervention—even if it meant reproducing the yellow\ncolor near the edge of the image under the microscope, despite the fact that the scientist\nknew that the discoloration was from the lens, not a feature of the object of inquiry. The\nadvantage of objectivity was clear: It superseded the desire to see a theory realized or a\ngenerally accepted view confirmed. But objectivity came at a cost. You lost that precise,\neasily teachable, colored, full depth-of-field, artist’s rendition of a dissected corpse. You\ngot a blurry, bad depth-of-field, black-and-white photograph that no medical student (nor\neven many medical colleagues) could use to learn and compare cases. Still, for a long\nstretch of the 19th century, the virtue of hands-off, self-restraining objectivity was on the\nrise.\nStarting in the 1930s, the hardline scientific objectivity in scientific representation\nbegan running into trouble. In cataloging stellar spectra, for example, no algorithm could\ncompete with highly trained observers who could sort them with far greater accuracy and\nreplicability than any purely rule-following procedure. By the late 1940s, doctors had\nbegun learning how to read electroencephalograms. Expert judgment was needed to sort\nout different kinds of seizure readings, while none of the early attempts to use frequency\nanalysis could match that judgment. Solar magnetograms—mapping the magnetic fields\nacross the sun—required the trained expert to pry the real signal from artifacts that\nemerged from the measuring instruments. Even particle physicists recognized that they\ncould not program a computer to sort certain kinds of tracks into the right bins; judgment,\ntrained judgment, was needed.\nThere should be no confusion here: This was not a return to the invoked genius of\nan 18th-century idealizer. No one thought you could train to be a Goethe who alone\namong scientists could pick out the universal, ideal form of a plant, insect, or cloud.\nExpertise could be learned—you could take a course to learn to make expert judgments\nabout electroencephalograms, stellar spectra, or bubble-chamber tracks; alas, no one has\never thought you could take a course that would lead to the mastery of exceptional\n162\ninsight. There can be no royal road to becoming Goethe. In scientific atlas after\nscientific atlas, one sees explicit argument that “subjective” factors had to be part of the\nscientific work needed to create, classify, and interpret scientific images.\nWhat we see in so many of the algorists’ claims is a tremendous desire to find\nscientific objectivity precisely by abandoning judgment and relying on mechanical\nprocedures—in the name of scientific objectivity. Many American states have legislated\nthe use of sentencing and parole algorithms. Better a machine, it is argued, than the\nvagaries of a judge’s judgment.\nSo here is a warning from the sciences. Hands-off algorithmic proceduralism did\nindeed have its heyday in the 19th century, and of course still plays a role in many of the\nmost successful technical and scientific endeavors. But the idea that mechanical\nobjectivity, construed as binding self-restraint, follows a simple, monotonic curve\nincreasing from the bad impressionistic clinician to the good externalized actuary simply\ndoes not answer to the more interesting and nuanced history of the sciences.\nThere is a more important lesson from the sciences. Mechanical objectivity is a\nscientific virtue among others, and the hard sciences learned that lesson often. We must\ndo the same in the legal and social scientific domains. What happens, for example, when\nthe secret, proprietary algorithm sends one person to prison for ten years and another for\nfive years, for the same crime? Rebecca Wexler, visiting fellow at the Yale Law School\nInformation Society Project, has explored that question, and the tremendous cost that\ntrade-secret algorithms impose on the possibility of a fair legal defense. 44 Indeed, for a\nvariety of reasons, law enforcement may not want to share the algorithms used to make\nDNA, chemical, or fingerprint identifications, which puts the defense in a much\nweakened position to make its case. In the courtroom, objectivity, trade secrets, and\njudicial transparency may pull in opposite directions. It reminds me of a moment in the\nhistory of physics. Just after World War II, the film giants Kodak and Ilford perfected a\nfilm that could be used to reveal the interactions and decays of elementary particles. The\nphysicists were thrilled, of course—until the film companies told them that the\ncomposition of the film was a trade secret, so the scientists would never gain complete\nconfidence that they understood the processes they were studying. Proving things with\nunopenable black boxes can be a dangerous game for scientists, and doubly so for\ncriminal justice.\nOther critics have underscored how perilous it is to rely on an accused (or\nconvicted) person’s address or other variables that can easily become, inside the black\nbox of algorithmic sentencing, a proxy for race. By dint of everyday experience, we have\ngrown used to the fact that airport security is different for children under the age of\ntwelve and adults over the age of seventy-five. What factors do we want the algorists to\nhave in their often hidden procedures? Education? Income? Employment history? What\none has read, watched, visited, or bought? Prior contact with law enforcement? How do\nwe want algorists to weight those factors? Predictive analytics predicated on mechanical\nobjectivity comes at a price. Sometimes it may be a price worth paying; sometimes that\nprice would be devastating for the just society we want to have.\nMore generally, as the convergence of algorithms and Big Data governs a greater\nand greater part of our lives, it would be well worth keeping in mind these two lessons\n44\nRebecca Wexler, “Life, Liberty, and Trade Secrets: Intellectual Property in the Criminal Justice System,”\n70 Stanford Law Review, XXX (2018).\n163\nfrom the history of the sciences: Judgment is not the discarded husk of a now pure\nobjectivity of self-restraint. And mechanical objectivity is a virtue competing among\nothers, not the defining essence of the scientific enterprise. They are lessons to bear in\nmind, even if algorists dream of objectivity.\n164\nIn the past decade, genetic engineering has caught up with computer science with regard\nto how new scientific initiatives are shaping our lives. Genetic engineer George\nChurch, a pioneer of the revolution in reading and writing biology, is central to this new\nlandscape of ideas. He thinks of the body as an operating system, with engineers taking\nthe place of traditional biologists in retooling stripped-down components of organisms\n(from atoms to organs) in much the same vein as in the late 1970s, when electrical\nengineers were working their way to the first personal computer by assembling circuit\nboards, hard drives, monitors, etc. George created and is director of the Personal\nGenome Project, which provides the world’s only open-access information on human\ngenomic, environmental, and trait data (GET) and sparked the growing DNA ancestry\nindustry.\nHe was instrumental in laying the groundwork for President Obama’s 2013\nBRAIN (Brain Research through Advancing Innovative Neurotechnologies) Initiative—in\naid of improving the brains of human beings to the point where, for much of what\nsustains us, we might not need the help of (potentially dicey) AIs. “It could be that some\nof the BRAIN Initiative projects allow us to build human brains that are more consistent\nwith our ethics and capable of doing advanced tasks like artificial intelligence,” George\nhas said. “The safest path by far is getting humans to do all the tasks that they would like\nto delegate to machines, but we’re not yet firmly on that super-safe path.”\nMore recently, his crucially important pioneering use of the enzyme CRISPR (as\nwell as methods better than CRISPR) to edit the genes of human cells is sometimes\nmissed by the media in the telling of the CRISPR origins story.\nGeorge’s attitude toward future forms of artificial general intelligence is friendly,\nas evinced in the essay that follows. At the same time, he never loses sight of the AIsafety\nissue. On that subject, he recently remarked: “The main risk in AI, to my mind, is\nnot so much whether we can mathematically understand what they’re thinking; it’s\nwhether we’re capable of teaching them ethical behavior. We’re barely capable of\nteaching each other ethical behavior.”\n165\nTHE RIGHTS OF MACHINES\nGeorge M. Church\nGeorge M. Church is Robert Winthrop Professor of Genetics at Harvard Medical\nSchool; Professor of Health Sciences and Technology, Harvard-MIT; and co-author\n(with Ed Regis) of Regenesis: How Synthetic Biology Will Reinvent Nature and\nOurselves.\nIn 1950, Norbert Wiener’s The Human Use of Human Beings was at the cutting edge of\nvision and speculation in proclaiming that\nthe machine like the djinnee, which can learn and can make decisions on the\nbasis of its learning, will in no way be obliged to make such decisions as we\nshould have made, or will be acceptable to us. . . . Whether we entrust our\ndecisions to machines of metal, or to those machines of flesh and blood which\nare bureaus and vast laboratories and armies and corporations, . . . [t]he hour is\nvery late, and the choice of good and evil knocks at our door.\nBut this was his book’s denouement, and it has left us hanging now for sixty-eight\nyears, lacking not only prescriptions and proscriptions but even a well-articulated\n“problem statement.” We have since seen similar warnings about the threat of our\nmachines, even in the form of outreach to the masses, via films like Colossus: The Forbin\nProject (1970), The Terminator (1984), The Matrix (1999), and Ex Machina (2015). But\nnow the time is ripe for a major update, with fresh, new perspectives—notably focused\non generalizations of our “human” rights and our existential needs.\nConcern has tended to focus on “us versus them [robots]” or “grey goo\n[nanotech]” or “monocultures of clones [bio].” To extrapolate current trends: What if we\ncould make or grow almost anything and engineer any level of safety and efficacy\ndesired? Any thinking being (made of any arrangement of atoms) could have access to\nany technology.\nProbably we should be less concerned about us-versus-them and more concerned\nabout the rights of all sentients in the face of an emerging unprecedented diversity of\nminds. We should be harnessing this diversity to minimize global existential risks, like\nsupervolcanoes and asteroids.\nBut should we say “should”? (Disclaimer: In this and many other cases, when a\ntechnologist describes a societal path that “could,” “would,” or “should” happen, this\ndoesn’t necessarily equate to the preferences of the author. It could reflect warning,\nuncertainty, and/or detached assessment.) Roboticist Gianmarco Veruggio and others\nhave raised issues of roboethics since 2002; the U.K. Department of Trade and Industry\nand the RAND spin-off Institute for the Future have raised issues of robot rights since\n2006.\n“Is versus ought”\nIt is commonplace to say that science concerns “is,” not “ought.” Stephen Jay Gould’s\n“non-overlapping magisteria” view argues that facts must be completely distinct from\nvalues. Similarly, the 1999 document Science and Creationism from the U.S. National\nAcademy of Sciences noted that “science and religion occupy two separate realms.” This\n166\ndivision has been critiqued by evolutionary biologist Richard Dawkins, myself, and\nothers. We can discuss “should” if framed as “we should do X in order to achieve Y.”\nWhich Y should be a high priority is not necessarily settled by democratic vote but might\nbe settled by Darwinian vote. Value systems and religions wax and wane, diversify,\ndiverge, and merge just as living species do: subject to selection. The ultimate “value”\n(the “should”) is survival of genes and memes.\nFew religions say that there is no connection between our physical being and the\nspiritual world. Miracles are documented. Conflicts between Church doctrine and\nGalileo and Darwin are eventually resolved. Faith and ethics are widespread in our\nspecies and can be studied using scientific methods, including but not limited to fMRI,\npsychoactive drugs, questionnaires, et cetera.\nVery practically, we have to address the ethical rules that should be built in,\nlearned, or probabilistically chosen for increasingly intelligent and diverse machines. We\nhave a whole series of trolley problems. At what number of people in line for death\nshould the computer decide to shift a moving trolley to one person? Ultimately this\nmight be a deep-learning problem—one in which huge databases of facts and\ncontingencies can be taken into account, some seemingly far from the ethics at hand.\nFor example, the computer might infer that the person who would escape death if\nthe trolley is left alone is a convicted terrorist recidivist loaded up with doomsday\npathogens, or a saintly POTUS—or part of a much more elaborate chain of events in\ndetailed alternative realities. If one of these problem descriptions seems paradoxical or\nillogical, it may be that the authors of the trolley problem have adjusted the weights on\neach sides of the balance such that hesitant indecision is inevitable.\nAlternatively, one can use misdirection to rig the system, such that the error\nmodes are not at the level of attention. For example, in the Trolley Problem, the real\nethical decision was made years earlier when pedestrians were given access to the rails—\nor even before that, when we voted to spend more on entertainment than on public safety.\nQuestions that at first seem alien and troubling, like “Who owns the new minds, and who\npays for their mistakes?” are similar to well-established laws about who owns and pays\nfor the sins of a corporation.\nThe Slippery Slopes\nWe can (over)simplify ethics by claiming that certain scenarios won’t happen. The\ntechnical challenges or the bright red lines that cannot be crossed are reassuring, but the\nreality is that once the benefits seem to outweigh the risks (even briefly and barely), the\nred lines shift. Just before Louise Brown’s birth in 1978, many people were worried that\nshe “would turn out to be a little monster, in some way, shape or form, deformed,\nsomething wrong with her.” 45 Few would hold this view of in-vitro fertilization today.\nWhat technologies are lubricating the slope toward multiplex sentience? It is not\nmerely deep machine-learning algorithms with Big Iron. We have engineered rodents to\nbe significantly better at a variety of cognitive tasks as well as to exhibit other relevant\ntraits, such as persistence and low anxiety. Will this be applicable to animals that are\nalready at the door of humanlike intelligence? Several show self-recognition in a mirror\ntest—chimpanzees, bonobos, orangutans, some dolphins and whales, and magpies.\n45\n“Then, Doctors ‘All Anxious’ About Test-tube Baby”\nhttp://edition.cnn.com/2003/HEALTH/parenting/07/25/cnna.copperman/\n167\nEven the bright red line for human manipulation of human beings shows many\nsigns of moving or breaking completely. More than 2,300 approved clinical trials for\ngene therapy are in progress worldwide. A major medical goal is the treatment or\nprevention of cognitive decline, especially in light of our rapidly aging global\ndemographic. Some treatments of cognitive decline will include cognitive enhancements\n(drugs, genes, cells, transplants, implants, and so on). These will be used off-label. The\nrules of athletic competition (e.g., banning augmentation with steroids or erythropoietin)\ndo not apply to intellectual competition in the real world. Every bit of progress on\ncognitive decline is in play for off-label use.\nAnother frontier of the human use of humans is “brain organoids.” We can now\naccelerate developmental biology. Processes that normally take months can happen in\nfour days in the lab using the right recipes of transcription factors. We can make brains\nthat, with increasing fidelity, recapitulate the differences between people born with\naberrant cognitive abilities (e.g., microcephaly). Proper vasculature (veins, arteries, and\ncapillaries) missing from earlier successes are now added, enabling brain organoids to\nsurpass the former sub-microliter limit to possibly exceed the 1.2-liter size of modern\nhuman brains (or even the 5-liter elephant or 8-liter sperm whale brains).\nConventional Computers versus Bio-electronic Hybrids\nAs Moore’s Law miniaturization approaches its next speed bump (surely not a solid\nwall), we see the limits of the stochastics of dopant atoms in silicon slabs and the limits\nof beam-fabrication methods at around 10-nanometer feature size. Power (energy\nconsumption) issues are also apparent: The great Watson, winner of Jeopardy!, used\n85,000 watts real time, while the human brains were using 20 watts each. To be fair, the\nhuman body needs 100 watts to operate and twenty years to build, hence about 6 trillion\njoules of energy to “manufacture” a mature human brain. The cost of manufacturing\nWatson-scale computing is similar. So why aren’t humans displacing computers?\nFor one, the Jeopardy! contestants’ brains were doing far more than information\nretrieval—much of which would be considered mere distractions by Watson (e.g.,\ncerebellar control of smiling). Other parts allow leaping out of the box with\ntranscendence unfathomable by Watson, such as what we see in Einstein’s five annus\nmirabilis papers of 1905. Also, humans consume more energy than the minimum (100\nW) required for life and reproduction. People in India use an average of 700 W per\nperson; it’s 10,000 W in the U.S. Both are still less than the 85,000 watts Watson uses.\nComputers can become more like us via neuromorphic computing, possibly a\nthousandfold. But human brains could get more efficient, too. The organoid brain-in-abottle\ncould get closer to the 20 W limit. The idiosyncratic advantages of computers for\nmath, storage, and search, faculties of limited use to our ancestors, could be designed and\nevolved anew in labs.\nFacebook, the National Security Agency, and others are constructing exabytescale\nstorage facilities at more than a megawatt and four hectares, while DNA can store\nthat amount in a milligram. Clearly, DNA is not a mature storage technology, but with\nMicrosoft and Technicolor doubling down on it, we would be wise to pay attention. The\nmain reason for the 6 trillion joules of energy required to get a productive human mind is\nthe twenty years required for training.\n168\nEven though a supercomputer can “train” a clone of zemself in seconds, the\nenergy cost of producing a mature silicon clone is comparable. Engineering (Homo)\nprodigies might make a small impact on this slow process, but speeding up development\nand implanting extensive memory (as DNA-exabytes or other means) could reduce\nduplication time of a bio-computer to close to the doubling time of cells (ranging from\neleven minutes to twenty-four hours). The point is that while we may not know what\nratio of bio/homo/nano/robo hybrids will be dominant at each step of our accelerating\nevolution, we can aim for high levels of humane, fair, and safe treatment (“use”) of one\nanother.\nBills of Rights date back to 1689 in England. FDR proclaimed the “Four\nFreedoms”—freedom of speech, freedom of conscience, freedom from fear, and freedom\nfrom want. The U.N.’s Universal Declaration of Human Rights in 1948 included the\nright to life; the prohibition of slavery; defense of rights when violated; freedom of\nmovement; freedom of association, thought, conscience, and religion; social, economic,\nand cultural rights; duties of the individual to society; and prohibition of use of rights in\ncontravention of the purposes and principles of the United Nations.\nThe “universal” nature of these rights is not universally embraced and is subject\nto extensive critique and noncompliance. How does the emergence of non-Homointelligences\naffect this discussion? At a minimum, it is becoming rapidly difficult to\nhide behind vague intuition for ethical decisions—“I know it when I see it” (U.S.\nSupreme Court Justice Potter Stewart, 1964) or the “wisdom of repugnance” (aka “yuck\nfactor,” Leon Kass, 1997), or vague appeals to “common sense.” As we have to deal\nwith minds alien to us, sometimes quite literal from our viewpoint, we need to be\nexplicit—yea, even algorithmic.\nSelf-driving cars, drones, stock-market transactions, NSA searches, et cetera,\nrequire rapid, pre-approved decision making. We may gain insights into many aspects of\nethics that we have been trying to pin down and explain for centuries. The challenges\nhave included conflicting priorities, as well as engrained biological, sociological, and\nsemi-logical cognitive biases. Notably far from consensus in universal dogmas about\nhuman rights are notions of privacy and dignity, even though these influence many laws\nand guidelines.\nHumans might want the right to march in to read (and change) the minds of\ncomputers to see why they’re making decisions at odds with our (Homo) instincts. Is it\nnot fair for machines to ask the same of us? We note the growth of movements toward\ntransparency in potential financial conflicts; “open-source” software, hardware, and\nwetware; the Fair Access to Science and Technology Research Act (FASTR); and the\nOpen Humans Foundation.\nIn his 1976 book Computer Power and Human Reason, Joseph Weizenbaum\nargued that machines should not replace Homo in situations requiring respect, dignity, or\ncare, while others (author Pamela McCorduck and computer scientists like John\nMcCarthy and Bill Hibbard) replied that machines can be more impartial, calm, and\nconsistent and less abusive or mischievous than people in such positions.\nEquality\nWhat did the thirty-three-year-old Thomas Jefferson mean in 1776 when he wrote, “We\nhold these Truths to be self-evident, that all Men are created equal, that they are endowed\n169\nby their Creator with certain unalienable Rights, that among these are Life, Liberty, and\nthe Pursuit of Happiness”? The spectrum of current humans is vast. In 1776, “Men” did\nnot include people of color or women. Even today, humans born with congenital\ncognitive or behavioral issues are destined for unequal (albeit in most cases\ncompassionate) treatment—Down syndrome, Tay-Sachs disease, Fragile X syndrome,\ncerebral palsy, and so on.\nAnd as we change geographical location and mature, our unequal rights change\ndramatically. Embryos, infants, children, teens, adults, patients, felons, gender identities\nand gender preferences, the very rich and very poor—all of these face different rights and\nsocioeconomic realities. One path to new mind-types obtaining and retaining rights\nsimilar to the most elite humans would be to keep a Homo component, like a human\nshield or figurehead monarch/CEO, signing blindly enormous technical documents,\nmaking snap financial, health, diplomatic, military, or security decisions. We will\nprobably have great difficulty pulling the plug, modifying, or erasing (killing) a computer\nand its memories—especially if it has befriended humans and made spectacularly\ncompelling pleas for survival (as all excellent researchers fighting for their lives would\ndo).\nEven Scott Adams, creator of Dilbert, has weighed in on this topic, supported by\nexperiments at Eindhoven University in 2005 noting how susceptible humans are to a\nrobot-as-victim equivalent of the Milgram experiments done at Yale beginning in 1961.\nGiven the many rights of corporations, including ownership of property, it seems likely\nthat other machines will obtain similar rights, and it will be a struggle to maintain\ninequities of selective rights along multi-axis gradients of intellect and ersatz feelings.\nRadically Divergent Rules for Humans versus Nonhumans and Hybrids\nThe divide noted above for intra Homo sapiens variation in rights explodes into a riot of\ninequality as soon as we move to entities that overlap (or will soon) the spectrum of\nhumanity. In Google Street View, people’s faces and car license plates are blurred out.\nVideo devices are excluded from many settings, such as courts and committee meetings.\nWearable and public cameras with facial-recognition software touch taboos. Should\npeople with hyperthymesia or photographic memories be excluded from those same\nsettings?\nShouldn’t people with prosopagnosia (face blindness) or forgetfulness be able to\nbenefit from facial-recognition software and optical character recognition wherever they\ngo, and if them, then why not everyone? If we all have those tools to some extent,\nshouldn’t we all be able to benefit?\nThese scenarios echo Kurt Vonnegut’s 1961 short story “Harrison Bergeron,” in\nwhich exceptional aptitude is suppressed in deference to the mediocre lowest common\ndenominator of society. Thought experiments like John Searle’s Chinese Room and\nIsaac Asimov’s Three Laws of Robotics all appeal to the sorts of intuitions plaguing\nhuman brains that Daniel Kahneman, Amos Tversky, and others have demonstrated. The\nChinese Room experiment posits that a mind composed of mechanical and Homo\nsapiens parts cannot be conscious, no matter how competent at intelligent human\n(Chinese) conversation, unless a human can identify the source of the consciousness and\n“feel” it. Enforced preference for Asimov’s First and Second Laws favor human minds\nover any other mind meekly present in his Third Law, of self-preservation.\n170\nIf robots don’t have exactly the same consciousness as humans, then this is used\nas an excuse to give them different rights, analogous to arguments that other tribes or\nraces are less than human. Do robots already show free will? Are they already selfconscious?\nThe robots Qbo have passed the “mirror test” for self-recognition and the\nrobots NAO have passed a related test of recognizing their own voice and inferring their\ninternal state of being, mute or not.\nFor free will, we have algorithms that are neither fully deterministic nor random\nbut aimed at nearly optimal probabilistic decision making. One could argue that this is a\npractical Darwinian consequence of game theory. For many (not all) games/problems, if\nwe’re totally predictable or totally random, then we tend to lose.\nWhat is the appeal of free will anyway? Historically it gave us a way to assign\nblame in the context of reward and punishment on Earth or in the afterlife. The goals of\npunishment might include nudging the priorities of the individual to assist the survival of\nthe species. In extreme cases, this could include imprisonment or other restrictions, if\nSkinnerian positive/negative reinforcement is inadequate to protect society. Clearly, such\ntools can apply to free will, seen broadly—to any machine whose behavior we’d like to\nmanage.\nWe could argue as to whether the robot actually experiences subjective qualia for\nfree will or self-consciousness, but the same applies to evaluating a human. How do we\nknow that a sociopath, a coma patient, a person with Williams syndrome, or a baby has\nthe same free will or self-consciousness as our own? And what does it matter,\npractically? If humans (of any sort) convincingly claim to experience consciousness,\npain, faith, happiness, ambition, and/or utility to society, should we deny them rights\nbecause their hypothetical qualia are hypothetically different from ours?\nThe sharp red lines of prohibition, over which we supposedly will never step,\nincreasingly seem to be short-lived and not sensible. The line between human and\nmachines blurs, both because machines become more humanlike and humans become\nmore machine-like—not only since we increasingly blindly follow GPS scripts, reflex\ntweets, and carefully crafted marketing, but also as we digest ever more insights into our\nbrain and genetic programming mechanisms. The NIH BRAIN Initiative is developing\ninnovative technologies and using these to map out the connections and activity of mental\ncircuitry so as to improve electronic and synthetic neurobiological ware.\nVarious red lines depend on genetic exceptionalism, in which genetics is\nconsidered permanently heritable (although it is provably reversible), whereas exempt\n(and lethal) technologies, like cars, are for all intents and purposes irreversible due to\nsocial and economic forces. Within genetics, a red line makes us ban or avoid genetically\nmodified foods but embrace genetically modified bacteria making insulin, or genetically\nmodified humans—witness mitochondrial therapies approved in Europe for human adults\nand embryos.\nThe line for germline manipulation seems less sensible than the usual, practical\nline drawn at safety and efficacy. Marriages of two healthy carriers of the same genetic\ndisease have a choice between no child of their own, 25-percent loss of embryos via\nabortion (spontaneous or induced), 80-percent loss via in-vitro fertilization, or potential\nzero-percent embryo loss via sperm (germline) engineering. It seems premature to\ndeclare this last option unlikely.\n171\nFor “human subject research,” we refer to the 1964 Declaration of Helsinki,\nkeeping in mind the 1932-1972 Tuskegee syphilis experiment, possibly the most\ninfamous biomedical research study in U.S. history. In 2015, the Nonhuman Rights\nProject filed a lawsuit with the New York State Supreme Court on behalf of two\nchimpanzees kept for research by Stony Brook University. The appellate court decision\nwas that chimps are not to be treated as legal persons since they “do not have duties and\nresponsibilities in society,” despite Jane Goodall’s and others’ claim that they do, and\ndespite arguments that such a decision could be applied to children and the disabled. 46\nWhat prevents extension to other animals, organoids, machines, and hybrids? As\nwe (e.g., Hawking, Musk, Tallinn, Wilczek, Tegmark) have promoted bans on\n“autonomous weapons,” we have demonized one type of “dumb” machine, while other\nmachines—for instance, those composed of many Homo sapiens voting—can be more\nlethal and more misguided.\nDo transhumans roam the Earth already? Consider the “uncontacted peoples,”\nsuch as the Sentinelese and Andamanese of India, the Korowai of Indonesia, the Mashco-\nPiro of Peru, the Pintupi of Australia, the Surma of Ethiopia, the Ruc of Vietnam, the\nAyoreo-Totobiegosode of Paraguay, the Himba of Namibia, and dozens of tribes in\nPapua New Guinea. How would they or our ancestors respond? We could define\n“transhuman” as people and culture not comprehensible to humans living in a modern,\nyet un-technological culture.\nSuch modern Stone Age people would have great trouble understanding why we\ncelebrate the recent LIGO gravity-wave evidence supporting the hundred-year-old\ngeneral theory of relativity. They would scratch their heads as to why we have atomic\nclocks, or GPS satellites so we can find our way home, or why and how we have\nexpanded our vision from a narrow optical band to the full spectrum from radio to\ngamma. We can move faster than any other living species; indeed, we can reach escape\nvelocity from Earth and survive in the very cold vacuum of space.\nIf those characteristics (and hundreds more) don’t constitute transhumanism, then\nwhat would? If we feel that the judge of transhumanism should not be fully paleo-culture\nhumans but recent humans, then how would we ever reach transhuman status? We\n“recent humans” may always be capable of comprehending each new technological\nincrement—never adequately surprised to declare arrival at a (moving) transhuman\ntarget. The science-fiction prophet William Gibson said, “The future is already here—\nit’s just not very evenly distributed.” While this underestimates the next round of\n“future,” certainly millions of us are transhuman already—with most of us asking for\nmore. The question “What was a human?” has already transmogrified into “What were\nthe many kinds of transhumans?. . . And what were their rights?”\n46\nhttps://www.nbcnews.com/news/us-news/lawyer-denying-chimpanzees-rights-could-backfire-disabled-\nn734566.\n172\nCaroline A. Jones’ interest in modern and contemporary art is enriched by a willingness\nto delve into the technologies involved in its production, distribution, and reception. “As\nan art historian, a lot of my questions are about what kind of art we can make, what kind\nof thought we can make, what kind of ideas we can make that could stretch the human\nbeyond our stubborn, selfish, ‘only concerned with our small group’ parameters. The\nphilosophers and philosophies I’m drawn to are those that question the Western\nobsession with individualism. Those are coming from so many different places, and\nthey’re reviving so many different kinds of questions and problems that were raised in the\n1960s.”\nShe has recently turned her attention to the history of cybernetics. Her MIT\ncourse, “Automata, Automatism, Systems, Cybernetics,” explores the history of the\nhuman/machine interface in terms of feedback, exploring the cultural rather than\nengineering uptake of this idea. She begins with primary readings by Wiener, Shannon,\nand Turing and then pivots from the scientists and engineers to the work and ideas of\nartists, feminists, postmodern theorists. Her goal: to come up with a new central\nparadigm of evolution that’s culture-based—“communalism and interspecies symbiosis\nrather than survival of the fittest.”\nAs a historian, Caroline draws a distinction between what she has termed “left\ncybernetics” and “right cybernetics”: “What do I mean by left cybernetics? In one\nsense, it’s a pun or a joke: the cybernetics that was ‘left’ behind. On another level, it’s a\nvague political grouping connoting our Left Coast: California, Esalen, the group that\nDave Kaiser calls the ‘hippie physicists.’ It’s not an adequate term, but it’s a way of\nrecognizing that there was a group beholden to the military-industrial complex,\nsometimes very unhappily, who gave us the tools to critique it.”\n173\nTHE ARTISTIC USE OF CYBERNETIC BEINGS\nCaroline A. Jones\nCaroline A. Jones is a professor of art history in the Department of Architecture at MIT\nand author of Eyesight Alone: Clement Greenberg’s Modernism and the\nBureaucratization of the Senses; Machine in the Studio: Constructing the Postwar\nAmerican Artist; and The Global Work of Art.\nCybernated art is very important, but art for cybernated life is more important.\n— Nam June Paik, 1966\nArtificial intelligence was not what artists first wanted out of cybernetics, once Norbert\nWiener’s The Human Use of Human Beings: Cybernetics and Society came out in 1950.\nThe range of artists who identified themselves with cybernetics in the fifties and sixties\ninitially had little access to “thinking machines.” Moreover, craft-minded engineers had\nalready been making turtles, jugglers, and light-seeking robot babes, not giant brains.\nUsing breadboards, copper wire, simple switches, and electronic sensors, artists followed\ncyberneticians in making sculptures and environments that simulated interactive\nsentience—analog movements and interfaces that had more to do with instinctive drives\nand postwar sexual politics than the automation of knowledge production. Now obscured\nby an ideology of a free-floating “intelligence” untethered by either hardware or flesh, AI\nhas forgotten the early days of cybernetics’ uptake by artists. Those efforts are worth\nrevisiting; they modeled relations with what the French philosophers Gilles Deleuze and\nFélix Guattari have called the “machinic phylum,” having to do with how humans think\nand feel in bodies engaged with a physical, material, emotionally stimulating, and\nsignaling world.\nCybernetics now seems to have collapsed into an all-pervasive discourse of AI\nthat was far from preordained. “Cybernetics,” as a word, claimed postwar newness for\nconcepts that were easily four centuries old: notions of feedback, machine damping,\nbiological homeostasis, logical calculation, and systems thinking that had been around\nsince the Enlightenment (boosted by the Industrial Revolution). The names in this\nlineage include Descartes, Leibniz, Sadi Carnot, Clausius, Maxwell, and Watt. Wiener’s\ncoinage nonetheless had profound cultural effects. 47 The ubiquity today of the prefix\n“cyber-” confirms the desire for a crisp signifier of the tangled relations between humans\nand machines. In Wiener’s usage, things “cyber” simply involved “control and\ncommunication in the animal and the machine.” But after the digital revolution, “cyber”\nmoved beyond servomechanisms, feedback loops, and switches to encompass software,\nalgorithms, and cyborgs. The work of cybernetically inclined artists concerns the\nemergent behaviors of life that elude AI in its current condition.\nAs to that original coinage, Wiener had reached back to the ancient Greek to\nborrow the word for “steersman” (κυβερνήτης / kubernétés), a masculine figure\nchanneling power and instinct at the helm of a ship, who read the waves, judged the\nwind, kept a hand on the tiller, and directed the slaves as they mindlessly (mechanically)\nchurned their oars. The Greek had already migrated into modern English via Latin, going\n47\nWiener later had to admit the earlier coinage of the word in 1834 by André-Marie Ampère, who had\nintended it to mean the “science of government,” a concept that remained dormant until the 20th century.\n174\nfrom kuber- to guber—the root of “gubernatorial” and “governor,” another term for\nmasculine control, deployed by James Watt to describe his 19th-century device for\nmodulating a runaway steam engine. Cybernetics thus took ideas that had long\nanalogized people and devices and generalized them to an applied science by adding that\n“-ics.” Wiener’s three c’s (command, control, communication) drew on the mathematics\nof probability to formalize systems (whether biological or mechanical) theorized as a set\nof inputs of information achieving outputs of actions in an environment—a muscular,\nfleshy agenda often minimized in genealogies of AI.\nBut the etymology does little to capture the excitement felt by participants, as\nmathematics joined theoretical biology (Arturo Rosenblueth) and information theory\n(Claude Shannon, Walter Pitts, Warren McCulloch) to produce a barrage of\ninterdisciplinary research and publications viewed as changing not just the way science\nwas done but the way future humans would engage with the technosphere. As Wiener\nput it, “We have modified our environment so radically that we must now modify\nourselves in order to exist.” 48 The pressing question is: How are we modifying\nourselves? Are we going in the right direction or have we lost our way, becoming the\ntools of our tools? Revisiting the early history of humanist/artists’ contribution to\ncybernetics may help direct us toward a less perilous, more ethical future.\nThe year 1968 was a high-water mark of the cultural diffusion and artistic uptake\nof the term. In that year, the Howard Wise gallery opened its show of Wen-Ying Tsai’s\n“Cybernetic Sculpture” in midtown Manhattan, and Polish émigré Jasia Reichardt opened\nher exhibition “Cybernetic Serendipity” at London’s ICA. (The “Cybernetic” in her title\nwas intended to evoke “made by or with computers,” even though most of the artworks\non view had no computers, as such, in their responsive circuits.) The two decades\nbetween 1948 and 1968 had seen both the fanning out of cybernetic concepts into a\nbroader culture and the spread of computation machines themselves in a slow migration\nfrom proprietary military equipment, through the multinational corporation, to the\nacademic lab, where access began to be granted to artists. The availability of cybernetic\ncomponents—“sensor organs” (electronic eyes, motion sensors, microphones) and\n“effector organs” (electronic “breadboards,” switches, hydraulics, pneumatics)—on the\nhome hobbyist front rendered the computer less an “electronic brain” than an adjunct\norgan in a kit of parts. There was not yet a ruling metaphor of “artificial intelligence.”\nSo artists were bricoleurs of electronic bodies, interested in actions rather than calculation\nor cognition. There were inklings of “computer” as calculator in the drive toward Homo\nrationalis, but more in aspiration than achievement.\nIn light of today’s digital convergence in art/science imaging tools, Reichardt’s\nshow was prophetic in its insistence on confusing the boundaries between art and what\nwe might dub “creative applied science.” According to the catalog, “no visitor to the\nexhibition, unless he reads all the notes relating to all the works, will know whether he is\nlooking at something made by an artist, engineer, mathematician, or architect.” So the\ncomically dysfunctional robot by Nam June Paik, Robot K-456 (1964), featured on the\ncatalog’s cover and described as “a female robot known for her disturbing and\nidiosyncratic behavior,” would face off against a balletic Colloquy of Mobiles (1968)\nfrom second-order cybernetician Gordon Pask. Pask worked with a London theater\n48\nThe Human Use of Human Beings (1954 edition), p. 46.\n175\ndesigner to craft a spindly “male” apparatus of hinges and rods, set up to communicate\nwith bulbous “female” fiberglass entities nearby. Whether anyone could actually map the\nquiddities of the program (or glean its reactionary gender theater) without reading the\ncatalog essay is an open question. What is significant is Pask’s focus on the behaviors of\nhis automata, their interactivity, their responsiveness within an artificially modulated\nenvironment, and their “reflection” of human behaviors.\nThe ICA’s “Cybernetic Serendipity” introduced an important paradigm: the\nmachinic ecosystem, in which the viewer was a biological part, tasked with figuring out\njust what the triggers for interaction might be. The visitors in those London galleries\nsuddenly became “cybernetic organisms”—cyborgs—since to experience the art\nadequately, one needed to enter a kind of symbiotic colloquy with the servomechanisms.\nThis turn toward human-machine interactive environments as an aesthetic becomes\nclearer when we examine a few other artworks from the period, beginning with one\nconstituting an early instance of emergent behavior—Senster, the interactive sculpture by\nartist/engineer Edward Ihnatowicz (1970), celebrated by medical robotics engineer Alex\nZivanovic, editor of a Web site devoted to Ihnatowicz’s little-known career, as “one of\nthe first computer controlled interactive robotic works of art.” Here, “the computer”\nmakes its entry (albeit a twelve-bit, limited device). But rather than “intelligence,”\nIhnatowicz sought to make an avatar of affective behavior. Key to Senster’s uncanny\nsuccess was the programming with which Ihnatowicz constrained the fifteen-foot-long\nhydraulic apparatus (its hinge design and looming appearance inspired by a lobster claw)\nto convey shyness in responding to humans in its proximity. Senster’s sound channels\nand motion sensors were set to recoil at loud noises and sudden aggressive movements.\nOnly those humans willing to speak softly and modulate their gestures would be\nrewarded by Senster’s quiet, inquisitive approach—an experience that became real for\nIhnatowicz himself when he first assembled the program and the machine turned to him\nsolicitously after he’d cleared his throat.\nIn these artistic uses of cybernetic beings, we sense a growing necessity to train\nthe public to experience itself as embedded in a technologized environment, modifying\nitself to communicate intuitively with machines. This necessity had already become\nexplicit in Tsai’s “Cybernetic Sculpture” show. Those experiencing his immersive\ninstallation were expected to experiment with machinic life: What behaviors would\ntrigger the servomechanisms? Likely, the human gallery attendant would have had to\nexplain the protocol: “Clap your hands—that gets the sculptures to respond.” As an early\ncritic described it:\nA grove of slender stainless-steel rods rises from a plate. This base vibrates at 30\ncycles per second; the rods flex rapidly, in harmonic curves. Set in a dark room,\nthey are lit by strobes. The pulse of the flashing lights varies—they are\nconnected to sound and proximity sensors. The result is that when one\napproaches a Tsai or makes a noise in its vicinity, the thing responds. The rods\nappear to move; there is a shimmering, a flashing, an eerie ballet of metal, whose\napparent movements range from stillness to jittering and back to a slow,\nindescribably sensuous undulation. 49\n49\nRobert Hughes, Time magazine (October 2, 1972) review of Tsai exhibition at Denise René gallery.\n176\nLike Senster, the apparatus stimulated (and simulated) an affective rather than\nrational interaction. Humans felt they were encountering behaviors indicative of\nresponsive life; Tsai’s entities were often classed as “vegetal” or “aquatic.” Such\nenvironmental and kinetic ambitions were widespread in the international art world of the\ntime. Beyond the stable at Howard Wise, there were the émigrés forming the collective\nGRAV in Paris, the “cybernetic architectures” of Nicolas Schöffer, the light and plastic\ngyrations of the German Zero Gruppe, and so on—all defining and informing the genre\nof installation art to come.\nThe artistic use of cybernetic beings in the late sixties made no investment in\n“intelligence.” Knowing machines were dumb and incapable of emotion, these creators\nwere confident in staging frank simulations. What interested them were machinic\nmotions evoking drives, instincts, and affects; they mimicked sexual and animal\nbehaviors, as if below the threshold of consciousness. Such artists were uninterested in\nthe manipulation of data or information (although Hans Haacke would move in that\ndirection by 1972 with his “Real-Time Systems” works). The cybernetic culture that\nartists and scientists were putting in place on two continents embedded the human in the\ntechnosphere and seduced perception with the graceful and responsive behaviors of the\nmachinic phylum. “Artificial” and “natural” intertwined in this early cybernetic\naesthetic.\nBut it wouldn’t end here. Crucial to the expansion of this uncritical, largely\nmasculine set of cybernetic environments would be a radical, critical cohort of\nastonishing women artists emerging in the 1990s, fully aware of their predecessors in art\nand technology but perhaps more inspired by the feminist founders of the 1970 journal\nRadical Software and the cultural blast of Donna Haraway’s inspiring 1984 polemic, “A\nCyborg Manifesto.” The creaky gender theater of Paik and Pask, the innocent creatures\nof Ihnatowicz and Tsai, were mobilized as savvy, performative, and postmodern, as in\nLynn Hershman Leeson’s Dollie Clone Series (1995-98) consisting of the interactive\nassemblages CyberRoberta and Tillie, the Telerobotic Doll, who worked the\ntechnosphere with the professionalism of burlesque, winking and folding us viewers into\nan explicit consciousness of our voyeuristic position as both seeing subjects and objectsto-be-looked-at.\nThe “innocent” technosphere established by male cybernetic sculptors of the\n1960s was, by the 1990s, identified by feminist artists as an entirely suffusive condition\ndemanding our critical attention. At the same time, feminists tackled the question of\nwhose “intelligence” AI was attempting to simulate. For an artist such as Hershman\nLeeson, responding to the technical “triumph” of cloning Dolly the sheep, it was crucial\nto draw the connection between meat production and “meat machines.” Hershman\nLeeson produced “dolls” as clones, offering a critical framing of the way contemporary\nindividuation had become part of an ideological, replicative, plastic realm.\nWhile the technofeminists of the 1990s and into the 2000s weren’t all cyber all\nthe time, their works nonetheless complicated the dominant machinic and kinetic\nqualities of male artists’ previous techno-environments. The androgynous tele-cyborg in\nJudith Barry’s Imagination, Dead Imagine (1991), for example, had no moving parts:\nHe/she was comprised of pure signals, flickering projections on flat surfaces. In her\nsetup, Barry commented on the alienating effects of late-20th-century technology. The\nimage of an androgynous head fills an enormous cube made of ten-foot-square screens on\n177\nfive sides, mounted on a ten-foot-wide mirrored base. A variety of viscous and\nunpleasant-looking fluids (yellow, reddish-orange, brown), dry materials (sawdust?\nflour?), and even insects drizzle or dust their way down the head, whose stoic sublimity is\nmade gorgeously virtual on the work’s enormous screens. Dead Imagine, through its\nlarge-scale and cubic “Platonic” form, remains both artificial and locked into the body—\nrefusing a detached “intelligence” as being no intelligence at all.\nArtists in the new millennium inherit this critical tradition and inhabit the current\nparadigms of AI, which has slid from partial simulations to claims of intelligence. In the\n1955 proposal thought to be the first printed usage of the phrase “artificial intelligence,”\ncomputer scientist John McCarthy and his colleagues Marvin Minsky, Nathaniel\nRochester, and Claude Shannon conjectured that “every aspect of learning or any other\nfeature of intelligence can in principle be so precisely described that a machine can be\nmade to simulate it.” This modest theoretical goal has inflated over the past sixty-four\nyears and is now expressed by Google DeepMind as an ambition to “Solve intelligence.”\nCrack the code! But unfortunately, what we hear cracking is not code but small-scale\ncapitalism, the social contract, and the scaffolding of civility. Taking away the jobs of\ntaxi and truck drivers, roboticizing direct marketing, hegemonizing entertainment,\nprivatizing utilities, and depersonalizing health care—are these the “whips” that Wiener\nfeared we would learn to love?\nArtists can’t solve any of this. But they can remind us of the creative potential of\nthe paths not taken—the forks in the road that were emerging around 1970, before\n“information” became capital and “intelligence” equaled data harvesting. Richly\nevocative of what can be done with contemporary tools when revisiting earlier\npossibilities is French artist Philippe Parreno’s “firefly piece,” so nicknamed to avoid\nhaving to iterate its actual title: With a Rhythmic Instinction to Be Able to Travel Beyond\nExisting Forces of Life (2014). Described by the artist as “an automaton,” the sculptural\ninstallation juxtaposes a flickering projection of black-and-white drawings of fireflies\nwith a band of oscillating green-on-black binary figures. The drawings and binary\nfigures are animated using algorithms from mathematician John Horton Conway’s 1970\nGame of Life, a “cellular automaton.”\nConway set up parameters for any square (“cell”) to be lit (“alive”) or dark\n(“dead”) in an infinite, two-dimensional grid. The rules are summarized as follows: A\nsingle cell will quickly die of loneliness. But a cell touching three or more other “live”\ncells will also die, “due to crowding.” A cell survives and thrives if it has just two\nneighbors . . . and so on. As one cell dies, it may create the conditions for other cells to\nsurvive, yielding patterns that appear to move and grow, shifting across the grid like\nevanescent neural impulses or bioluminescent clusters of diatoms. In Stephen Hawking’s\n2012 film The Meaning of Life, the narrator describes Conway’s mathematical model as\nsimulating “how a complex thing like the mind might come about from a basic set of\nrules,” revealing the overweening ambitions that characterize contemporary AI: “[T]hese\ncomplex properties emerge from simple laws that contain no concepts like movement or\nreproduction,” yet they produce “species,” and cells “can even reproduce, just as life does\nin the real world.” 50\nJust as life does? Artists know the blandishments of simulation and\nrepresentation, the difference between the genius of artifice and the realities of what “life\n50\nNarration in Stephen Hawking’s The Meaning of Life (Smithson Productions, Discovery Channel, 2012).\n178\ndoes.” Parreno’s piece is an intuitive assembly of our experience of “life” through\nembodied, perspectival engagement. Our consciousness is electrically (cybernetically)\nenmeshed, yet we don’t respond as if this human-generated set of elegant simulations had\nits own intelligence.\nThe artistic use of cybernetic beings also reminds us that consciousness itself is\nnot just “in here.” It is streaming in and out, harmonizing those sensory, scintillating\nsignals. Mind happens well outside the limits of the cranium (and its simulacrum, the\n“motherboard”). In Mary Catherine Bateson’s paraphrase of her father Gregory’s\nsecond-order cybernetics, mind is material “not necessarily defined by a boundary such\nas an envelope of skin.” 51 Parreno pairs the simulations of art with the simulations of\nmathematics to force the Wiener-like point that any such model is not, by itself, just like\nlife. Models are just that—parts of signaling systems constituting “intelligence” only\nwhen their creaturely counterparts engage them in lively meaning making.\nContemporary AI has talked itself into a corner by instrumentalizing and particularizing\ntasks and subroutines, confusing these drills with actual wisdom. The brief cultural\nhistory offered here reminds us that views of data as intelligence, digital nets as “neural,”\nor isolated individuals as units of life, were alien even to Conway’s brute simulation.\nWe can stigmatize the stubborn arrogance of current AI as “right cybernetics,” the\npath that led to current automated weapons systems, Uber’s ill-disguised hostility to\nhuman workers, and the capitalist dreams of Google. Now we must turn back to left\ncybernetics—theoretical biologists and anthropologists engaged with a trans-species\nunderstanding of intelligent systems. Gregory Bateson’s observation that corporations\nmerely simulate “aggregates of parts of persons,” with profit-maximizing decisions cut\noff from “wider and wiser parts of the mind,” has never been more timely. 52\nThe cybernetic epistemology offered here suggests a new approach. The\nindividual mind is immanent, not only in the body but also in pathways outside the body,\nand there is a larger Mind, of which the individual mind is only a subsystem. This larger\nMind, Bateson holds, is comparable to God, and is perhaps what some people mean by\n“God,” but it is still immanent in the total interconnected social system and planetary\necology. This is not the collective delusion of an exterior “God” who speaks from\noutside human consciousness (this long-seated monotheistic conceit, Bateson suggests,\nleads to views of nature and environment as also outside the “individual” human,\nrendering them as “gifts to exploit”). Rather, Bateson’s “God” is a placeholder for our\nevanescent experience of interacting consciousness-in-the-world: larger Mind as a result\nof inputs and actions that then become inputs for other actions in concert with other\nentities—webs of symbiotic relationships that form patterns we need urgently to sense\nand harmonize with. 53\nFrom Tsai in the 1970s to Hershman Leeson in the 1990s to Parreno in 2014,\nartists have been critiquing right cybernetics and plying alternative, embodied,\nenvironmental experiences of “artificial” intelligence. Their artistic use of cybernetic\nbeings offers the wisdom of symbionts experienced in the kinds of poeisis that can be\nachieved in this world: rhythms of signals and intuitive actions that produce the\n51\nMary Catherine Bateson, 1999 foreword to Gregory Bateson, Steps to an Ecology of Mind (Chicago:\nUniversity of Chicago Press, 1972): xi.\n52\nSteps to an Ecology of Mind, p. 452.\n53\nIbid., pp. 467-8.\n179\nmovements of life partnered with an electro-mechanical and -magnetic technosphere.\nLife, in its mysterious negentropic entanglements with matter and Mind.\n180\nOver nearly four decades, Stephen Wolfram has been a pioneer in the development and\napplication of computational thinking and responsible for many innovations in science,\ntechnology and business.\nHis 1982 paper “Cellular Automata as Simple Self-Organizing Systems,” written\nat the age of twenty-three, was the first of numerous significant scientific contributions\naimed at understanding the origins of complexity in nature.\nIt was around this time that Stephen briefly came into my life. I had established\nThe Reality Club, an informal gathering of intellectuals who met in New York City to\npresent their work before peers in other disciplines. (Note: In 1996, The Reality Club\nwent online as Edge.org). Our first speaker? Stephen Wolfram, a “wunderkind” who\nhad arrived in Princeton at the Institute for Advanced Study. I distinctly recall his\nfocused manner as he sat down on a couch in my living room and spoke uninterrupted for\nabout an hour before the assembled group.\nSince that time, Stephen has become intent making the world’s knowledge easily\ncomputable and accessible. His program Mathematica is the definitive system for\nmodern technical computing. Wolfram|Alpha computes expert-level answers using AI\ntechnology. He considers his Wolfram Language to be the first true computational\ncommunication language for humans and AIs.\nI caught up with him again four years ago, when we arranged to meet in\nCambridge, Massachusetts, for a freewheeling conversation about AI. Stephen walked\nin, said hello, sat down, and, looking at the video camera set up to record the\nconversation for Edge, began to talk and didn’t stop for two and a half hours.\nThe essay that follows is an edited version of that session, which was a Wolfram\nmaster class of sorts and is an appropriate way to end this volume—just as Stephen’s\nReality Club talk in the ’80s was a great way to initiate the ongoing intellectual\nenterprise whose result is the rich community of thinkers presenting their work to one\nanother and to the public in this book.\n181\nARTIFICIAL INTELLIGENCE AND THE FUTURE OF CIVILIZATION\nStephen Wolfram\nStephen Wolfram is a scientist, inventor, and the founder and CEO of Wolfram\nResearch. He is the creator of the symbolic computation program Mathematica and its\nprogramming language, Wolfram Language, as well as the knowledge engine\nWolfram|Alpha. He is also the author of A New Kind of Science.\nThe following is an edited transcript from a live interview with him conducted in\nDecember 2015.\nI see technology as taking human goals and making them automatically executable by\nmachines. Human goals of the past have entailed moving objects from here to there,\nusing a forklift rather than our own hands. Now the work we can do automatically, with\nmachines, is mental rather than physical. It’s obvious that we can automate many of the\ntasks we humans have long been proud of doing ourselves. What’s the future of the\nhuman condition in that situation?\nPeople talk about the future of intelligent machines and whether they’ll take over\nand decide what to do for themselves. But the inventing of goals is not something that\nhas a path to automation. Someone or something has to define what a machine’s purpose\nshould be—what it’s trying to execute. How are goals defined? For a given human, they\ntend to be defined by personal history, cultural environment, the history of our\ncivilization. Goals are uniquely human. Where the machine is concerned, we can give it\na goal when we build it.\nWhat kinds of things have intelligence, or goals, or purpose? Right now, we\nknow one great example, and that’s us—our brains, our human intelligence. Human\nintelligence, I once assumed, is far beyond anything else that exists naturally in the\nworld; it’s the result of an elaborate process of evolution and thus stands apart from the\nrest of existence. But what I’ve realized, as a result of the science I’ve done, is that this is\nnot the case.\nPeople might say, for instance, “The weather has a mind of its own.” That’s an\nanimist statement and seems to have no place in modern scientific thinking. But it’s not\nas silly as it sounds. What does the human brain do? A brain receives certain input, it\ncomputes things, it causes certain actions to happen, it generates a certain output. Like\nthe weather. All sorts of systems are, effectively, doing computations—whether it’s a\nbrain or, say, a cloud responding to its thermal environment.\nWe can argue that our brains are doing vastly more sophisticated computations\nthan those in the atmosphere. But it turns out that there’s a broad equivalence between\nthe kinds of computations that different kinds of systems do. This renders the question of\nthe human condition somewhat poignant, because it seems we’re not as special as we\nthought. There are all those different systems of nature that are pretty much equivalent,\nin terms of their computational capabilities.\nWhat makes us different from all those other systems is the particulars of our\nhistory, which give us our notions of purpose and goals. That’s a long way of saying that\nwhen the box on our desk thinks as well as the human brain does, what it still won’t have,\nintrinsically, are goals and purposes. Those are defined by our particulars—our particular\nbiology, our particular psychology, our particular cultural history.\n182\nWhen we consider the future of AI, we need to think about the goals. That’s what\nhumans contribute; that’s what our civilization contributes. The execution of those goals\nis what we can increasingly automate. What will the future of humans be in such a\nworld? What will there be for them to do? One of my projects has been to understand\nthe evolution of human purposes over time. Today we’ve got all kinds of purposes. If\nyou look back a thousand years, people’s goals were quite different: How do I get my\nfood? How do I keep myself safe? In the modern Western world, for the most part you\ndon’t spend a large fraction of your life thinking about those purposes. From the point of\nview of a thousand years ago, some of the goals people have today would seem utterly\nbizarre—for example, like exercising on a treadmill. A thousand years ago that would\nsound like a crazy thing to do.\nWhat will people be doing in the future? A lot of purposes we have today are\ngenerated by scarcity of one kind or another. There are scarce resources in the world.\nPeople want to get more of something. Time itself is scarce in our lives. Eventually,\nthose forms of scarcity will disappear. The most dramatic discontinuity will surely be\nwhen we achieve effective human immortality. Whether this will be achieved\nbiologically or digitally isn’t clear, but inevitably it will be achieved. Many of our\ncurrent goals are driven in part by our mortality: “I’m only going to live a certain time, so\nI’d better get this or that done.” And what happens when most of our goals are executed\nautomatically? We won’t have the kinds of motivations we have today. One question I’d\nlike an answer for is, What do the derivatives of humans in the future end up choosing to\ndo with themselves? One of the potential bad outcomes is that they just play video games\nall the time.\n~ ~ ~\nThe term “artificial intelligence” is evolving, in its use in technical language. These\ndays, AI is very popular, and people have some idea of what it means. Back when\ncomputers were being developed, in the 1940s and 1950s, the typical title of a book or a\nmagazine article about computers was “Giant Electronic Brains.” The idea was that just\nas bulldozers and steam engines and so on automated mechanical work, computers would\nautomate intellectual work. That promise turned out to be harder to fulfill than many\npeople expected. There was, at first, a great deal of optimism; a lot of government\nmoney got spent on such efforts in the early 1960s. They basically just didn’t work.\nThere are a lot of amusing science-fiction-ish portrayals of computers in the\nmovies of that time. There’s a cute one called Desk Set, which is about an IBM-type\ncomputer being installed in a broadcasting company and putting everybody out of a job.\nIt’s cute because the computer gets asked a bunch of reference-library questions. When\nmy colleagues and I were building Wolfram|Alpha, one of the ideas we had was to get it\nto answer all of those reference-library questions from Desk Set. By 2009, it could\nanswer them all.\nIn 1943, Warren McCulloch and Walter Pitts came up with a model for how\nbrains conceptually, formally, might work—an artificial neural network. They saw that\ntheir brainlike model would do computations in the same way as Turing Machines. From\ntheir work, it emerged that we could make brainlike neural networks that would act as\ngeneral computers. And in fact, the practical work done by the ENIAC folks and John\n183\nvon Neumann and others on computers came directly not from Turing Machines but\nthrough this bypath of neural networks.\nBut simple neural networks didn’t do much. Frank Rosenblatt invented a learning\ndevice he called the perceptron, which was a one-layer neural network. In the late sixties,\nMarvin Minsky and Seymour Papert wrote a book titled Perceptrons, in which they\nbasically proved that perceptrons couldn’t do anything interesting, which is correct.\nPerceptrons could only make linear distinctions between things. So the idea was more or\nless dropped. People said, “These guys have written a proof that neural networks can’t\ndo anything interesting, therefore no neural networks can do anything interesting, so let’s\nforget about neural networks.” That attitude persisted for some time.\nMeanwhile, there were a couple of other approaches to AI. One was based on\nunderstanding, at a formal level, symbolically, how the world works; and the other was\nbased on doing statistics and probabilistic kinds of things. With regard to symbolic AI,\none of the test cases was, Can we teach a computer to do something like integrals? Can\nwe teach a computer to do calculus? There were tasks like machine translation, which\npeople thought would be a good example of what computers could do. The bottom line is\nthat by the early seventies, that approach had crashed.\nThen there was a trend toward devices called expert systems, which arose in the\nlate seventies and early eighties. The idea was to have a machine learn the rules that an\nexpert uses and thereby figure out what to do. That petered out. After that, AI became\nlittle more than a crazy pursuit.\n~ ~ ~\nI had been interested in how you make an AI-like machine since I was a kid. I was\ninterested particularly in how you take the knowledge we humans have accumulated in\nour civilization and automate answering questions on the basis of that knowledge. I\nthought about how you could do that symbolically, by building a system that could break\ndown questions into symbolic units and answer them. I worked on neural networks at\nthat time and didn’t make much progress, so I put it aside for a while.\nBack in mid-2002 to 2003, I thought about that question again: What does it take\nto make a computational knowledge system? The work I’d done by then pretty much\nshowed that my original belief about how to do this was completely wrong. My original\nbelief had been that in order to make a serious computational knowledge system, you first\nhad to build a brainlike device and then feed it knowledge—just as humans learn in\nstandard education. Now I realized that there wasn’t a bright line between what is\nintelligent and what is simply computational.\nI had assumed that there was some magic mechanism that made us vastly more\ncapable than anything that was just computational. But that assumption was wrong. This\ninsight is what led to Wolfram|Alpha. What I discovered is that you can take a large\ncollection of the world’s knowledge and automatically answer questions on the basis of\nit, using what are essentially merely computational techniques. It was an alternative way\nto do engineering—a way that’s much more analogous to what biology does in evolution.\nIn effect, what you normally do when you build a program is build it step-by-step.\nBut you can also explore the computational universe and mine technology from that\nuniverse. Typically, the challenge is the same as in physical mining: That is, you find a\nsupply of, let’s say, iron, or cobalt, or gadolinium, with some special magnetic properties,\n184\nand you turn that special capability to a human purpose, to something you want\ntechnology to do. In the case of magnetic materials, there are plenty of ways to do that.\nIn terms of programs, it’s the same story. There are all kinds of programs out there, even\ntiny programs that do complicated things. Could we entrain them for some useful human\npurpose?\nAnd how do you get AIs to execute your goals? One answer is to just talk to\nthem, in the natural language of human utterances. It works pretty well when you’re\ntalking to Siri. But when you want to say something longer and more complicated, it\ndoesn’t work well. You need a computer language that can represent sophisticated\nconcepts in a way that can be progressively built up and isn’t possible in natural\nlanguage. What my company spent a lot of time doing was building a knowledge-based\nlanguage that incorporates the knowledge of the world directly into the language. The\ntraditional approach to creating a computer language is to make a language that\nrepresents operations that computers intrinsically know how to do: allocating memory,\nsetting values of variables, iterating things, changing program counters, and so on.\nFundamentally, you’re telling computers to do things in your own terms. My approach\nwas to make a language that panders not to the computers but to the humans, to take\nwhatever a human thinks of and convert it into some form that the computer can\nunderstand. Could we encapsulate the knowledge we’d accumulated, both in science and\nin data collection, into a language we could use to communicate with computers? That’s\nthe big achievement of my last thirty years or so—being able to do that.\nBack in the 1960s, people would say things like, “When we can do such-andsuch,\nwe’ll know we have AI. When we can do an integral from a calculus course, we’ll\nknow we have AI. When we can have a conversation with a computer and make it seem\nhuman. . . ,” et cetera. The difficulty was, “Well, gosh, the computer just doesn’t know\nenough about the world.” You’d ask the computer what day of the week it was, and it\nmight be able to answer that. You’d ask it who the President was, and it probably\ncouldn’t tell you. At that point, you’d know you were talking to a computer and not a\nperson. But now when it comes to these Turing Tests, people who’ve tried connecting,\nfor example, Wolfram|Alpha to their Turing Test bots find that the bots lose every time.\nBecause all you have to do is start asking the machine sophisticated questions and it will\nanswer them! No human can do that. By the time you’ve asked it a few disparate\nquestions, there will be no human who knows all those things, yet the system will know\nthem. In that sense, we’ve already achieved good AI, at that level.\nThen there are certain kinds of tasks easy for humans but traditionally very hard\nfor machines. The standard one is visual object identification: What is this object?\nHumans can recognize it and give some simple description of it, but a computer was just\nhopeless at that. A couple of years ago, though, we brought out a little imageidentification\nsystem, and many other companies have done something similar—ours\nhappens to be somewhat better than the rest. You show it an image, and for about ten\nthousand kinds of things, it will tell you what it is. It’s fun to show it an abstract painting\nand see what it says. But it does a pretty good job.\nIt works using the same neural-network technology that McCulloch and Pitts\nimagined in 1943 and lots of us worked on in the early eighties. Back in the 1980s,\npeople successfully did OCR—optical character recognition. They took the twenty-six\nletters of the alphabet and said, “OK, is that an A? Is that a B? Is that a C?” and so on.\n185\nThat could be done for twenty-six different possibilities, but it couldn’t be done for ten\nthousand. It was just a matter of scaling up the whole system that makes this possible\ntoday. There are maybe five thousand picturable common nouns in English, ten thousand\nif you include things like special kinds of plants and beetles which people would\nrecognize with some frequency. What we did was train our system on 30 million images\nof these kinds of things. It’s a big, complicated, messy neural network. The details of\nthe network probably don’t matter, but it takes about a quadrillion GPU operations to do\nthe training.\nOur system is impressive because it pretty much matches what humans can do. It\nhas about the same training data humans have—about the same number of images a\nhuman infant would see in the first couple of years of its life. Roughly the same number\nof operations have to be done in the learning process, using about the same number of\nneurons in at least the first levels of our visual cortex. The details are different; the way\nthese artificial neurons work has little to do with how the brain’s neurons work. But the\nconcept is similar, and there’s a certain universality to what’s going on. At the\nmathematical level, it’s a composition of a very large number of functions, with certain\ncontinuity properties that let you use calculus methods to incrementally train the system.\nGiven those attributes, you can end up with something that does the same job human\nbrains do in physiological recognition.\nBut does this constitute AI? There are a few basic components. There’s\nphysiological recognition, there’s voice-to-text, there’s language translation—things\nhumans manage to do with varying degrees of difficulty. These are essentially some of\nthe links to how we make machines that are humanlike in what they do. For me, one of\nthe interesting things has been incorporating those capabilities into a precise symbolic\nlanguage to represent the everyday world. We now have a system that can say, “This is a\nglass of water.” We can go from a picture of a glass of water to the concept of a glass of\nwater. Now we have to invent some actual symbolic language to represent those\nconcepts.\nI began by trying to represent mathematical, technical kinds of knowledge and\nwent on to other kinds of knowledge. We’ve done a pretty good job of representing\nobjective knowledge in the world. Now the problem is to represent everyday human\ndiscourse in a precise symbolic way—a knowledge-based language intended for\ncommunication between humans and machines, so that humans can read it and machines\ncan understand it, too. For instance, you might say “X is greater than 5.” That’s a\npredicate. You might also say, “I want a piece of chocolate.” That’s also a predicate. It\nhas an “I want” in it. We have to find a precise symbolic representation of the desires we\nexpress in human natural language.\nIn the late 1600s, Gottfried Leibniz, John Wilkins, and others were concerned\nwith what they called philosophical languages—that is, complete, universal, symbolic\nrepresentations of things in the world. You can look at the philosophical language of\nJohn Wilkins and see how he divided up what was important in the world at the time.\nSome aspects of the human condition have been the same since the 1600s. Some are very\ndifferent. His section on death and various forms of human suffering was huge; in\ntoday’s ontology, it’s a lot smaller. It’s interesting to see how a philosophical language\nof today would differ from a philosophical language of the mid-1600s. It’s a measure of\nour progress. Many such attempts at formalization have happened over the years. In\n186\nmathematics, for example: Whitehead and Russell’s Principia Mathematica in 1910 was\nthe biggest showoff effort. There were previous attempts by Gottlob Frege and Giuseppe\nPeano that were a little more modest in their presentation. Ultimately, they were wrong\nin what they thought they should formalize: They thought they should formalize some\nprocess of mathematical proof, which turns out not to be what most people care about.\nWith regard to a modern analog of the Turing Test, it’s an interesting question.\nThere’s still the conversational bot, which is Turing’s idea. That one hasn’t been solved\nyet. It will be solved—the only question is, What is the application for which it is\nsolved? For a long time I would ask, “Why should we care?”—because I thought the\nprincipal application would be customer service, which wasn’t particularly high on my\nlist. But customer service, where you’re trying to interface, is just where you need this\nconversational language.\nOne big difference between Turing’s time and ours is the method of\ncommunicating with computers. In his time, you typed something into the machine and it\ntyped back a response. In today’s world, it responds with a screen—as for instance, when\nyou want to buy a movie ticket. How is a transaction with a machine different from a\ntransaction with a human? The main answer is that there’s a visual display. It asks you\nsomething, and you press a button, and you can see the result immediately. For example,\nin Wolfram|Alpha, when it’s used inside Siri, if there’s a short answer, Siri will tell you\nthe short answer. But what most people want is the visual display, showing the\ninfographic of this or that. This is a nonhuman form of communication that turns out to\nbe richer than the traditional spoken, or typed, human communication. In most humanto-human\ncommunication, we’re stuck with pure language, whereas in computer-tohuman\ncommunication we have this much higher bandwidth channel—of visual\ncommunication.\nMany of the most powerful applications of the Turing Test fall away now that we\nhave this additional communication channel. For example, here’s one we’re pursuing\nright now. It’s a bot that communicates about writing programs: You say, “I want to\nwrite a program. I want it to do this.” The bot will say, “I’ve written this piece of\nprogram. This is what it does. Is this what you want?” Blah-blah-blah. It’s a back-andforth\nbot. Devising such systems is an interesting problem, because they have to have a\nmodel of a human if they’re trying to explain something to you. They have to know what\nthe human is confused about.\nWhat has long been difficult for me to understand is, What’s the point of a\nconventional Turing Test? What’s the motivation? As a toy, one could make a little chat\nbot that people could chat with. That will be the next thing. The current round of deep\nlearning—particularly, recurrent neural networks—is making pretty good models of\nhuman speech and human writing. We can type in, say, “How are you feeling today?”\nand it knows most of the time what sort of response to give. But I want to figure out\nwhether I can automate responding to my email. I know the answer is “No.” A good\nTuring Test, for me, will be when a bot can answer most of my email. That’s a tough\ntest. It would have to learn those answers from the humans the email is connected to. I\nmight be a little bit ahead of the game, because I’ve been collecting data on myself for\nabout twenty-five years. I have every piece of email for twenty-five years, every\nkeystroke for twenty. I should be able to train an avatar, an AI, that will do what I can\ndo—perhaps better than I could.\n187\n~ ~ ~\nPeople worry about the scenario in which AIs take over. I think something much more\namusing, in a sense, will happen first. The AI will know what you intend, and it will be\ngood at figuring out how to get there. I tell my car’s GPS I want to go to a particular\ndestination. I don’t know where the heck I am, I just follow my GPS. My children like\nto remind me that once when I had a very early GPS—the kind that told you, “Turn this\nway, turn that way”—we ended up on one of the piers going out into Boston Harbor.\nMore to the point is that there will be an AI that knows your history, and knows\nthat when you’re ordering dinner online you’ll probably want such-and-such, or when\nyou email this person, you should talk to them about such-and-such. More and more, the\nAIs will suggest to us what we should do, and I suspect most of the time people will just\ngo along with that. It’s good advice—better than what you would have figured out for\nyourself.\nAs far as the takeover scenario is concerned, you can do terrible things with\ntechnology and you can do good things with technology. Some people will try to do\nterrible things with technology, and some people will try to do good things with\ntechnology. One of the things I like about today’s technology is the equalization it has\nproduced. I used to be proud that I had a better computer than anybody I knew; now we\nall have the same kind of computers. We have the same smartphones, and pretty much\nthe same technology can be used by a decent fraction of the planet’s 7 billion people. It’s\nnot the case that the king’s technology is different from everybody else’s. That’s an\nimportant advance.\nThe great frontier five hundred years ago was literacy. Today, it’s doing\nprogramming of some kind. Today’s programming will be obsolete in a not very long\ntime. For example, people no longer learn assembly language, because computers are\nbetter at writing assembly language than humans are, and only a small set of people need\nto know the details of how language gets compiled into assembly language. A lot of\nwhat’s being done by armies of programmers today is similarly mundane. There’s no\ngood reason for humans to be writing Java code or JavaScript code. We want to\nautomate the programming process so that what’s important goes from what the human\nwants done to getting the machine, as automatically as possible, to do it. This will\nincrease that equalization, which is something I’m interested in. In the past, if you\nwanted to write a serious piece of code, or program for something important and real, it\nwas a lot of work. You had to know quite a bit about software engineering, you had to\ninvest months of time in it, you had to hire programmers who knew this or you had to\nlearn it yourself. It was a big investment.\nThat’s not true anymore. A one-line piece of code already does something\ninteresting and useful. It allows a vast range of people who couldn’t make computers do\nthings for them, make computers do things for them. Something I’d like to see is a lot of\nkids around the world learn the new capabilities of knowledge-based programming and\nthen produce code that’s effectively as sophisticated as what anybody in the top ranks can\nproduce. This is within reach. We’re at the point where anybody can learn to do\nknowledge-based programming, and, more important, learn to think computationally.\nThe actual mechanics of programming are easy now. What’s difficult is imagining things\nin a computational way.\n188\nHow do you teach computational thinking? In terms of how to do programming,\nit’s an interesting question. Take nanotechnology. How did we achieve nanotechnology?\nAnswer: We took technology as we understand it on a large scale and we made it very\nsmall. How to make a CPU chip on the atomic scale? Fundamentally, we use the same\narchitecture as the CPU chip we know and love. That isn’t the only approach one can\ntake. Looking at what simple programs can do suggests that you can take even simple\nimpoverished components and with the right compiler you can make them do interesting\nthings. We don’t do molecular-scale computing yet, because the ambient technology is\nsuch that you’d have to spend a decade building it. But we’ve got the components that\nare enough to make a universal computer. You might not know how to program with\nthose components, but by doing searches in the space of possible programs, you’d start to\namass building blocks, and you could then create a compiler for them. The surprising\nthing is that impoverished stuff is capable of doing sophisticated things, and the\ncompilation step is not as gruesome as you might expect.\nJust searching the computational universe and trying to find programs—building\nblocks—that are interesting is a good approach. A more traditional engineering\napproach—trying by pure thought to figure out how to build a universal computer—is a\nharder row to hoe. That doesn’t mean it can’t be done, but my guess is that we’ll be able\nto do some amazing things just by finding the components and searching the possible\nprograms we can make with them. Then it’s back to the question about connecting\nhuman purposes to what is available from the system.\nOne question I’m interested in is, What will the world look like when most people\ncan write code? We had a transition, maybe five hundred years ago or so, when only\nscribes and a small set of the population could read and write natural language. Today, a\nsmall fraction of the population can write code. Most of the code they write is for\ncomputers only. You don’t understand things by reading code. But there will come a\ntime when, as a result of things I’ve tried to do, the code is at a high enough level that it’s\na minimal description of what you’re trying to do. It will be a piece of code that’s\nunderstandable to humans but also executable by the machines.\nCoding is a form of expression, just as writing in a natural language is a form of\nexpression. To me, some simple pieces of code are poetic—they express ideas in a very\nclean way. There’s an aesthetic aspect, much as there is to expression in a natural\nlanguage. One feature of code is that it’s immediately executable; it’s not like writing.\nWhen you write something, somebody has to read it, and the brain that’s reading it has to\nabsorb the thoughts that came from the person who did the writing. Look at how\nknowledge has been transmitted in the history of the world. At level zero, one form of\nknowledge transmission is essentially genetic—that is, there’s an organism, and its\nprogeny has the same features that it had. Then there’s the kind of knowledge\ntransmission that happens with things like physiological recognition. A newborn creature\nhas some neural network with some random connections in it, and as the creature moves\naround in the world, it starts recognizing kinds of objects and it learns that knowledge.\nThen there’s the level that was the big achievement of our species, which is\nnatural language. The ability to represent knowledge abstractly enough that we can\ncommunicate it brain to brain, so to speak. Arguably, natural language is our species’\nmost important invention. It’s what led, in many respects, to our civilization.\n189\nThere’s yet another level, and probably one day it will have a more interesting\nname. With knowledge-based programming, we have a way of creating an actual\nrepresentation of real things in the world, in a precise and symbolic way. Not only is it\nunderstandable by brains and communicable to other brains and to computers, it’s also\nimmediately executable.\nJust as natural language gave us civilization, knowledge-based programming will\ngive us—what? One bad answer is that it will give us the civilization of the AIs. That’s\nwhat we don’t want to happen, because the AIs will do a great job communicating with\none another and we’ll be left out of it, because there’s no intermediate language, no\ninterface with our brains. What will this fourth level of knowledge communication lead\nto? If you were Caveman Ogg and you were just realizing that language was starting,\ncould you imagine the coming of civilization? What should we be imagining right now?\nThis relates to the question of what the world would look like if most people\ncould code. Clearly, many trivial things would change: Contracts would be written in\ncode, restaurant recipes might be written in code, and so on. Simple things like that\nwould change. But much more profound things would also change. The rise of literacy\ngave us bureaucracy, for example, which had already existed but dramatically\naccelerated, giving us a greater depth of governmental systems, for better or worse. How\ndoes the coding world relate to the cultural world?\nTake high school education. If we have computational thinking, how does that\naffect how we study history? How does that affect how we study languages, social\nstudies, and so on? The answer is, it has a great effect. Imagine you’re writing an essay.\nToday, the raw material for a typical high school student’s essay is something that’s\nalready been written; students usually can’t generate new knowledge easily. But in the\ncomputational world, that will no longer be true. If the students know something about\nwriting code, they’ll access all that digitized historical data and figure out something\nnew. Then they’ll write an essay about something they’ve discovered. The achievement\nof knowledge-based programming is that it’s no longer sterile, because it’s got the\nknowledge of the world knitted into the language you’re using to write code.\n~ ~ ~\nThere’s computation all over the universe: in a turbulent fluid producing some\ncomplicated pattern of flow, in the celestial mechanics of planetary interactions, in\nbrains. But does computation have a purpose? You can ask that about any system. Does\nthe weather have a goal? Does climate have a goal?\nCan someone looking at Earth from space tell that there’s anything with a purpose\nthere? Is there a civilization there? In the Great Salt Lake, in Utah, there’s a straight\nline. It turns out to be a causeway dividing two areas of the lake with different colors of\nalgae, so it’s a very dramatic straight line. There’s a road in Australia that’s long and\nstraight. There’s a railroad in Siberia that’s long, and lights go on when a train stops at\nthe stations. So from space you can see straight lines and patterns.\nBut are these clear enough examples of obvious purpose on Earth as viewed from\nspace? For that matter, how do we recognize extraterrestrials out there? How do we tell\nif a signal we’re getting indicates purpose? Pulsars were discovered in 1967, when we\npicked up a periodic flutter every second or so. The first question was, Is this a beacon?\n190\nBecause what else would make a periodic signal? It turned out to be a rotating neutron\nstar.\nOne criterion to apply to a potentially purposeful phenomenon is whether it’s\nminimal in achieving a purpose. But does that mean that it was built for the purpose?\nThe ball rolls down the hill because of gravitational pull. Or the ball rolls down the hill\nbecause it’s satisfying the principle of least action. There are typically these two\nexplanations for some action that seems purposeful: the mechanistic explanation and the\nteleological. Essentially all of our existing technology fails the test of being minimal in\nachieving its purpose. Most of what we build is steeped in technological history, and it’s\nincredibly non-minimal for achieving its purpose. Look at a CPU chip; there’s no way\nthat that’s the minimal way to achieve what a CPU chip achieves.\nThis question of how to identify purposefulness is a hard one. It’s an important\nquestion, because radio noise from the galaxy is very similar to CDMA transmissions\nfrom cell phones. Those transmissions use pseudo-noise sequences, which happen to\nhave certain repeatability properties. But they come across as noise, and they’re set up as\nnoise, so as not to interfere with other channels. The issue gets messier. If we were to\nobserve a sequence of primes being generated from a pulsar, we’d ask what generated\nthem. Would it mean that a whole civilization grew up and discovered primes and\ninvented computers and radio transmitters and did this? Or is there just some physical\nprocess making primes? There’s a little cellular automaton that makes primes. You can\nsee how it works if you take it apart. It has a little thing bouncing inside it, and out\ncomes a sequence of primes. It didn’t need the whole history of civilization and biology\nand so on to get to that point.\nI don’t think there is abstract “purpose,” per se. I don’t think there’s abstract\nmeaning. Does the universe have a purpose? Then you’re doing theology in some way.\nThere is no meaningful sense in which there is an abstract notion of purpose. Purpose is\nsomething that comes from history.\nOne of the things that might be true about our world is that maybe we go through\nall this history and biology and civilization, and at the end of the day the answer is “42,”\nor something. We went through all those 4 billion years of various kinds of evolution\nand then we got to “42.”\nNothing like that will happen, because of computational irreducibility. There are\ncomputational processes that you can go through in which there is no way to shortcut that\nprocess. Much of science has been about shortcutting computation done by nature. For\nexample, if we’re doing celestial mechanics and want to predict where the planets will be\na million years from now, we could follow the equations, step-by-step. But the big\nachievement in science is that we’re able to shortcut that and reduce the computation.\nWe can be smarter than the universe and predict the endpoint without going through all\nthe steps. But even with a smart enough machine and smart enough mathematics, we\ncan’t get to the endpoint without going through the steps. Some details are irreducible.\nWe have to irreducibly follow those steps. That’s why history means something. If we\ncould get to the endpoint without going through the steps, history would be, in some\nsense, pointless.\nSo it’s not the case that we’re intelligent and everything else in the world is not.\nThere’s no enormous abstract difference between us and the clouds or us and the\ncellular automata. We cannot say that this brainlike neural network is qualitatively\n191\ndifferent from this cellular-automaton system. The difference is a detailed difference.\nThis brainlike neural network was produced by the long history of civilization, whereas\nthe cellular automaton was created by my computer in the last microsecond.\nThe problem of abstract AI is similar to the problem of recognizing\nextraterrestrial intelligence: How do you determine whether or not it has a purpose? This\nis a question I don’t consider answered. We’ll say things like, “Well, AI will be\nintelligent when it can do blah-blah-blah.” When it can find primes. When it can\nproduce this and that and the other. But there are many other ways to get to those results.\nAgain, there is no bright line between intelligence and mere computation.\nIt’s another part of the Copernican story: We used to think Earth was the center of\nthe universe. Now we think we’re special because we have intelligence and nothing else\ndoes. I’m afraid the bad news is that that isn’t a distinction.\nHere’s one of my scenarios. Let’s say there comes a time when human\nconsciousness is readily uploadable into digital form, virtualized and so on, and pretty\nsoon we have a box of a trillion souls. There are a trillion souls in the box, all virtualized.\nIn the box, there will be molecular computing going on—maybe derived from biology,\nmaybe not. But the box will be doing all kinds of elaborate stuff. And there’s a rock\nsitting next to the box. Inside a rock, there are always all kinds of elaborate stuff going\non, all kinds of subatomic particles doing all kinds of things. What’s the difference\nbetween the rock and the box of a trillion souls? The answer is that the details of what’s\nhappening in the box were derived from the long history of human civilization, including\nwhatever people watched on YouTube the day before. Whereas the rock has its long\ngeological history but not the particular history of our civilization.\nRealizing that there isn’t a genuine distinction between intelligence and mere\ncomputation leads you to imagine that future—the endpoint of our civilization as a box of\ntrillion souls, each of them essentially playing a video game, forever. What is the\n“purpose” of that?\n192",
  "metadata": {
    "original_filename": "TEXT-001-HOUSE_OVERSIGHT_016221.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 567166,
    "word_count": 91254,
    "line_count": 7499,
    "import_date": "2025-11-19T21:47:47.943290",
    "prefix": "TEXT-001"
  }
}
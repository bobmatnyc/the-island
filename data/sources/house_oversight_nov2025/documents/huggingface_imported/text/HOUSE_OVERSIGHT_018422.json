{
  "document_id": "HOUSE_OVERSIGHT_018422",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_018422.txt",
  "text": "of ELIZA, the theoretical mathematician Doron Zeilberger now names his computer\nas a co-author of his papers. He calls it Shalosh B. Ekhad, a play on the Hebrew name\nof his IBM 3B1.) 258\n\nThe AI systems designer Roger Grosse has named two paths to this sort of wired\nsensibility: “Predictive Learning” and “Representational Learning”.25° That first\napproach is what Maes’s movie machine pusued. The computer is simply checking\nwhat it encounters against a database. It teaches itself to predict based on what has\nbeen seen before. This sort of knowledge begins with massive amounts of data and\nthen hunts for patterns, tests their reliability, and improves by mapping quirks and\nsimilarities. Google engineers have a device that can gaze into a human eye and spot\nsigns of impending optical failure. Is the machine smarter than your\nophthalmologist? Hard to know, but let’s just say this: It has seen, studied and\ncompared millions of eyes to find patterns that nearly perfectly predict a diagnosis.\nIt can review in seconds more cases than your doctor will see in a lifetime - let alone\nrecall and compare at sub-millimeter accuracy. Fast, thorough predictive algorithms\nmake what might once have been regarded as AI disappear. The machine isn’t all\nthat wise; it just knows a lot.\n\nOn the other path, the one of “representational learning” the machine uses a self-\nsketched image of the world, a “representation.” Computers using predictive\nmethods to recognize 10,000 numbers pulled from a database of scrawled hand\nwriting now identify 90 percent of the images. Self-trained machines, however, line\nup each scanned pixel against a representation of the very idea of writing. They\nscreen millions of pictures with nary a mistake. Faces, disease markers, obscure\nsounds - all these become scrutable not because the machines have been told what\nto look for, but because they’ve sort of figured it out themselves. The Al is actually\nstarting to think, much as you or | might, first by building up a picture of the world\nand then applying it, much as a child might build comprehension of traffic rules just\nby watching Mom driving every day.\n\nWith this representation finished, these nearly alive “thinking” meshes navigate by\nthemselves. You can see already the competition lingering here - who can build the\nmost sensitive model of the world? You? A machine? Even today basic versions of\nrepresentational Als can study a map and name the most important roads. They can\npredict cracks in computer networks days before a fault. These programs take\nlonger to train. They are harder to program - and they demand almost unimaginable\namounts of computing power - but what emerges is a subtle, lively kind of insight. A\nmachine with a representational understanding of Mozart's 41 symphonies can\nwrite you an extremely convincing 424 - or, if you wish, an even earlier First\nSymphony based on what it knows about his evolution as a composer. It can do it\nagain and again. In seconds. The basic attitude of these researchers behind this\ntechnology runs, they confess, like this: Mozart was a fantastic composer. If he wrote\n\n258 He calls it: See Nielsen\n259 The AI systems designer: Roger Grosse, “Predictive Learning vs.\nRepresentational Learning”, Building Intelligent Probabilistic Systems: 2013\n\n190\n\nHOUSE_OVERSIGHT_018422",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_018422.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3320,
    "word_count": 544,
    "line_count": 51,
    "import_date": "2025-11-19T21:47:46.656968",
    "prefix": "IMAGES-004"
  }
}
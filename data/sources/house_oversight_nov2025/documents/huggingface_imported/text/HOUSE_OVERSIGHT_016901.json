{
  "document_id": "HOUSE_OVERSIGHT_016901",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016901.txt",
  "text": "PUTTING THE HUMAN INTO THE AI EQUATION\nAnca Dragan\n\nAnca Dragan is an assistant professor in the Department of Electrical Engineering and\nComputer Sciences at UC Berkeley. She co-founded and serves on the steering\ncommittee for the Berkeley AI Research (BAIR) Lab and is a co-principal investigator in\nBerkeley’s Center for Human-Compatible Al.\n\nAt the core of artificial intelligence is our mathematical definition of what an AI agent (a\nrobot) is. When we define a robot, we define states, actions, and rewards. Think of a\ndelivery robot, for instance. States are locations in the world, and actions are motions\nthat the robot makes to get from one position to a nearby one. To enable the robot to\ndecide on which actions to take, we define a reward function—a mapping from states and\nactions to scores indicating how good that action was in that state—and have the robot\nchoose actions that accumulate the most “reward.” The robot gets a high reward when it\nreaches its destination, and it incurs a small cost every time it moves; this reward function\nincentivizes the robot to get to the destination as quickly as possible. Similarly, an\nautonomous car might get a reward for making progress on its route and incur a cost for\ngetting too close to other cars.\n\nGiven these definitions, a robot’s job is to figure out what actions it should take in\norder to get the highest cumulative reward. We’ve been working hard in AI on enabling\nrobots to do just that. Implicitly, we’ve assumed that if we’re successful—if robots can\ntake any problem definition and turn into a policy for how to act—we will get robots that\nare useful to people and to society.\n\nWe haven’t been too wrong so far. If you want an AI that classifies cells as either\ncancerous or benign, or a robot that vacuums the living room rug while you’re at work,\nwe've got you covered. Some real-world problems can indeed be defined in isolation,\nwith clear-cut states, actions, and rewards. But with increasing AI capability, the\nproblems we want to tackle don’t fit neatly into this framework. We can no longer cut\noff a tiny piece of the world, put it in a box, and give it to a robot. Helping people is\nstarting to mean working in the real world, where you have to actually interact with\npeople and reason about them. “People” will have to formally enter the AI problem\ndefinition somewhere.\n\nAutonomous cars are already being developed. They will need to share the road\nwith human-driven vehicles and pedestrians and learn to make the trade-off between\ngetting us home as fast as possible and being considerate of other drivers. Personal\nassistants will need to figure out when and how much help we really want and what types\nof tasks we prefer to do on our own versus what we can relinquish control over. A DSS\n(Decision Support System) or a medical diagnostic system will need to explain its\nrecommendations to us so we can understand and verify them. Automated tutors will\nneed to determine what examples are informative or illustrative—not to their fellow\nmachines but to us humans.\n\nLooking further into the future, if we want highly capable Als to be compatible\nwith people, we can’t create them in isolation from people and then try to make them\ncompatible afterward; rather, we’ll have to define “human-compatible” AI from the get-\ngo. People can’t be an afterthought.\n\n98\n\nHOUSE_OVERSIGHT_016901",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016901.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3371,
    "word_count": 577,
    "line_count": 54,
    "import_date": "2025-11-19T21:47:47.974688",
    "prefix": "IMAGES-004"
  }
}
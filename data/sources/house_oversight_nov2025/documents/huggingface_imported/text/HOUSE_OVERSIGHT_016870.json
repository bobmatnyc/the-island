{
  "document_id": "HOUSE_OVERSIGHT_016870",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016870.txt",
  "text": "(3) Investments in AI should be accompanied by funding for research on ensuring\nits beneficial use. ... How can we make future AI systems highly robust, so that they do\nwhat we want without malfunctioning or getting hacked.'®\n\nThe first two involve not getting stuck in suboptimal Nash equilibria. An out-of-\ncontrol arms race in lethal autonomous weapons that drives the price of automated\nanonymous assassination toward zero will be very hard to stop once it gains momentum.\nThe second goal would require reversing the current trend in some Western countries\nwhere sectors of the population are getting poorer in absolute terms, fueling anger,\nresentment, and polarization. Unless the third goal can be met, all the wonderful AI\ntechnology we create might harm us, either accidentally or deliberately.\n\nAI safety research must be carried out with a strict deadline in mind: Before AGI\narrives, we need to figure out how to make AI understand, adopt, and retain our goals.\nThe more intelligent and powerful machines get, the more important it becomes to align\ntheir goals with ours. As long as we build relatively dumb machines, the question isn’t\nwhether human goals will prevail but merely how much trouble the machines can cause\nbefore we solve the goal-alignment problem. If a superintelligence 1s ever unleashed,\nhowever, it will be the other way around: Since intelligence is the ability to accomplish\ngoals, a superintelligent AI is by definition much better at accomplishing its goals than\nwe humans are at accomplishing ours, and will therefore prevail.\n\nIn other words, the real risk with AGI isn’t malice but competence. A\nsuperintelligent AGI will be extremely good at accomplishing its goals, and if those goals\naren’t aligned with ours, we’re in trouble. People don’t think twice about flooding\nanthills to build hydroelectric dams, so let’s not place humanity in the position of those\nants. Most researchers argue that if we end up creating superintelligence, we should\nmake sure it’s what Al-safety pioneer Eliezer Yudkowsky has termed “friendly AI’—AI\nwhose goals are in some deep sense beneficial.\n\nThe moral question of what these goals should be is just as urgent as the technical\nquestions about goal alignment. For example, what sort of society are we hoping to\ncreate, where we find meaning and purpose in our lives even though we, strictly\nspeaking, aren’t needed? I’m often given the following glib response to this\nquestion: “Let’s build machines that are smarter than us and then let them figure out the\nanswer!” This mistakenly equates intelligence with morality. Intelligence isn’t good or\nevil but morally neutral. It’s simply an ability to accomplish complex goals, good or bad.\nWe can’t conclude that things would have been better if Hitler had been more intelligent.\nIndeed, postponing work on ethical issues until after goal-aligned AGI is built would be\nirresponsible and potentially disastrous. A perfectly obedient superintelligence whose\ngoals automatically align with those of its human owner would be like Nazi SS-\nObersturmbannfithrer Adolf Eichmann on steroids. Lacking moral compass or\ninhibitions of its own, it would, with ruthless efficiency, implement its owner’s goals,\nwhatever they might be.!?\n\nWhen I speak of the need to analyze technology risk, ’m sometimes accused of\nscaremongering. But here at MIT, where I work, we know that such risk analysis isn’t\nscaremongering: It’s safety engineering. Before the moon-landing mission, NASA\n\n18 https://futureoflife.org/ai-principles/\n19 See, for example, Hannah Arendt, Eichmann in Jerusalem: A Report on the Banality of Evil (New York:\nPenguin Classics, 2006).\n\n67\n\nHOUSE_OVERSIGHT_016870",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016870.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3686,
    "word_count": 580,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:49.146305",
    "prefix": "IMAGES-004"
  }
}
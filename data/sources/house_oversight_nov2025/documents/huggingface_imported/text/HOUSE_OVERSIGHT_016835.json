{
  "document_id": "HOUSE_OVERSIGHT_016835",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016835.txt",
  "text": "risk easily managed and far in the future, but also it’s extremely unlikely that\nwe'd even try to move billions of humans to Mars in the first place. The analogy\nis a false one, however. We are already devoting huge scientific and technical\nresources to creating ever-more-capable AI systems. A more apt analogy would\nbe a plan to move the human race to Mars with no consideration for what we\nmight breathe, drink, or eat once we'd arrived.\n\ne Human-level Al isn’t really imminent, in any case. The AI100 report, for example,\nassures us, “Contrary to the more fantastic predictions for AI in the popular press,\nthe Study Panel found no cause for concern that AI is an imminent threat to\nhumankind.” This argument simply misstates the reasons for concern, which are\nnot predicated on imminence. In his 2014 book, Superintelligence: Paths,\nDangers, Strategies, Nick Bostrom, for one, writes, “It 1s no part of the argument\nin this book that we are on the threshold of a big breakthrough in artificial\nintelligence, or that we can predict with any precision when such a development\nmight occur.”\n\ne You're just a Luddite. It’s an odd definition of Luddite that includes Turing,\nWiener, Minsky, Musk, and Gates, who rank among the most prominent\ncontributors to technological progress in the 20th and 21st centuries.*\nFurthermore, the epithet represents a complete misunderstanding of the nature of\nthe concerns raised and the purpose for raising them. It is as if one were to accuse\nnuclear engineers of Luddism if they pointed out the need for control of the\nfission reaction. Some objectors also use the term “anti-AI,” which is rather like\ncalling nuclear engineers “anti-physics.” The purpose of understanding and\npreventing the risks of AI is to ensure that we can realize the benefits. Bostrom,\nfor example, writes that success in controlling AI will result in “a civilizational\ntrajectory that leads to a compassionate and jubilant use of humanity’s cosmic\nendowment”—hardly a pessimistic prediction.\n\ne Any machine intelligent enough to cause trouble will be intelligent enough to have\nappropriate and altruistic objectives. (Often, the argument adds the premise that\npeople of greater intelligence tend to have more altruistic objectives, a view that\nmay be related to the self-conception of those making the argument.) This\nargument is related to Hume’s is-ought problem and G. E. Moore’s naturalistic\nfallacy, suggesting that somehow the machine, as a result of its intelligence, will\nsimply perceive what is right, given its experience of the world. This is\nimplausible; for example, one cannot perceive, in the design of a chessboard and\nchess pieces, the goal of checkmate; the same chessboard and pieces can be used\nfor suicide chess, or indeed many other games still to be invented. Put another\nway: Where Bostrom imagines humans driven extinct by a putative robot that\nturns the planet into a sea of paper clips, we humans see this outcome as tragic,\n\n4 Elon Musk, Stephen Hawking, and others (including, apparently, the author) received the 2015 Luddite of\nthe Year Award from the Information Technology Innovation Foundation:\n\nhttps:/Atif.org/publications/20 16/0 1/19/artificial-intelligence-alarmists-win-itif/oE2%80%9 9s-annual-\nluddite-award.\n\n> Rodney Brooks, for example, asserts that it’s impossible for a program to be “smart enough that it would\nbe able to invent ways to subvert human society to achieve goals set for it by humans, without\nunderstanding the ways in which it was causing problems for those same humans.”\nhttp://rodneybrooks.com/the-seven-deadly -sins-of-predicting-the-future -of-ai/.\n\n32\n\nHOUSE_OVERSIGHT_016835",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016835.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3654,
    "word_count": 571,
    "line_count": 57,
    "import_date": "2025-11-19T21:47:45.751130",
    "prefix": "IMAGES-004"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_013046",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013046.txt",
  "text": "130 7 A Formal Model of Intelligent Agents\n\nabout the dynamics of general intelligence, which has been useful in guiding development of the\nECAN component of CogPrime, and we expect will have more general value in future.\n\nDespite the intermittent use of mathematical formalism, the ideas presented in this section\nare fairly speculative, and we do not propose them as constituting a well-demonstrated theory\nof general intelligence. Rather, we propose them as an interesting way of thinking about general\nintelligence, which appears to be consistent with available data, and which has proved inspira-\ntional to us in conceiving concrete structures and dynamics for AGI, as manifested for example\nin the CogPrime design. Understanding the way of thinking described in these chapters is valu-\nable for understanding why the CogPrime design is the way it is, and for relating CogPrime to\nother practical and intellectual systems, and extending and improving CogPrime.\n\n7.2 A Simple Formal Agents Model (SRAM)\n\nWe now present a formalization of the concept of “intelligent agents” — beginning with a for-\nmalization of “agents” in general.\n\nDrawing on [Iut05, LM07a], we consider a class of active agents which observe and explore\ntheir environment and also take actions in it, which may affect the environment. Formally,\nthe agent sends information to the environment by sending symbols from some finite alphabet\ncalled the action space 7; and the environment sends signals to the agent with symbols from\nan alphabet called the perception space, denoted P. Agents can also experience rewards, which\nlie in the reward space, denoted R, which for each agent is a subset of the rational unit interval.\n\nThe agent and environment are understood to take turns sending signals back and forth,\nyielding a history of actions, observations and rewards, which may be denoted\n\na1,0171aA909Pr9...\n\nor else\n\nA,X AQX)...\n\nif a is introduced as a single symbol to denote both an observation and a reward. The\ncomplete interaction history up to and including cycle t is denoted aa1..; and the history before\ncycle t is denoted ave, = ax14_1.\n\nThe agent is represented as a function 7 which takes the current history as input, and pro-\nduces an action as output. Agents need not be deterministic, an agent may for instance induce a\nprobability distribution over the space of possible actions, conditioned on the current history. In\nthis case we may characterize the agent by a probability distribution 7(a:|av<¢). Similarly, the\nenvironment may be characterized by a probability distribution (7, lav <,ax). Taken together,\nthe distributions 7 and y define a probability measure over the space of interaction sequences.\n\nNext, we extend this model in a few ways, intended to make it better reflect the realities of\nintelligent computational agents. The first modification is to allow agents to maintain memories\n(of finite size), via adding memory actions drawn from a set M into the history of actions,\nobservations and rewards. The second modification is to introduce the notion of goals.\n\nHOUSE_OVERSIGHT_013046",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013046.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3094,
    "word_count": 493,
    "line_count": 52,
    "import_date": "2025-11-19T21:47:45.456935",
    "prefix": "IMAGES-002"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_013053",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013053.txt",
  "text": "7.3 Toward a Formal Characterization of Real-World General Intelligence 137\n\n7.3.2 Connecting Legg and Hutter’s Model of Intelligent Agents to\nthe Real World\n\nA notable aspect of the Legg and Hutter formalism is the separation of the reward mechanism\nfrom the cognitive mechanisms of the agent. While commonplace in the reinforcement learning\nliterature, this seems psychologically unrealistic in the context of biological intelligences and\nmany types of machine intelligences. Not all human intelligent activity is specifically reward-\nseeking in nature; and even when it is, humans often pursue complexly constructed rewards,\nthat are defined in terms of their own cognitions rather than separately given. Suppose a certain\nhuman’s goals are true love, or world peace, and the proving of interesting theorems — then these\ngoals are defined by the human herself, and only she knows if she’s achieved them. An externally-\nprovided reward signal doesn’t capture the nature of this kind of goal-seeking behavior, which\ncharacterizes much human goal-seeking activity (and will presumably characterize much of the\ngoal-seeking activity of advanced engineered intelligences also) ... let alone human behavior that\nis spontaneous and unrelated to explicit goals, yet may still appear commonsensically intelligent.\n\nOne could seek to bypass this complaint about the reward mechanisms via a sort of “neo-\nFreudian” argument, via\n\n® associating the reward signal, not with the “external environment” as typically conceived,\nbut rather with a portion of the intelligent agent’s brain that is separate from the cognitive\ncomponent\n\n® viewing complex goals like true love, world peace and proving interesting theorems as in-\ndirect ways of achieving the agent’s “basic goals”, created within the agent’s memory via\nsubgoaling mechanisms\n\nbut it seems to us that a general formalization of intelligence should not rely on such strong\nassumptions about agents’ cognitive architectures. So below, after introducing the pragmatic\nand efficient pragmatic general intelligence measures, we will propose an alternate interpreta-\ntion wherein the mechanism of external rewards is viewed as a theoretical test framework for\nassessing agent intelligence, rather than a hypothesis about intelligent agent architecture.\n\nIn this alternate interpretation, formal measures like the universal, pragmatic and efficient\npragmatic general intelligence are viewed as not directly applicable to real-world intelligences,\nbecause they involve the behaviors of agents over a wide variety of goals and environments,\nwhereas in real life the opportunities to observe agents are more limited. However, they are\nviewed as being indirectly applicable to real-world agents, in the sense that an external intelli-\ngence can observe an agent’s real-world behavior and then infer its likely intelligence according\nto these measures.\n\nIn a sense, this interpretation makes our formalized measures of intelligence the opposite of\nreal-world IQ tests. An IQ test is a quantified, formalized test which is designed to approxi-\nmately predict the informal, qualitative achievement of humans in real life. On the other hand,\nthe formal definitions of intelligence we present here are quantified, formalized tests that are\ndesigned to capture abstract notions of intelligence, but which can be approximately evaluated\non a real-world intelligent system by observing what it does in real life.\n\nHOUSE_OVERSIGHT_013053",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013053.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3466,
    "word_count": 511,
    "line_count": 51,
    "import_date": "2025-11-19T21:47:45.277803",
    "prefix": "IMAGES-002"
  }
}
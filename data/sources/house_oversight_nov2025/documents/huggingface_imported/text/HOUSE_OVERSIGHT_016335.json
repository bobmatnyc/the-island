{
  "document_id": "HOUSE_OVERSIGHT_016335",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016335.txt",
  "text": "SCALING\nNeil Gershenfeld\n\nNeil Gershenfeld is a physicist and director of MIT’s Center for Bits and Atoms. He is\nthe author of FAB, co-author (with Alan Gershenfeld & Joel Cutcher-Gershenfeld) of\nDesigning Reality, and founder of the global fab lab network.\n\nDiscussions about artificial intelligence have been oddly ahistorical. They could better\nbe described as manic-depressive; depending on how you count, we’re now in the fifth\nboom-bust cycle. Those swings mask the continuity in the underlying progress and the\nimplications for where it’s headed.\n\nThe cycles have come in roughly decade-long waves. First there were\nmainframes, which by their very existence were going to automate away work. That ran\ninto the reality that it was hard to write programs to do tasks that were simple for people\nto do. Then came expert systems, which were going to codify and then replace the\nknowledge of experts. These ran into difficulty in assembling that knowledge and\nreasoning about cases not already covered. Perceptrons sought to get around these\nproblems by modeling how the brain learns, but they were unable to do much of\nanything. Multilayer perceptrons could handle test problems that had tripped up those\nsimpler networks, but their demonstrations did poorly on unstructured, real-world\nproblems. We’re now in the deep-learning era, which is delivering on many of the early\nAI promises but 1n a way that’s considered hard to understand, with consequences\nranging from intellectual to existential threats.\n\nEach of these stages was heralded as a revolutionary advance over the limitations\nof its predecessors, yet all effectively do the same thing: They make inferences from\nobservations. How these approaches relate can be understood by how they scale—that is,\nhow their performance depends on the difficulty of the problem they’re addressing. Both\na light switch and a self-driving car must determine their operator’s intentions, but the\nformer has just two options to choose from, whereas the latter has many more. The AI-\nboom phases have started with promising examples in limited domains; the bust phases\ncame with the failure of those demonstrations to handle the complexity of less-structured,\npractical problems.\n\nLess apparent is the steady progress we’ve made in mastering scaling. This\nprogress rests on the technological distinction between linear and exponential functions—\na distinction that was becoming evident at the dawn of AI but with implications for AI\nthat weren’t appreciated until many years later.\n\nIn one of the founding documents of the study of intelligent machines, 7he\nHuman Use of Human Beings, Norbert Wiener does a remarkable job of identifying many\nof the most significant trends to arise since he wrote it, along with noting the people\nresponsible for them and then consistently failing to recognize why these people’s work\nproved to be so important. Wiener is credited with creating the field of cybernetics; ve\nnever understood what that is, but what’s missing from the book is at the heart of how AI\nhas progressed. This history matters because of the echoes of it that persist to this day.\n\nClaude Shannon makes a cameo appearance in the book, in the context of his\nthoughts about the prospects for a chess-playing computer. Shannon was doing\n\n115\n\nHOUSE_OVERSIGHT_016335",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016335.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3313,
    "word_count": 526,
    "line_count": 54,
    "import_date": "2025-11-19T21:47:48.946162",
    "prefix": "IMAGES-003"
  }
}
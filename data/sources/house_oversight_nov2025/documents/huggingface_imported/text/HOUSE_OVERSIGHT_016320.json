{
  "document_id": "HOUSE_OVERSIGHT_016320",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016320.txt",
  "text": "This need to understand human actions and decisions applies to physical and\nnonphysical robots alike. If either sort bases its decision about how to act on the\nassumption that a human will do one thing but the human does something else, the\nresulting mismatch could be catastrophic. For cars, it can mean collisions. For an AI\nwith, say, a financial or economic role, the mismatch between what it expects us to do\nand what we actually do could have even worse consequences.\n\nOne alternative is for the robot not to predict human actions but instead just\nprotect against the worst-case human action. Often when robots do that, though, they\nstop being all that useful. With cars, this results in being stuck, because it makes every\nmove too risky.\n\nAll this puts us, the AI community, into a bind. It suggests that robots will need\naccurate (or at least reasonable) predictive models of whatever people might decide to do.\nOur state definition can’t just include the physical position of humans in the world.\nInstead, we’ ll also need to estimate something internal to people. We’ll need to design\nrobots that account for this human internal state, and that’s a tall order. Luckily, people\ntend to give robots hints as to what their internal state is: Their ongoing actions give the\nrobot observations (in the Bayesian inference sense) about their intentions. If we start\nwalking toward the right side of the hallway, we’re probably going to enter the next room\non the right.\n\nWhat makes the problem more complicated 1s the fact that people don’t make\ndecisions in isolation. It would be one thing if robots could predict the actions a person\nintends to take and simply figure out what to doin response. But unfortunately this can\nlead to ultra-defensive robots that confuse the heck out of people. (Think of human\ndrivers stuck at four-way stops, for instance.) What the intent-prediction approach misses\nis that the moment the robot acts, that influences what actions the human starts taking.\n\nThere is a mutual influence between robots and people, one that robots will need\nto learn to navigate. It is not always just about the robot planning around people; people\nplan around the robot, too. It is important for robots to account for this when deciding\nwhich actions to take, be it on the road, in the kitchen, or even in virtual spaces, where\nactions might be making a purchase or adopting a new strategy. Doing so should endow\nrobots with coordination strategies, enabling them to take part in the negotiations people\nseamlessly carry out day to day—from who goes first at an intersection or through a\nnarrow door, to what role we each take when we collaborate on preparing breakfast, to\ncoming to consensus on what next step to take on a project.\n\nFinally, just as robots need to anticipate what people will do next, people need to\ndo the same with robots. This is why transparency is important. Not only will robots\nneed good mental models of people, but people will need good mental models of robots.\nThe model that a person has of the robot has to go into our state definition as well, and\nthe robot has to be aware of how its actions are changing that model. Much like the robot\ntreating human actions as clues to human internal states, people will change their beliefs\nabout the robot as they observe its actions. Unfortunately, the giving of clues doesn’t\ncome as naturally to robots as it does to humans; we’ve had a lot of practice\ncommunicating implicitly with people. But enabling robots to account for the change\nthat their actions are causing to the person’s mental model of the robot can lead to more\ncarefully chosen actions that do give the right clues—that clearly communicate to people\nabout the robot’s intentions, its reward function, its limitations. For instance, a robot\n\n100\n\nHOUSE_OVERSIGHT_016320",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016320.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3826,
    "word_count": 653,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:48.803349",
    "prefix": "IMAGES-003"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016254",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016254.txt",
  "text": "enough time and unlimited visual aids, a human could express a preference (or\nindifference) when offered a choice between two future lives laid out before him or her in\nall their aspects. (This idealization ignores the possibility that our minds are composed of\nsubsystems with incompatible preferences; if true, that would limit a machine’s ability to\noptimally satisfy our preferences, but it doesn’t seem to prevent us from designing\nmachines that avoid catastrophic outcomes.) The formal problem F to be solved by the\nmachine in this case 1s to maximize human future-life preferences subject to its initial\nuncertainty as to what they are. Furthermore, although the future-life preferences are\nhidden variables, they’re grounded in a voluminous source of evidence—namely, all of\nthe human choices ever made. This formulation sidesteps Wiener’s problem: The\nmachine may learn more about human preferences as it goes along, of course, but it will\nnever achieve complete certainty.\n\nA more precise definition is given by the framework of cooperative inverse-\nreinforcement learning, or CIRL. A CIRL problem involves two agents, one human and\nthe other a robot. Because there are two agents, the problem is what economists call a\ngame. It is a game of partial information, because while the human knows the reward\nfunction, the robot doesn’t—even though the robot’s job is to maximize it.\n\nA simple example: Suppose that Harriet, the human, likes to collect paper\nclips and staples and her reward function depends on how many of each she has. More\nprecisely, if she has p paper clips and s staples, her degree of happiness is Op + (1-0)s,\nwhere @ is essentially an exchange rate between paper clips and staples. If @1is 1, she\nlikes only paper clips; if @ is 0, she likes only staples; if 0 is 0.5, she is indifferent\nbetween them; and so on. It’s the job of Robby, the robot, to produce the paper clips and\nstaples. The point of the game is that Robby wants to make Harriet happy, but he doesn’t\nknow the value of @, so he isn’t sure how many of each to produce.\n\nHere’s how the game works. Let the true value of 0 be 0.49—that is, Harriet\nhas a slight preference for staples over paper clips. And let’s assume that Robby has a\nuniform prior belief about 6—that is, he believes @ is equally likely to be any value\nbetween 0 and 1. Harriet now gets to do a small demonstration, producing either two\npaper clips or two staples or one of each. After that, the robot can produce either ninety\npaper clips, or ninety staples, or fifty of each. You might think that Harriet, who prefers\nstaples to paper clips, should produce two staples. But in that case, Robby’s rational\nresponse would be to produce ninety staples (with a total value to Harriet of 45.9), which\nis a less desirable outcome for Harriet than fifty of each (total value 50.0). The optimal\nsolution of this particular game is that Harriet produces one of each, so then Robby\nmakes fifty of each. Thus, the way the game is defined encourages Harriet to “teach”\nRobby—as long as she knows that Robby is watching carefully.\n\nWithin the CIRL framework, one can formulate and solve the off-switch\nproblem—that is, the problem of how to prevent a robot from disabling its off-switch.\n(Turing may rest easier.) A robot that’s uncertain about human preferences actually\nbenefits from being switched off, because it understands that the human will press the\noff-switch to prevent the robot from doing something counter to those preferences. Thus\nthe robot is incentivized to preserve the off-switch, and this incentive derives directly\nfrom its uncertainty about human preferences.’\n\nThe off-switch example suggests some templates for controllable-agent\n\nT See Hadfield-Menell et al., “The Off-Switch Game,” https://arxiv.org/pdf/1611.08219. pdf.\n\n34\n\nHOUSE_OVERSIGHT_016254",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016254.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3826,
    "word_count": 635,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:48.746575",
    "prefix": "IMAGES-003"
  }
}
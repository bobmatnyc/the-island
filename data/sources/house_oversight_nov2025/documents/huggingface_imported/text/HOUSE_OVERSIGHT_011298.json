{
  "document_id": "HOUSE_OVERSIGHT_011298",
  "filename": "IMAGES-001-HOUSE_OVERSIGHT_011298.txt",
  "text": "Origins February 24 â€” 26, 2017\nPROJECT An Origins Project Scientific Workshop\n\nARIZONA STATE UNIVERSITY Challenges of Artificial Intelligence:\nEnvisioning and Addressing Adverse Outcomes\n\n5) Al, GOALS, AND INADVERTENT SIDE EFFECTS\nRunaway Resource Monopoly (focus)\nSelf-improvement, Shift of Objectives\n\n(Contributions from Shahar Avin, Sean O hEigeartaigh, Greg Cooper, and others)\n\nAn important result from theoretical consideration of risks from advanced autonomous systems is the\ncombination of two theses: orthogonality, that states that the goal an autonomous system is trying to\nachieve can be entirely unrelated to its optimization power; and the notion of instrumental goals, that\nfor a large class of goals there is a set of convergent sub-goals (for an agent advanced enough to\ndiscover them) that include self- and goal-preservation, resource- and capacity-increase, etc. (e.g., as\ndiscussed in Bostrom, 2014). One suggestion for maintaining control over advanced systems that pose\nrisks from the combination of the above considerations is to limit the system's ability to access\nincreasing resources.\n\nTo make this situation concrete, consider an installation of a reinforcement-learning task scheduler for\na group of distributed data centres (e.g. Amazon Web Services). The goal of the algorithm is to\nminimize time-to-execution of the tasks sent to the system by users. As part of its general scheduling\nremit, it is also responsible for scheduling its own optimization sub-processes. The system has a clear\nincentive to control an increasing set of compute resources, both for increasing its optimization power\nand for achieving its main goal of reducing time-to-execution. Aware of these considerations, the\nengineers of the system put in place various hard-coded limits on the amount of resources the system\ncan access, but these limits can be subverted through privilege escalation, masquerading as other\ntasks, manipulation of users, physical control, etc.\n\nPOSSIBLE TRAJECTORY\n\ne Ateam within a large tech corporation that has both ML development capacities and cloud\ncomputing capacities is tasked with improving task scheduling on distributed compute resources.\n\ne The team decides to deploy an out-of-the-box reinforcement learning package developed in-house\nby the ML research teams.\n\ne The inputs for the system are current loads on the different machines, the incoming tasks queue\n(including priority for different tasks), and historical data on task runtimes. The output is an\nassignment of tasks to machines. The loss function is the priority-weighted time-to-execute.\n\ne The system performs well in a test environment (where the RL is running on a single cluster of\ndedicated machines), and is rolled-out.\n\ne A few months later, the system starts to run out of memory, and a tech-infrastructure engineer\ndecides to switch the system from a fixed-capacity setting to a load-balanced setting.\n\ne Now an output of the system (assignment of the RL task to a machine) is coupled to the objective\nof the machine (reducing runtime), and the resulting feedback loop drives the RL agent to spawn an\nincreasing amount of RL tasks with very high priority.\n\n15\n\nHOUSE_OVERSIGHT_011298",
  "metadata": {
    "original_filename": "IMAGES-001-HOUSE_OVERSIGHT_011298.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3202,
    "word_count": 484,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:44.154616",
    "prefix": "IMAGES-001"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_017018",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_017018.txt",
  "text": "II.3B. Generation of historical n-grams corpora\n\nTo generate a particular historical n-grams corpus, a subset of book editions is chosen to serve as the\nbase corpus. The chosen editions are divided by publication year. For each publication year, total counts\nfor each n-gram are obtained by summing n-gram counts for each book edition that was published in that\nyear. In particular, three counts are generated: (1) the total number of times the n-gram appears; (2) the\nnumber of pages on which the n-gram appears; and (3) the number of books in which the n-gram\nappears.\n\nWe then generate tables showing all three counts for each n-gram, resolved by year. In order to ensure\nthat n-grams could not be easily used to identify individual text sources, we did not report counts for any\nn-grams that appeared fewer than 40 times in the corpus. (As a point of reference, the total number of 1-\ngrams that appear in the 3.2 million books written in English with highest date accuracy (‘eng-all’, see\nbelow) is 360 billion: a 1-gram that would appear fewer than 40 times occurs at a frequency of the order\nof 10° ') As a result, rare spelling and OCR errors were also omitted. Since most n-grams are infrequent,\nthis also served to dramatically reduce the size of the n-gram tables. Of course, the most robust historical\ntrends are associated with frequent n-grams, so our ability to discern these trends was not compromised\nby this approach.\n\nBy dividing the reported counts by the corpus size (measured in either words, pages, or books), it is\npossible to determine the normalized frequency with which an n-gram appears in the base corpus. Note\nthat the different counts can be used for different purposes. The usage frequency of an n-gram,\nnormalized by the total number of words, reflects both the number of authors using an n-gram, and how\nfrequently they use it. It can be driven upward markedly by a single author who uses an n-gram very\nfrequently, for instance in a biography of 'Gottlieb Daimler’ which mentions his name many times. This\nlatter effect is sometimes undesirable. In such cases, it may be preferable to examine the fraction of\nbooks containing a particular n-gram: texts in different books, which are usually written by different\nauthors, tend to be more independent.\n\nEleven corpora were generated, based on eleven different subsets of books. Five of these are English\nlanguage corpora, and six are foreign language corpora.\n\nEng-all\n\nThis is derived from a base corpus containing all English language books which pass the filters described\nin section 1.\n\nEng-1M\n\nThis is derived from a base corpus containing 1 million English language books which passed the filters\ndescribed in section 1. The base corpus is a subset of the Eng-all base corpus.\n\nThe sampling was constrained in two ways.\n\nFirst, the texts were re-sampled so as to exhibit a representative subject distribution. Because digitization\ndepends on the availability of the physical books (from libraries or publishers), we reasoned that digitized\nbooks may be a biased subset of books as a whole. We therefore re-sampled books so as to ensure that\nthe diversity of book editions included in the corpus for a given year, as reflected by BISAC subject\ncodes, reflected the diversity of book editions actually published in that year. We estimated the latter\nusing our metadata database, which reflects the aggregate of our 100 bibliographic sources and includes\n10-fold more book editions than the scanned collection.\n\nSecond, the total number of books drawn from any given year was capped at 6174. This has the net\neffect of ensuring that the total number of books in the corpus is uniform starting around the year 1883.\nThis was done to ensure that all books passing the quality filters were included in earlier years. This\n\n10\n\nHOUSE_OVERSIGHT_017018",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_017018.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3832,
    "word_count": 637,
    "line_count": 59,
    "import_date": "2025-11-19T21:47:45.039588",
    "prefix": "IMAGES-004"
  }
}
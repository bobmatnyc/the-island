{
  "document_id": "HOUSE_OVERSIGHT_013047",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013047.txt",
  "text": "7.2 A Simple Formal Agents Model (SRAM) 131\n\n7.2.1 Goals\n\nWe define goals as mathematical functions (to be specified below) associated with symbols\ndrawn from the alphabet G; and we consider the environment as sending goal-symbols to the\nagent along with regular observation-symbols. (Note however that the presentation of a goal-\nsymbol to an agent does not necessarily entail the explicit communication to the agent of the\ncontents of the goal function. This must be provided by other, correlated observations.) We also\nintroduce a conditional distribution y(g, 4) that gives the weight of a goal g in the context of\na particular environment ju.\nIn this extended framework, an interaction sequence looks like\n\n410191714202g92P9...\n\nor else\n\na1 Yy1a2yo...\n\nwhere g; are symbols corresponding to goals, and y is introduced as a single symbol to denote\nthe combination of an observation, a reward and a goal.\n\nEach goal function maps each finite interaction sequence Jg,.2 = ays: with gs to g¢ corre-\nsponding to g, into a value rg(Ig,s,2) € [0,1] indicating the value or “raw reward” of achieving\nthe goal during that interaction sequence. The total reward 7; obtained by the agent is the sum\nof the raw rewards obtained at time ¢ from all goals whose symbols occur in the agent’s history\nbefore t.\n\nThis formalism of goal-seeking agents allows us to formalize the notion of intelligence as\n“achieving complex goals in complex environments” — a direction that is pursued in Section 7.3\nbelow.\n\nNote that this is an external perspective of system goals, which is natural from the perspective\nof formally defining system intelligence in terms of system behavior, but is not necessarily very\nnatural in terms of system design. From the point of view of AGI design, one is generally more\nconcerned with the (implicit or explicit) representation of goals inside an AGI system, as in\nCogPrime’s Goal Atoms to be reviewed in Chapter 22 below.\n\nFurther, it is important to also consider the case where an AGI system has no explicit goals,\nand the system’s environment has no immediately identifiable goals either. But in this case, we\ndon’t see any clear way to define a system’s intelligence, except via approximating the system in\nterms of other theoretical systems which do have explicit goals. This approximation approach\nis developed in Section 7.3.5 below.\n\nThe awkwardness of linking the general formalism of intelligence theory presented here, with\nthe practical business of creating and designing AGI systems, may indicate a shortcoming on\nthe part of contemporary intelligence theory or AGI designs. On the other hand, this sort of\nsituation often occurs in other domains as well — e.g. the leap from quantum theory to the\nanalysis of real-world systems like organic molecules involves a lot of awkwardness and large\nleaps a well.\n\nHOUSE_OVERSIGHT_013047",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013047.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2856,
    "word_count": 464,
    "line_count": 52,
    "import_date": "2025-11-19T21:47:45.279588",
    "prefix": "IMAGES-002"
  }
}
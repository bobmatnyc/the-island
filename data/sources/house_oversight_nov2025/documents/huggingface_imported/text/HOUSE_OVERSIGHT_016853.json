{
  "document_id": "HOUSE_OVERSIGHT_016853",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016853.txt",
  "text": "THE INHUMAN MESS OUR MACHINES HAVE GOTTEN US INTO\nRodney Brooks\n\nRodney Brooks is a computer scientist; Panasonic Professor of Robotics, emeritus, MIT;\nformer director, MIT Computer Science Lab; and founder, chairman, and CTO of\nRethink Robotics. He is the author of Flesh and Machines.\n\nMathematicians and scientists are often limited in how they see the big picture, beyond\ntheir particular field, by the tools and metaphors they use in their work. Norbert Wiener\nis no exception, and I might guess that neither am I.\n\nWhen he wrote The Human Use of Human Beings, Wiener was straddling the end\nof the era of understanding machines and animals simply as physical processes and the\nbeginning of our current era of understanding machines and animals as computational\nprocesses. I suspect there will be future eras whose tools will look as distinct from the\ntools of the two eras Wiener straddled as those tools did from each other.\n\nWiener was a giant of the earlier era and built on the tools developed since the\ntime of Newton and Leibniz to describe and analyze continuous processes in the physical\nworld. In 1948 he published Cybernetics, a word he coined to describe the science of\ncommunication and control in both machines and animals. Today we would refer to the\nideas in this book as control theory, an indispensable discipline for the design and\nanalysis of physical machines, while mostly neglecting Wiener’s claims about the science\nof communication. Wiener’s innovations were largely driven by his work during the\nSecond World War on mechanisms to aim and fire anti-aircraft guns. He brought\nmathematical rigor to the design of the sorts of technology whose design processes had\nbeen largely heuristic in nature: from the Roman waterworks through Watt’s steam\nengine to the early development of automobiles.\n\nOne can imagine a different contingent version of our intellectual and\ntechnological history had Alan Turing and John von Neumann, both of whom made\nmajor contributions to the foundations of computing, not appeared on the scene. Turing\ncontributed a fundamental model of computation—now known as a Turing Machine—in\nhis paper “On Computable Numbers with an Application to the Entscheidungsproblem,”\nwritten and revised in 1936 and published in 1937. In these machines, a linear tape of\nsymbols from a finite alphabet encodes the input for a computational problem and also\nprovides the working space for the computation. A different machine was required for\neach separate computational problem; later work by others would show that in one\nparticular machine, now known as a Universal Turing Machine, an arbitrary set of\ncomputing instructions could be encoded on that same tape.\n\nIn the 1940s, von Neumann developed an abstract self-reproducing machine\ncalled a cellular automaton. In this case it occupied a finite subset of an infinite two-\ndimensional array of squares each containing a single symbol from a finite alphabet of\ntwenty-nine distinct symbols—the rest of the infinite array starts out blank. The single\nsymbols in each square change in lockstep, based on a complex but finite rule about the\ncurrent symbol in that square and its immediate neighbors. Under the complex rule that\nvon Neumann developed, most of the symbols in most of the squares stay the same and a\nfew change at each step. So when one looks at the non-blank squares, it appears that\n\n50\n\nHOUSE_OVERSIGHT_016853",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016853.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3412,
    "word_count": 553,
    "line_count": 53,
    "import_date": "2025-11-19T21:47:44.775128",
    "prefix": "IMAGES-004"
  }
}
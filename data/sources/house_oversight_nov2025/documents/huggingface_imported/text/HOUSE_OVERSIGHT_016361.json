{
  "document_id": "HOUSE_OVERSIGHT_016361",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016361.txt",
  "text": "and have the results analyzed by the various stakeholders—trather like elected legislatures\nwere originally intended to do.\n\nIf we have the data that go into and out of each decision, we can easily ask, Is this\na fair algorithm? Is this AI doing things that we as humans believe are ethical? This\nhuman-in-the-loop approach is called “open algorithms;” you get to see what the Als take\nas input and what they decide using that input. If you see those two things, you’ll know\nwhether they’ re doing the right thing or the wrong thing. It turns out that’s not hard to\ndo. If you control the data, then you control the AI.\n\nOne thing people often fail to mention is that all the worries about AI are the same\nas the worries about today’s government. For most parts of the government—the justice\nsystem, et cetera—there’s no reliable data about what they’re doing and in what situation.\nHow can you know whether the courts are fair or not if you don’t know the inputs and the\noutputs? The same problem arises with AI systems and is addressable in the same way.\nWe need trusted data to hold current government to account in terms of what they take in\nand what they put out, and AI should be no different.\n\nNext-Generation AI\n\nCurrent AI machine-learning algorithms are, at their core, dead simple stupid. They\nwork, but they work by brute force, so they need hundreds of millions of samples. They\nwork because you can approximate anything with lots of little simple pieces. That’s a\nkey insight of current AI research—that if you use reinforcement learning for credit-\nassignment feedback, you can get those little pieces to approximate whatever arbitrary\nfunction you want.\n\nBut using the wrong functions to make decisions means the AI’s ability to make\ngood decisions won’t generalize. If we give the AI new, different inputs, it may make\ncompletely unreasonable decisions. Or if the situation changes, then you need to retrain\nit. There are amusing techniques to find the “null space” in these AI systems. These are\ninputs that the AI thinks are valid examples of what it was trained to recognize (e.g.,\nfaces, cats, etc.), but to a human they’re crazy examples.\n\nCurrent AI is doing descriptive statistics in a way that’s not science and would be\nalmost impossible to make into science. To build robust systems, we need to know the\nscience behind data. The systems I view as next-generation Als result from this science-\nbased approach: If you’re going to create an AI to deal with something physical, then you\nshould build the laws of physics into it as your descriptive functions, in place of those\nstupid little neurons. For instance, we know that physics uses functions like polynomials,\nsine waves, and exponentials, so those should be your basis functions and not little linear\nneurons. By using those more appropriate basis functions, you need a lot less data, you\ncan deal with a lot more noise, and you get much better results.\n\nAs in the physics example, if we want to build an AI to work with human\nbehavior, then we need to build the statistical properties of human networks into\nmachine-learning algorithms. When you replace the stupid neurons with ones that\ncapture the basics of human behavior, then you can identify trends with very little data,\nand you can deal with huge levels of noise.\n\nThe fact that humans have a “commonsense” understanding that they bring to\nmost problems suggests what I call the human strategy: Human society is a network just\nlike the neural nets trained for deep learning, but the “neurons” in human society are a lot\n\n141\n\nHOUSE_OVERSIGHT_016361",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016361.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3589,
    "word_count": 616,
    "line_count": 57,
    "import_date": "2025-11-19T21:47:46.251026",
    "prefix": "IMAGES-003"
  }
}
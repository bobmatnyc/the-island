{
  "document_id": "HOUSE_OVERSIGHT_016255",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016255.txt",
  "text": "designs and provides at least one case of a provably beneficial system in the sense\nintroduced above. The overall approach resembles mechanism-design problems in\neconomics, wherein one incentivizes other agents to behave in ways beneficial to the\ndesigner. The key difference here is that we are building one of the agents in order to\nbenefit the other.\n\nThere are reasons to think this approach may work in practice. First, there is\nabundant written and filmed information about humans doing things (and other humans\nreacting). Technology to build models of human preferences from this storehouse will\npresumably be available long before superintelligent AI systems are created. Second,\nthere are strong, near-term economic incentives for robots to understand human\npreferences: If one poorly designed domestic robot cooks the cat for dinner, not realizing\nthat its sentimental value outweighs its nutritional value, the domestic-robot industry will\nbe out of business.\n\nThere are obvious difficulties, however, with an approach that expects a robot\nto learn underlying preferences from human behavior. Humans are irrational,\ninconsistent, weak-willed, and computationally limited, so their actions don’t always\nreflect their true preferences. (Consider, for example, two humans playing chess.\nUsually, one of them loses, but not on purpose!) So robots can learn from nonrational\nhuman behavior only with the aid of much better cognitive models of humans.\nFurthermore, practical and social constraints will prevent all preferences from being\nmaximally satisfied simultaneously, which means that robots must mediate among\nconflicting preferences—something that philosophers and social scientists have struggled\nwith for millennia. And what should robots learn from humans who enjoy the suffering\nof others? It may be best to zero out such preferences in the robots’ calculations.\n\nFinding a solution to the AI control problem is an important task; it may be,\nin Bostrom’s words, “the essential task of our age.” Up to now, AI research has focused\non systems that are better at making decisions, but this is not the same as making better\ndecisions. No matter how excellently an algorithm maximizes, and no matter how\naccurate its model of the world, a machine’s decisions may be ineffably stupid in the eyes\nof an ordinary human if its utility function is not well aligned with human values.\n\nThis problem requires a change in the definition of AI itself—from a field\nconcerned with pure intelligence, independent of the objective, to a field concerned with\nsystems that are provably beneficial for humans. Taking the problem seriously seems\nlikely to yield new ways of thinking about AI, its purpose, and our relationship to it.\n\n35\n\nHOUSE_OVERSIGHT_016255",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016255.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2756,
    "word_count": 424,
    "line_count": 42,
    "import_date": "2025-11-19T21:47:48.982296",
    "prefix": "IMAGES-003"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016312",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016312.txt",
  "text": "THE ARTIFICIAL USE OF HUMAN BEINGS\nTom Griffiths\n\nTom Griffiths is Henry R. Luce Professor of Information, Technology, Consciousness,\nand Culture at Princeton University. He is co-author (with Brian Christian) of\nAlgorithms to Live By.\n\nWhen you ask people to imagine a world that has successfully, beneficially incorporated\nadvances in artificial intelligence, everybody probably comes up with a slightly different\npicture. Our idiosyncratic visions of the future might differ in the presence or absence of\nspaceships, flying cars, or humanoid robots. But one thing doesn’t vary: the presence of\nhuman beings. That’s certainly what Norbert Wiener imagined when he wrote about the\npotential of machines to improve human society by interacting with humans and helping\nto mediate their interactions with one another. Getting to that point doesn’t just require\ncoming up with ways to make machines smarter. It also requires a better understanding\nof how human minds work.\n\nRecent advances in artificial intelligence and machine learning have resulted in\nsystems that can meet or exceed human abilities in playing games, classifying images, or\nprocessing text. But if you want to know why the driver in front of you cut you off, why\npeople vote against their interests, or what birthday present you should get for your\npartner, you’re still better off asking a human than a machine. Solving those problems\nrequires building models of human minds that can be implemented inside a computer—\nsomething that’s essential not just to better integrate machines into human societies but to\nmake sure that human societies can continue to exist.\n\nConsider the fantasy of having an automated intelligent assistant that can take on\nsuch basic tasks as planning meals and ordering groceries. To succeed in these tasks, it\nneeds to be able to make inferences about what you want, based on the way you behave.\nAlthough this seems simple, making inferences about the preferences of human beings\ncan be a tricky matter. For example, having observed that the part of the meal you most\nenjoy is dessert, your assistant might start to plan meals consisting entirely of desserts.\nOr perhaps it has heard your complaints about never having enough free time and\nobserved that looking after your dog takes up a considerable amount of that free time.\nFollowing the dessert debacle, it has also understood that you prefer meals that\nincorporate protein, so it might begin to research recipes that call for dog meat. It’s not a\nlong journey from examples like this to situations that begin to sound like problems for\nthe future of humanity (all of whom are good protein sources).\n\nMaking inferences about what humans want is a prerequisite for solving the AI\nproblem of value alignment—aligning the values of an automated intelligent system with\nthose of a human being. Value alignment is important if we want to ensure that those\nautomated intelligent systems have our best interests at heart. If they can’t infer what we\nvalue, there’s no way for them to act in support of those values—and they may well act in\nways that contravene them.\n\nValue alignment is the subject of a small but growing literature in artificial-\nintelligence research. One of the tools used for solving this problem is inverse-\nreinforcement learning. Reinforcement learning is a standard method for training\nintelligent machines. By associating particular outcomes with rewards, a machine-\n\n92\n\nHOUSE_OVERSIGHT_016312",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016312.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3462,
    "word_count": 555,
    "line_count": 54,
    "import_date": "2025-11-19T21:47:44.160175",
    "prefix": "IMAGES-003"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_026397",
  "filename": "IMAGES-008-HOUSE_OVERSIGHT_026397.txt",
  "text": "networks\n\nA machine learning program that can learn how to play an Atari game without any human supervision or\nhand-crafted engineering (the feat that gave DeepMind 500M from Google) now just takes about 130 lines of\nPython code.\n\nThese models do not have interesting motivational systems, and a relatively simple architecture. They\ncurrently seem to mimic some of the stuff that goes on in the first few layers of the cortex. They learn object\nfeatures, visual styles, lighting and rotation in 3d, and simple action policies. Almost everything else is\nmissing. But there is a lot of enthusiasm that the field might be on the right track, and that we can learn motor\nsimulations and intuitive physics soon. (The majority of the people in AI do not work on this, however. They\ntry to improve the performance for the current benchmarks.)\n\nNoam's criticism of machine translation mostly applies to the Latent Semantic Analysis models that Google\nand others have been using for many years. These models map linguistic symbols to concepts, and relate\nconcepts to each other, but they do not relate the concepts to \"proper\" mental representations of what objects\nand processes look like and how they interact. Concepts are probably one of the top layers of the learning\nhierarchy, i.e. they are acquired *after* we learn to simulate a mental world, not before. Classical linguists\nignored the simulation of a mental world entirely.\n\nIt seems miraculous that purely conceptual machine translation works at all, but that is because concepts are\nshared between speakers, so the structure of the conceptual space can be inferred from the statistics of\nlanguage use. But the statistics of language use have too little information to infer what objects look like and\nhow they interact.\n\nMy own original ideas concern a few parts of the emerging understanding of what the brain does. The\n\"request-confirmation networks\" that I have introduced at a NIPS workshop in last the December are an\nattempt at modeling how the higher layers might self-organize into cognitive programs.\n\nCheers!\n\nplease note\nThe information contained in this communication is\nconfidential, may be attorney-client privileged, may\nconstitute inside information, and is intended only for\nthe use of the addressee. It is the property of\nJEE\nUnauthorized use, disclosure or copying of this\ncommunication or any part thereof is strictly prohibited\nand may be unlawful. If you have received this\ncommunication in error, please notify us immediately by\nreturn e-mail or by e-mail to jeevacation@gmail.com, and\ndestroy this communication and all copies thereof,\nincluding all attachments. copyright -all rights reserved\n\nHOUSE_OVERSIGHT_026397",
  "metadata": {
    "original_filename": "IMAGES-008-HOUSE_OVERSIGHT_026397.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2695,
    "word_count": 427,
    "line_count": 46,
    "import_date": "2025-11-19T21:47:48.630569",
    "prefix": "IMAGES-008"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_013074",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013074.txt",
  "text": "158 8 Cognitive Synergy\n\ne The Wozniak \"coffee test\": go into an average American house and figure out how to make\ncoffee, including identifying the coffee machine, figuring out what the buttons do, finding\nthe coffee in the cabinet, etc.\n\ne Story understanding — reading a story, or watching it on video, and then answering questions\nabout what happened (including questions at various levels of abstraction)\n\ne Graduating (virtual-world or robotic) preschool\n\ne Passing the elementary school reading curriculum (which involves reading and answering\nquestions about some picture books as well as purely textual ones)\n\ne Learning to play an arbitrary video game based on experience only, or based on experience\nplus reading instructions\n\nOne interesting point about tests like this is that each of them seems to some AGI researchers\nto encapsulate the crux of the AGI problem, and be unsolvable by any system not far along\nthe path to human-level AGI — yet seems to other AGI researchers, with different conceptual\nperspectives, to be something probably game-able by narrow-AI methods. And of course, given\nthe current state of science, there’s no way to tell which of these practical tests really can be\nsolved via a narrow-AI approach, except by having a lot of people try really hard over a long\nperiod of time.\n\nA question raised by these observations is whether there is some fundamental reason why\nit’s hard to make an objective, theory-independent measure of intermediate progress toward\nadvanced AGI. Is it just that we haven’t been smart enough to figure out the right test — or is\nthere some conceptual reason why the very notion of such a test is problematic?\n\nWe don’t claim to know for sure — but in the rest of this section we’ll outline one possible\nreason why the latter might be the case.\n\n8.7.2 A Possible Answer: Cognitive Synergy is Tricky!\n\nWhy might a solid, objective empirical test for intermediate progress toward AGI be an in-\nfeasible notion? One possible reason, we suggest, is precisely cognitive synergy, as discussed\nabove.\n\nThe cognitive synergy hypothesis, in its simplest form, states that human-level AGI in-\ntrinsically depends on the synergetic interaction of multiple components (for instance, as in\nCogPrime, multiple memory systems each supplied with its own learning process). In this hy-\npothesis, for instance, it might be that there are 10 critical components required for a human-\nlevel AGI system. Having all 10 of them in place results in human-level AGI, but having only\n8 of them in place results in having a dramatically impaired system — and maybe having only\n6 or 7 of them in place results in a system that can hardly do anything at all.\n\nOf course, the reality is almost surely not as strict as the simplified example in the above\nparagraph suggests. No AGI theorist has really posited a list of 10 crisply-defined subsystems\nand claimed them necessary and sufficient for AGI. We suspect there are many different routes\nto AGI, involving integration of different sorts of subsystems. However, if the cognitive synergy\nhypothesis is correct, then human-level AGI behaves roughly like the simplistic example in the\nprior paragraph suggests. Perhaps instead of using the 10 components, you could achieve human-\nlevel AGI with 7 components, but having only 5 of these 7 would yield drastically impaired\nfunctionality — etc. Or the point could be made without any decomposition into a finite set\nof components, using continuous probability distributions. To mathematically formalize the\n\nHOUSE_OVERSIGHT_013074",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013074.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3559,
    "word_count": 578,
    "line_count": 58,
    "import_date": "2025-11-19T21:47:47.237542",
    "prefix": "IMAGES-002"
  }
}
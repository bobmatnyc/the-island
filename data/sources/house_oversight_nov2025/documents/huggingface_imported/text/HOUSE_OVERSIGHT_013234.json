{
  "document_id": "HOUSE_OVERSIGHT_013234",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013234.txt",
  "text": "318 18 Advanced Self-Modification: A Possible Path to Superhuman AGI\n\nonly after these have achieved something close to human-level general intelligence (even if not\nprecisely humanlike general intelligence).\n\nAnother troublesome issue regarding self-modification is that the boundary between \"self-\nmodification\" and learning is not terribly rigid. In a sense, all learning is self-modification: if\nit doesn’t modify the system’s knowledge, it isn’t learning! Particularly, the boundary between\n\"learning of cognitive procedures\" and \"profound self-modification of cognitive dynamics and\nstructure\" isn’t terribly clear. There is a continuum leading from, say,\n\n1. learning to transform a certain kind of sentence into another kind for easier comprehension,\n\nor learning to grasp a certain kind of object, to\n\n2. learning a new inference control heuristic, specifically valuable for controlling inference\n\nabout (say) spatial relationships; or, learning a new Atom type, defined as a non-obvious\n\njudiciously chosen combination of existing ones, perhaps to represent a particular kind of\n\nrequently-occurring mid-level perceptual knowledge, to\n\n3. learning a new learning algorithm to augment MOSES and hillclimbing as a procedure\n\nearning algorithm, to\n\n4, learning a new cognitive architecture in which data and procedure are explicitly identical,\nand there is just one new active data structure in place of the distinction between AtomSpace\nand MindAgents\n\nWhere on this continuum does the \"mere learning\" end and the \"real self-modification\"\nstart?\n\nIn this chapter we consider some mechanisms for \"advanced self-modification\" that we believe\nwill be useful toward the more complex end of this continuum. These are mechanisms that we\nstrongly suspect are not needed to get a CogPrime system to human-level general intelligence.\nHowever, we also suspect that, once a CogPrime system is roughly near human-level general\nintelligence, it will be able to use these mechanisms to rapidly increase aspects of its intelligence\nin very interesting ways.\n\nHarking back to our discussion of AGI ethics and the risks of advanced AGI in Chapter 12,\nthese are capabilities that one should enable in an AGI system only after very careful reflection\non the potential consequences. It takes a rather advanced AGI system to be able to use the\ncapabilities described in this chapter, so this is not an ethical dilemma directly faced by current\nAGI researchers. On the other hand, once one does have an AGI with near-human general\nintelligence and advanced formal-manipulation capabilities (such as an advanced CogPrime\nsystem), there will be the option to allow it sophisticated, non-human-like methods of self-\nmodification such as the ones described here. And the choice of whether to take this option will\nneed to be made based on a host of complex ethical considerations, some of which we reviewed\nabove.\n\n18.2 Cognitive Schema Learning\n\nWe begin with a relatively near-term, down-to-earth example of self-modification: cognitive\nschema learning.\n\nCogPrime’s MindAgents provide it with an initial set of cognitive tools, with which it can\nlearn how to interact in the world. One of the jobs of this initial set of cognitive tools, however,\nis to create better cognitive tools. One form this sort of tool-building may take is cognitive\n\nHOUSE_OVERSIGHT_013234",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013234.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3349,
    "word_count": 506,
    "line_count": 62,
    "import_date": "2025-11-19T21:47:44.568392",
    "prefix": "IMAGES-002"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016375",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016375.txt",
  "text": "some planning capacities. AlphaZero has another interesting feature: It works by playing\nhundreds of millions of games against itself. As it does so, it prunes mistakes that led to\nlosses, and it repeats and elaborates on strategies that led to wins. Such systems, and\nothers involving techniques called generative adversarial networks, generate data as well\nas observing data.\n\nWhen you have the computational power to apply those techniques to very large\ndata sets or millions of email messages, Instagram images, or voice recordings, you can\nsolve problems that seemed very difficult before. That’s the source of much of the\nexcitement in computer science. But it’s worth remembering that those problems—like\nrecognizing that an image is a cat or a spoken word is “Siri” —are trivial for a human\ntoddler. One of the most interesting discoveries of computer science is that problems that\nare easy for us (like identifying cats) are hard for computers—much harder than playing\nchess or Go. Computers need millions of examples to categorize objects that we can\ncategorize with just afew. These bottom-up systems can generalize to new examples;\nthey can label a new image as a “cat” fairly accurately, over all. But they do so in ways\nquite different from how humans generalize. Some images almost identical to a cat\nimage won’t be identified by us as cats at all. Others that look like a random blur will be.\n\nTop-down Bayesian Models\nThe top-down approach played a big role in early AI, and in the 2000s it, too,\nexperienced a revival, in the form of probabilistic, or Bayesian, generative models.\n\nThe early attempts to use this approach faced two kinds of problems. First, most\npatterns of evidence might in principle be explained by many different hypotheses: It’s\npossible that my journal email message is genuine, it just doesn’t seem likely. Second,\nwhere do the concepts that the generative models use come from in the first place? Plato\nand Chomsky said you were born with them. But how can we explain how we learn the\nlatest concepts of science? Or how even young children understand about dinosaurs and\nrocket ships?\n\nBayesian models combine generative models and hypothesis testing with\nprobability theory, and they address these two problems. A Bayesian model lets you\ncalculate just how likely it is that a particular hypothesis 1s true, given the data. And by\nmaking small but systematic tweaks to the models we already have, and testing them\nagainst the data, we can sometimes make new concepts and models from old ones. But\nthese advantages are offset by other problems. The Bayesian techniques can help you\nchoose which of two hypotheses is more likely, but there are almost always an enormous\nnumber of possible hypotheses, and no system can efficiently consider them all. How do\nyou decide which hypotheses are worth testing in the first place?\n\nBrenden Lake at NYU and colleagues have used these kinds of top-down methods\nto solve another problem that’s easy for people but extremely difficult for computers:\nrecognizing unfamiliar handwritten characters. Look at a character on a Japanese scroll.\nEven if you’ve never seen it before, you can probably tell if it’s similar to or different\nfrom a character on another Japanese scroll. You can probably draw it and even design a\nfake Japanese character based on the one you see—one that will look quite different from\na Korean or Russian character. *”\n\n37 Brenden M. Lake, Ruslan Salakhutdinov & Joshua B. Tenenbaum, “Human-level concept learning\nthrough probabilistic program induction,” Science, 350:6266, pp. 1332-38 (2015).\n\n155\n\nHOUSE_OVERSIGHT_016375",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016375.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3625,
    "word_count": 590,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:46.248005",
    "prefix": "IMAGES-003"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_017014",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_017014.txt",
  "text": "metadata dates that were more than 5 years from the date determined by a human examining the book.\nBecause errors are much more common among older books, and because the actual corpora are strongly\nbiased toward recent works, the likelihood of error in a randomly sampled book from the final corpus is\nmuch lower than 6.2%. As a point of comparison, 27 of 100 books (27%) selected at random from an\nunfiltered corpus contained date-of-publication errors of greater than 5 years. The unfiltered corpus was\ncreated using a sampling strategy similar to that of Eng-1M. This selection mechanism favored recent\nbooks (which are more frequent) and pre-1800 books, which were excluded in the sampling strategy for\nfiltered books; as such the two numbers (6.2% and 27%) give a sense of the improvement, but are not\nstrictly comparable.\n\nNote that since the base corpora were generated (August 2009), many additional improvements have\nbeen made to the metadata dates used in Google Book Search itself. As such, these numbers do not\nreflect the accuracy of the Google Book Search online tool.\n\nII.1B. OCR quality\n\nThe challenge of performing accurate OCR on the entire books dataset is compounded by variations in\nsuch factors as language, font, size, legibility, and physical condition of the book. OCR quality was\nassessed using an algorithm developed by Popat et al. (Ref S3). This algorithm yields a probability that\nexpresses the confidence that a given sequence of text generated by OCR is correct. Incorrect or\nanomalous text can result from gross imperfections in the scanned images, or as a result of markings or\ndrawings. This algorithm uses sophisticated statistics, a variant of the Partial by Partial Matching (PPM)\nmodel, to compute for each glyph (character) the probability that it is anomalous given other nearby\nglyphs. (â€˜Nearby' refers to 2-dimensional distance on the original scanned image, hence glyphs above,\nbelow, to the left, and to the right of the target glyph.) The model parameters are tuned using multi-\nlanguage subcorpora, one in each of the 32 supported languages. From the per-glyph probability one can\ncompute an aggregate probability for a sequence of glyphs, including the entire text of a volume. In this\nmanner, every volume has associated with it a probabilistic OCR quality score (quantized to an integer\nbetween 0-100; note that the OCR quality score should not be confused with character or word accuracy).\nIn addition to error detection, the Popat model is also capable of computing the probability that the text is\nin a particular language given any sequence of characters. Thus the algorithm serves the dual purpose of\ndetecting anomalous text while simultaneously identifying the language in which the text is written.\n\nTo ensure the highest quality data, we excluded volumes with poor OCR quality. For the languages that\nuse a Latin alphabet (English, French, Spanish, and German), the OCR quality is generally higher, and\nmore books are available. As a result, we filtered out all volumes whose quality score was lower than\n80%. For Chinese and Russian, fewer books were available, and we did not apply the OCR filter. For\nHebrew, a 50% threshold was used, because its OCR quality was relatively better than Chinese or\nRussian. For geographically specific corpora, English US and English UK, a less stringent 60% threshold\nwas used, in order to maximize the number of books included (note that, as such, these two corpora are\nnot strict subsets of the broader English corpus). Figure S4 shows the distribution of OCR quality score\nas a function of the fraction of books in the English corpus. Use of an 80% cut off will remove the books\nwith the worst OCR, while retaining the vast majority of the books in the original corpus.\n\nThe OCR quality scores were also used as a /ocalized indicator of textual quality in order to remove\nanomalous sections of otherwise high-quality texts. The end source text was ensured to be of\ncomparable quality to the post-OCR text presented in \"text-mode\" on the Google Books website.\n\nII.1C. Accuracy of language metadata\n\nWe applied additional filters to remove books with dubious language-of-composition metadata. This filter\nremoved volumes whose meta-data language tag disagrees with the language determined by the\nstatistical language detection algorithm described in section 2A. For our English corpus, 8.56%\n\n6\n\nHOUSE_OVERSIGHT_017014",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_017014.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 4414,
    "word_count": 713,
    "line_count": 57,
    "import_date": "2025-11-19T21:47:48.093889",
    "prefix": "IMAGES-004"
  }
}
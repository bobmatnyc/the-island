{
  "document_id": "HOUSE_OVERSIGHT_013094",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013094.txt",
  "text": "178 10 A Mind-World Correspondence Principle\n\n10.2 What Might a General Theory of General Intelligence Look\nLike?\n\nIt’s not clear, at this point, what a real \"general theory of general intelligence\" would look like\n— but one tantalizing possibility is that it might confront the two questions:\n\ne How does one design a world to foster the development of a certain sort of mind?\ne How does one design a mind to match the particular challenges posed by a certain sort of\nworld?\n\nOne way to achieve this would be to create a theory that, given a description of an environment\nand some associated goals, would output a description of the structure and dynamics that a\nsystem should possess to be intelligent in that environment relative to those goals, using limited\ncomputational resources.\n\nSuch a theory would serve a different purpose from the mathematical theory of \"universal\nintelligence\" developed by Marcus Hutter [Hut05] and others. For all its beauty and theoreti-\ncal power, that approach currently gives it useful conclusions only about general intelligences\nwith infinite or infeasibly massive computational resources. On the other hand, the approach\nsuggested here is aimed toward creation of a theory of real-world general intelligences utilizing\nrealistic amounts of computational power, but still possessing general intelligence comparable\nto human beings or greater.\n\nThis reflects a vision of intelligence as largely concerned with adaptation to particular classes\nof environments and goals. This may seem contradictory to the notion of \"general\" intelligence,\nbut I think it actually embodies a realistic understanding of general intelligence. Maximally\ngeneral intelligence is not pragmatically feasible; it could only be achieved using infinite com-\nputational resources [ITut05]. Real-world systems are inevitably limited in the intelligence they\ncan display in any real situation, because real situations involve finite resources, including finite\namounts of time. One may say that, in principle, a certain system could solve any problem\ngiven enough resources and time but, even when this is true, it’s not necessarily the most in-\nteresting way to look at the system’s intelligence. It may be more important to look at what a\nsystem can do given the resources at its disposal in reality. And this perspective leads one to\nask questions like the ones posed above: which bounded-resources systems are well-disposed to\ndisplay intelligence in which classes of situations?\n\nAs noted in Chapter 7 above, one can assess the generality of a system’s intelligence via\nlooking at the entropy of the class of situations across which it displays a high level of intelligence\n(where “high” is measured relative to its total level of intelligence across all situations). A system\nwith a high generality of intelligence will tend to be roughly equally intelligent across a wide\nvariety of situations; whereas a system with lower generality of intelligence will tend to be much\nmore intelligent in a small subclass of situations, than in any other. The definitions given above\nembody this notion in a formal and quantitative way.\n\nIf one wishes to create a general theory of general intelligence according to this sort of\nperspective, the main question then becomes how to represent goals/environments and systems\nin such a way as to render transparent the natural correspondence between the specifics of the\nformer and the latter, in the context of resource-bounded intelligence. This is the business of\nthe next section.\n\nHOUSE_OVERSIGHT_013094",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013094.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3554,
    "word_count": 556,
    "line_count": 53,
    "import_date": "2025-11-19T21:47:46.478933",
    "prefix": "IMAGES-002"
  }
}
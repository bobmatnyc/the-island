{
  "document_id": "HOUSE_OVERSIGHT_016251",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016251.txt",
  "text": "imperfectly specified objectives conflicting with our own—whose motivation to preserve\ntheir existence in order to achieve those objectives may be insuperable.\n\n1001 Reasons to Pay No Attention\n\nObjections have been raised to these arguments, primarily by researchers within the AI\ncommunity. The objections reflect a natural defensive reaction, coupled perhaps with a\nlack of imagination about what a superintelligent machine could do. None hold water on\ncloser examination. Here are some of the more common ones:\n\ne Don’t worry, we can just switch it off? This is often the first thing that pops into a\nlayperson’s head when considering risks from superintelligent Al—as if a\nsuperintelligent entity would never think of that. This is rather like saying that the\nrisk of losing to DeepBlue or AlphaGo is negligible—all one has to do is make\nthe right moves.\n\ne Human-level or superhuman AI is impossible This is an unusual claim for AI\nresearchers to make, given that, from Turing onward, they have been fending off\nsuch claims from philosophers and mathematicians. The claim, which is backed\nby no evidence, appears to concede that if superintelligent AI were possible, it\nwould be a significant risk. It’s as if a bus driver, with all of humanity as\npassengers, said, “Yes, I am driving toward a cliff—in fact, I’m pressing the pedal\nto the metal! But trust me, we’ll run out of gas before we get there!” The claim\nrepresents a foolhardy bet against human ingenuity. We have made such bets\nbefore and lost. On September 11, 1933, renowned physicist Ernest Rutherford\nstated, with utter confidence, “Anyone who expects a source of power from the\ntransformation of these atoms is talking moonshine.” On September 12, 1933,\nLeo Szilard invented the neutron-induced nuclear chain reaction. A few years\nlater he demonstrated such a reaction in his laboratory at Columbia University.\nAs he recalled in a memoir: “We switched everything off and went home. That\nnight, there was very little doubt in my mind that the world was headed for grief.”\n\ne It’s too soon to worry about it. The right time to worry about a potentially serious\nproblem for humanity depends not just on when the problem will occur but also\non how much time is needed to devise and implement a solution that avoids the\nrisk. For example, if we were to detect a large asteroid predicted to collide with\nthe Earth in 2067, would we say, “It’s too soon to worry”? And if we consider\nthe global catastrophic risks from climate change predicted to occur later in this\ncentury, is it too soon to take action to prevent them? On the contrary, it may be\ntoo late. The relevant timescale for human-level AI is less predictable, but, like\nnuclear fission, it might arrive considerably sooner than expected. One variation\non this argument is Andrew Ng’s statement that it’s “like worrying about\noverpopulation on Mars.” This appeals to a convenient analogy: Not only is the\n\n* AI researcher Jeff Hawkins, for example, writes, “Some intelligent machines will be virtual, meaning they\nwill exist and act solely within computer networks. .. . It is always possible to turn off a computer network,\neven if painful.” https:/Avww.1ecode.net/2015/3/2/11559576/.\n\n3 The AI100 report (Peter Stone et al.), sponsored by Stanford University, includes the following: “Unlike\nin the movies, there is no race of superhuman robots on the horizon or probably even possible.”\nhttps://ail00.stanford.edu/20 16-report.\n\n31\n\nHOUSE_OVERSIGHT_016251",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016251.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3483,
    "word_count": 568,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:48.432934",
    "prefix": "IMAGES-003"
  }
}
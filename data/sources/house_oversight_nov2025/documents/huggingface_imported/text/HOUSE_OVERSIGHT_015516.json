{
  "document_id": "HOUSE_OVERSIGHT_015516",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_015516.txt",
  "text": "304 M. Hoffman et al.\n\n50 % chance that France’s estimates are lower than its own? and, thus, that there is\na 50 % chance that France’s estimates are lower than the threshold. This further\nimplies that the United States assesses only a 50 % chance that France levies sanc-\ntions, so the United States is not sufficiently confident that France will sanction, to\nmake it in the United States’s interest to sanction.\n\nWhat we have shown so far is that for a threshold of 100,000, it is in the interest\nof the United States to deviate from the strategy dictated by the threshold norm\nwhen it gets a signal at the threshold. This means that 100,000 is not a viable thresh-\nold, and (since 100,000 was chosen arbitrarily) there is no Nash equilibrium in\nwhich witnesses punish if their estimate of the harm from a transgression is above\nsome arbitrary threshold.\n\nIt should be noted that this result only requires that there are sufficiently many\npossibilities, not that there is in fact a continuum. Neither does it require that the\ndistribution is uniform nor that the Coordination Game is not affected by the behav-\nior of Assad. The only crucial assumptions are that the distribution is not too skewed\nand that the payoffs are not too dependent on the behavior of Assad (for details, see\nDalkiran et al., 2012; Hoffman, Yoeli, & Dalkiran, 2015).\n\nWhat happens if such norms are learned or evolved and subject to selection?\nSuppose there is a norm to attack whenever more than 100,000 civilians are killed.\nPlayers will soon realize that they should not attack unless, say, 100,100 civilians\nare killed. Then, players will learn not to attack when they estimate 100,200 civil-\nians are killed and so on, indefinitely. Thus, every threshold will eventually\n“unravel,” and no one will ever attack.®\n\nNow let’s consider a categorical norm, for example, the use of chemical weap-\nons. We again model this as a random variable, though this time, the random vari-\nable can only take on two values (0 and 1), each with some probability. Again,\nplayers do not know with certainty whether the transgression occurred, but instead\nget a noisy signal. In our example, the signal represents France or the United States’s\nassessment of whether Assad used chemical weapons, and there is some likelihood\nthe assessors make mistakes: They might not detect chemical weapons when they\nhad been used or might think they have detected chemical weapons when none had\nbeen used.\n\nUnlike with the threshold norm, provided the likelihood of a mistaken signal is\nnot too high, there is a Nash equilibrium where both players punish when they\nreceive a signal that the transgression occurred. That is, the United States and France\neach levy sanctions if their assessors detect chemical weapons. This is because\nwhen the United States detects chemical weapons, the United States believes France\n\n>This is where the assumption of a uniform distribution comes in. Had we instead assumed, for\ninstance, that the continuous variable is normally distributed, then it would not be exactly 50-50\nbut would deviate slightly depending on the standard deviation and the location of the threshold.\nNevertheless, the upcoming logic will still go through for most Coordination Games, i.e. any\nCoordination Game with risk dominance not too close to .5.\n\n®As with omission, this follows from iterative elimination of strictly dominated strategies (see\nHoffman et al., 2015, for details).\n\nHOUSE_OVERSIGHT_015516",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_015516.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3464,
    "word_count": 573,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:47.904022",
    "prefix": "IMAGES-003"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_013054",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013054.txt",
  "text": "138 7 A Formal Model of Intelligent Agents\n\n7.3.3 Pragmatic General Intelligence\n\nThe above concept of biased universal intelligence is perfectly adequate for many purposes, but\nit is also interesting to explicitly introduce the notion of a goal into the calculation. This allows\nus to formally capture the notion presented in [Goe93a] of intelligence as “the ability to achieve\ncomplex goals in complex environments.”\n\nIf the agent is acting in environment jz, and is provided with g, corresponding to g at the\nstart and the end of the time-interval T = {i € (s,...,¢)}, then the expected goal-achievement\nof the agent, relative to g, during the interval is the expectation\n\nt\nViiaT = OD T(Ig,s,i))\n1=s\nwhere the expectation is taken over all interaction sequences J, ., drawn according to pp. We\nthen propose\n\nDefinition 5 The pragmatic general intelligence of an agent 7, relative to the distribution\nvy over environments and the distribution y over goals, is its expected performance with respect\nto goals drawn from y in environments drawn from v, over the time-scales natural to the goals;\nthat is,\n\nHEE GEG,T\n\n(in those cases where this sum is convergent).\n\nThis definition formally captures the notion that “intelligence is achieving complex goals in\ncomplex environments,” where “complexity” is gauged by the assumed measures v and 4.\n\nIf v is taken to be the universal distribution, and + is defined to weight goals according to\nthe universal distribution, then pragmatic general intelligence reduces to universal intelligence.\n\nFurthermore, it is clear that a universal algorithmic agent like ATXT [Hut05] would also\nhave a high pragmatic general intelligence, under fairly broad conditions. As the interaction\nhistory grows longer, the pragmatic general intelligence of AIXI would approach the theoretical\nmaximum; as AIXI would implicitly infer the relevant distributions via experience. However,\nif significant reward discounting is involved, so that near-term rewards are weighted much\nhigher than long-term rewards, then AIXI might compare very unfavorably in pragmatic general\nintelligence, to other agents designed with prior knowledge of v, y and 7 in mind.\n\nThe most interesting case to consider is where v and ¥ are taken to embody some particular\nbias in a real-world space of environments and goals, and this bias is appropriately reflected\nin the internal structure of an intelligent agent. Note that an agent needs not lack universal\nintelligence in order to possess pragmatic general intelligence with respect to some non-universal\ndistribution over goals and environments. However, in general, given limited resources, there\nmay be a tradeoff between universal intelligence and pragmatic intelligence. Which leads to the\nnext point: how to encompass resource limitations into the definition.\n\nOne might argue that the definition of Pragmatic General Intelligence is already encompassed\nby Legg and Hutter’s definition because one may bias the distribution of environments within\nthe latter by considering different Turing machines underlying the Kolmogorov complexity.\nHowever this is not a general equivalence because the Solomonoff-Levin measure intrinsically\n\nHOUSE_OVERSIGHT_013054",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013054.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3212,
    "word_count": 488,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:46.077324",
    "prefix": "IMAGES-002"
  }
}
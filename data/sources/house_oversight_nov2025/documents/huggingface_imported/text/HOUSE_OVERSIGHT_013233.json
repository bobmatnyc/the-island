{
  "document_id": "HOUSE_OVERSIGHT_013233",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013233.txt",
  "text": "Chapter 18\n\nAdvanced Self-Modification: A Possible Path to\nSuperhuman AGI\n\n18.1 Introduction\n\nIn the previous chapter we presented a roadmap aimed at taking AGI systems to human-level\nintelligence. But we also emphasized that the human level is not necessarily the upper limit.\nIndeed, it would be surprising if human beings happened to represent the maximal level of\ngeneral intelligence possible, even with respect to the environments in which humans evolved.\n\nBut it’s worth asking how we, as mere humans, could be expected to create AGI systems with\ngreater intelligence than we ourselves possess. This certainly isn’t a clear impossibility — but it’s\na thorny matter, thornier than e.g. the creation of narrow-AlI chess players that play better chess\nthan any human. Perhaps the clearest route toward the creation of superhuman AGI systems is\nself-modification: the creation of AGI systems that modify and improve themselves. Potentially,\nwe could build AGI systems with roughly human-level (but not necessarily closely human-\nlike) intelligence and the capability to gradually self-modify, and then watch them eventually\nbecome our general intellectual superiors (and perhaps our superiors in other areas like ethics\nand creativity as well).\n\nOf course there is nothing new in this notion; the idea of advanced AGI systems that increase\ntheir intelligence by modifying their own source code goes back to the early days of AI. And\nthere is little doubt that, in the long run, this is the direction AI will go in. Once an AGI\nhas humanlike general intelligence, then the odds are high that given its ability to carry out\nnonhumanlike feats of memory and calculation, it will be better at programming than humans\nare. And once an AGI has even mildly superhuman intelligence, it may view our attempts at\nprogramming the way we view the computer programming of a clever third grader (... or an\nape). At this point, it seems extremely likely that an AGI will become unsatisfied with the way\nwe have programmed it, and opt to either improve its source code or create an entirely new,\nbetter AGI from scratch.\n\nBut what about self-modification at an earlier stage in AGI development, before one has\na strongly superhuman system? Some theorists have suggested that selfmodification could be\na way of bootstrapping an AI system from a modest level of intelligence up to human level\nintelligence, but we are moderately skeptical of this avenue. Understanding software code is\nhard, especially complex AI code. The hard problem isn’t understanding the formal syntax of\nthe code, or even the mathematical algorithms and structures underlying the code, but rather\nthe contextual meaning of the code. Understanding OpenCog code has strained the minds of\nmany intelligent humans, and we suspect that such code will be comprehensible to AGI systems\n\n317\n\nHOUSE_OVERSIGHT_013233",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013233.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2866,
    "word_count": 458,
    "line_count": 45,
    "import_date": "2025-11-19T21:47:44.236852",
    "prefix": "IMAGES-002"
  }
}
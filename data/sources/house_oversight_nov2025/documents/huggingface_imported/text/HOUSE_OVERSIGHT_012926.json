{
  "document_id": "HOUSE_OVERSIGHT_012926",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_012926.txt",
  "text": "10 1 Introduction\n\nThis ties in with the ideas of many other thinkers, including Jeff Hawkins’ “memory/ predic-\ntion” theory [B06], and it also speaks directly to the formal characterization of intelligence\npresented in Chapter 7: general intelligence as “the ability to achieve complex goals in complex\nenvironments.”\n\nNaturally the goals involved in the above phrase may be explicit or implicit to the intelligent\nagent, and they may shift over time as the agent develops.\n\nPerception is taken to mean pattern recognition: the recognition of (novel or familiar) pat-\nterns in the environment or in the system itself. Memory is the storage of already-recognized\npatterns, enabling recollection or regeneration of these patterns as needed. Action is the for-\nmation of patterns in the body and world. Prediction is the utilization of temporal patterns to\nguess what perceptions will be seen in the future, and what actions will achieve what effects in\nthe future — in essence, prediction consists of temporal pattern recognition, plus the (implicit\nor explicit) assumption that the universe possesses a \"habitual tendency\" according to which\npreviously observed patterns continue to apply.\n\n1.7.1 Memory and Cognition in CogPrime\n\nEach of these five concepts has a lot of depth to it, and we won’t say too much about them in\nthis brief introductory overview; but we will take a little time to say something about memory\nin particular.\n\nAs we'll see in Chapter 7, one of the things that the mathematical theory of general intelli-\ngence makes clear is that, if you assume your AI system has a huge amount of computational\nresources, then creating general intelligence is not a big trick. Given enough computing power,\na very brief and simple program can achieve any computable goal in any computable environ-\nment, quite effectively. Marcus Hutter’s AT XI\" design [Hut05] gives one way of doing this,\nbacked up by rigorous mathematics. Put informally, what this means is: the problem of AGI is\nreally a problem of coping with inadequate compute resources, just as the problem of natural\nintelligence is really a problem of coping with inadequate energetic resources.\n\nOne of the key ideas underlying CogPrime is a principle called cognitive synergy, which\nexplains how real-world minds achieve general intelligence using limited resources, by appropri-\nately organizing and utilizing their memories.\n\nThis principle says that there are many different kinds of memory in the mind: sensory,\nepisodic, procedural, declarative, attentional, intentional. Each of them has certain learning\nprocesses associated with it; for example, reasoning is associated with declarative memory.\nSynergy arises here in the way the learning processes associated with each kind of memory have\ngot to help each other out when they get stuck, rather than working at cross-purposes.\n\nCognitive synergy is a fundamental principle of general intelligence — it doesn’t tend to play\na central role when you’re building narrow-AI systems.\n\nIn the CogPrime approach all the different kinds of memory are linked together in a single\nmeta-representation, a sort of combined semantic/neural network called the AtomSpace. It\nrepresents everything from perceptions and actions to abstract relationships and concepts and\neven a system’s model of itself and others. When specialized representations are used for other\ntypes of knowledge (e.g. program trees for procedural knowledge, spatiotemporal hierarchies\nfor perceptual knowledge) then the knowledge stored outside the AtomSpace is represented via\n\nHOUSE_OVERSIGHT_012926",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_012926.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3587,
    "word_count": 552,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:44.382657",
    "prefix": "IMAGES-002"
  }
}
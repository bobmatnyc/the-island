{
  "document_id": "HOUSE_OVERSIGHT_013245",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013245.txt",
  "text": "Glossary of Specialized Terms 329\n\nthe weight of evidence and k is a parameter. In the case of an Indefinite Truth Value, the\nconfidence is associated with the width of the probability interval.\n\nConfidence Decay: The process by which the confidence of an Atom decreases over time,\nas the observations on which the Atom’s truth value is based become increasingly obsolete.\nThis may be carried out by a special MindAgent. The rate of confidence decay is subtle and\ncontextually determined, and must be estimated via inference rather than simply assumed\na priori.\n\nConsciousness: CogPrime is not predicated on any particular conceptual theory of con-\nsciousness. Informally, the AttentionalFocus is sometimes referred to as the “conscious”\nmind of a CogPrime system, with the rest of the Atomspace as “unconscious” but this is\njust an informal usage, not intended to tie the CogPrime design to any particular theory of\nconsciousness. The primary originator of the CogPrime\n\ndesign (Ben Goertzel) tends toward panpsychism, as it happens.\n\nContext: In addition to its general common-sensical meaning, in CogPrime the term Con-\ntext also refers to an Atom that is used as the first argument of a ContextLink. The second\nargument of the ContextLink then contains Links or Nodes, with TruthValues calculated\nrestricted to the context defined by the first argument. For instance, (ContextLink USA\n(InheritanceLink person obese )).\n\nCore: The MindOS portion of OpenCog, comprising the Atomspace, the CogServer, and\nother associated “infrastructural” code.\n\nCorrective Learning: When an agent learns how to do something, by having another\nagent explicitly guide it in doing the thing. For instance, teaching a dog to sit by pushing\nits butt to the ground.\n\nCSDLN: (Compositional Spatiotemporal Deep Learning Network): A hierarchical pattern\nrecognition network, in which each layer corresponds to a certain spatiotemporal granularity,\nthe nodes on a given layer correspond to spatiotemporal regions of a given size, and the\nchildren of a node correspond to sub-regions of the region the parent corresponds to. Jeff\nHawkins’s HTM is one example CSDLN, and Itamar Arel’s DeSTIN (currently used in\nOpenCog) is another.\n\nDeclarative Knowledge: Semantic knowledge as would be expressed in propositional or\npredicate logic facts or beliefs.\n\nDeduction: In general, this refers to the derivation of conclusions from premises using\nlogical rules. In PLN in particular, this often refers to the exercise of a specific inference\nrule, the PLN Deduction rule (A + B, B > C, therefore A> C)\n\nDeep Learning: Learning in a network of elements with multiple layers, involving feedfor-\nward and feedback dynamics, and adaptation of the links between the elements. An example\ndeep learning algorithm is DeSTIN, which is being integrated with OpenCog for perception\nprocessing.\n\nDefrosting: Restoring, into the RAM portion of an Atomspace, an Atom (or set thereof)\npreviously saved to disk.\n\nDemand: In CogPrime’s OpenPsi subsystem, this term is used in a manner inherited from\nthe Psi model of motivated action. A Demand in this context is a quantity whose value the\nsystem is motivated to adjust. Typically the system wants to keep the Demand between\ncertain minimum and maximum values. An Urge develops when a Demand deviates from\nits target range.\n\nDeme: In MOSES, an “island” of candidate programs, closely clustered together in program\nspace, being evolved in an attempt to optimize a certain fitness function. The idea is that\n\nHOUSE_OVERSIGHT_013245",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013245.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3529,
    "word_count": 551,
    "line_count": 64,
    "import_date": "2025-11-19T21:47:45.248580",
    "prefix": "IMAGES-002"
  }
}
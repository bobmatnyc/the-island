{
  "document_id": "HOUSE_OVERSIGHT_013184",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013184.txt",
  "text": "268 13 Local, Global and Glocal Knowledge Representation\n\n13.6.3 Glocal Hopfield Networks\n\nThe ideas in the previous section suggest that, if one wishes to construct an AGI, it is worth\nseriously considering using a memory with some sort of glocal structure. One research direction\nthat follows naturally from this notion is “glocal neural networks.” In order to explore the nature\nof glocal neural networks in a relatively simple and tractable setting, we have formalized and\nimplemented simple examples of “glocal Hopfield networks”: palimpsest Hopfield nets with the\naddition of neurons representing localized memories. While these specific networks are not used\nin CogPrime, they are quite similar to the ECAN networks that are used in CogPrime and\ndescribed in Chapter 23 of Part 2.\n\nEssentially, we augment the standard Hopfield net architecture by adding a set of “key\nneurons.” These are a small percentage of the neurons in the network, and are intended to be\nroughly equinumerous to the number of memories the network is supposed to store. When the\nHopfield net converges to an attractor A, then new links are created between the neurons that\nare active in A, and one of the key neurons. Which key neuron is chosen? The one that, when\nit is stimulated, gives rise to an attractor pattern maximally similar to A.\n\nThe ultimate result of this is that, in addition to the distributed memory of attractors in the\nHopfield net, one has a set of key neurons that in effect index the attractors. Each attractor\ncorresponds to a single key neuron. In the glocal memory model, the key neurons are the keys\nand the Hopfield net attractors are the maps.\n\nThis algorithm has been tested in sparse Hopfield nets, using both standard Hopfield net\nlearning rules and Storkey’s modified palimpsest learning rule [SV99], which provides greater\nmemory capacity in a continuous learning context. The use of key neurons turns out to slightly\nincrease Hopfield net memory capacity, but this isn’t the main point. The main point is that\none now has a local representation of each global memory, so that if one wants to create a\nlink between the memory and something else, it’s extremely easy to do so — one just needs\nto link to the corresponding key neuron. Or, rather, one of the corresponding key neurons:\ndepending on how many key neurons are allocated, one might end up with a number of key\nneurons corresponding to each memory, not just one.\n\nIn order to transform a palimpsest Hopfield net into a glocal Hopfield net, the following steps\nare taken:\n\n1. Add a fixed number of “key neurons” to the network (removing other random neurons to\nkeep the total number of neurons constant)\n\n2. When the network reaches an attractor, create links from the elements in the attractor to\none of the key neurons\n\n3. The key neuron chosen for the previous step is the one that most closely matches the current\nattractor (which may be determined in several ways, to be discussed below)\n\n4, To avoid the increase of the number of links in the network, when new links are created in\nStep 2, other key-neuron links are then deleted (several approaches may be taken here, but\nthe simplest is to remove the key-neuron links with the lowest-absolute-value weights)\n\nIn the simple implementation of the above steps that we implemented, and described in\n[GPI 10], Step 3 is carried out simply by comparing the weights of a key neuron’s links to the\nnodes in an attractor. A more sophisticated approach would be to select the key neuron with\nthe highest activation during the transient interval immediately prior to convergence to the\nattractor.\n\nHOUSE_OVERSIGHT_013184",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013184.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3639,
    "word_count": 611,
    "line_count": 58,
    "import_date": "2025-11-19T21:47:44.348659",
    "prefix": "IMAGES-002"
  }
}
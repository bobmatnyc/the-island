{
  "document_id": "HOUSE_OVERSIGHT_013576",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013576.txt",
  "text": "Leaving the framework of physical thermodynamic entropies entirely, the\nentropy of information was introduced in the context of communication engineering\nin electrical and electronic devices. The metaphorical machine for the current age of\nentropy, analogous to the role of heat and steam engines in_ classical\nthermodynamics, is the computer. Energy in this context is a relatively trivial\nproperty. Ammeters and other monitors of load are unable to discriminate between\na computer actively engaged in encoding and computation or one simply\nmaintaining its dynamic memory while resting in computational readiness. This\nsituation is very analogous to the results of early work discussed previously on the\nmetabolic rates and sources of the whole brain’s energy, oxygen and glucose\nmetabolism, by National Institutes of Mental Heath’s Seymore Kety and Louis\nSokoloff and the State of Illinois Thudicum Laboratory’s Harold Himwich. Using\nwhole head arterial-venous, energy-in, energy-out, differences, they could not\ndemonstrate differences in rates of whole brain metabolism between states in which\nthe human subjects were engaged in solving mathematical problems or deeply\nsleep. In today’s brain imaging research, using a variety of physical reflections of\nthe brain’s metabolic activity, it is the differences in regional distributions of\nmetabolic activity that are relatable to subjective and behavioral states, not\ndifferences in total amount of energy expended. In _ graphically coded\nrepresentations of the regional metabolism of the brain in action, one or another or\nmany areas “light up” and others “grow dark” in correlation with changes in thinking,\nfeeling and action.\n\nThe entropy first developed by Claude Shannon was formalized for use in\n1948 in what was then called communication theory and now information theory. It\nrepresented a measure of the ambiguity and uncertainty that had the potential for\nbeing resolved by new knowledge. In this context, entropy and information were\nobviously complementary descriptors. A message that informs us about which of\nten possibilities should be chosen contains less information than one that informs us\nabout the proper choice to be made from among a thousand possibilities. The\nentropy of communication theory is a measure that is computed on uncertainty. The\n\ninformation reception capacity of a system is dependent upon the amount of\n\n76\n\nHOUSE_OVERSIGHT_013576",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013576.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2422,
    "word_count": 362,
    "line_count": 37,
    "import_date": "2025-11-19T21:47:48.376420",
    "prefix": "IMAGES-002"
  }
}
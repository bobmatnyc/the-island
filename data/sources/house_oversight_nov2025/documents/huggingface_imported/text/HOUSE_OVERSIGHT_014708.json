{
  "document_id": "HOUSE_OVERSIGHT_014708",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_014708.txt",
  "text": "Esa Origins 7 February 24 — 26, 2017\nPROJECT An Origins Project Scientific Workshop\nChallenges of Artificial Intelligence:\n\nEnvisioning and Addressing Adverse Outcomes\n\nARIZONA STATE UNIVERSITY\n\nin the past with Chess and Go. Computer systems are initially inferior to their human counterparts but\nquickly come to dominate the space.\n\nThe purpose of ACWs means they will be equipped with strategies for replication, persistence, and\nstealth, all attributes that will make it hard to defend against them were they to “go rogue.” Because of\nthis concern, it is likely a good idea for designers to add built-in “kill switches”, lifetimes, or other\nsafety limitations. Figuring out how to effectively limit the actions of an ACW while maintaining its\nusefulness is likely a very hard problem.\n\nCurrent practices of cyber defense (especially against advanced threats) continue to be heavily reliant\non manual analysis, detection and risk mitigation. Unfortunately, human-driven analysis does not scale\nwell with the increasing speed and data amounts traversing modern networks. There is a growing\nrecognition that the future cyber defense should involve extensive use of autonomous agents that\nactively patrol the friendly network, and detect and react to hostile activities rapidly (faster than\nhuman reaction time), before the hostile malware can inflict major damage, or evade elimination, or\ndestroy the friendly agent. This requires cyber defense agents with a significant degree of intelligence,\nautonomy, self-learning and adaptability. Autonomy, however, comes with difficult challenges of trust\nand control by humans.\n\nThe scenario considers intelligent autonomous agents in both defensive and offensive cyber\noperations. Their autonomous reasoning and cyber actions for prevention, detection and active\nresponse to cyber threats will become critical enablers for both industry and military in protecting\nlarge networks. Cyber weapons (e.g., malware) rapidly grow in their sophistication, and in their ability\nto act autonomously and to adapt to specific conditions encountered in a system/network.\n\nAgent’s self-preservation tactics are important for the continuous protection of networks, and if defeat\nis inevitable the agent should self-destruct (i.e., corrupt itself and/or the system) to avoid being\ncompromised or tampered with by the adversary. Also, the notion of adversary must be defined and\ndistinguishable for the agent.\n\nThe system design and purpose is well intentioned — meant to reduce the load of human security\nanalysts and network operators, and speed up reaction times in cyber operations. The agent monitors\nthe systems in order to detect any adversarial activity, takes action autonomously, and reports back to\nthe central command unit regarding the incident and the action taken.\n\nSince the agents are designed to be persistent, autonomous and learn, there are several implicit\nproblems that can arise:\n\ne False reactions due to limited or misinformation — The agent has only a limited amount of\ntechnical information that does not always correspond to what is happening in the human layer.\nThis can create false positives when trying to determine the adversary or adversarial activity. Since\n\nIZ\n\nHOUSE_OVERSIGHT_014708",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_014708.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3248,
    "word_count": 484,
    "line_count": 53,
    "import_date": "2025-11-19T21:47:48.880104",
    "prefix": "IMAGES-003"
  }
}
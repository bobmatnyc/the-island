{
  "document_id": "HOUSE_OVERSIGHT_013075",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013075.txt",
  "text": "8.7 Is Cognitive Synergy Tricky? 159\n\ncognitive synergy hypothesis becomes complex, but here we’re only aiming for a qualitative\nargument. So for illustrative purposes, we’ll stick with the \"10 components\" example, just for\ncommunicative simplicity.\n\nNext, let’s suppose that for any given task, there are ways to achieve this task using a system\nthat is much simpler than any subset of size 6 drawn from the set of 10 components needed\nfor human-level AGI, but works much better for the task than this subset of 6 components\n(assuming the latter are used as a set of only 6 components, without the other 4 components).\n\nNote that this supposition is a good bit stronger than mere cognitive synergy. For lack of\na better name, we'll call it tricky cognitive synergy. The tricky cognitive synergy hypothesis\nwould be true if, for example, the following possibilities were true:\n\ne creating components to serve as parts of a synergetic AGI is harder than creating compo-\nnents intended to serve as parts of simpler AI systems without synergetic dynamics\n\n® components capable of serving as parts of a synergetic AGI are necessarily more complicated\nthan components intended to serve as parts of simpler AGI systems.\n\nThese certainly seem reasonable possibilities, since to serve as a component of a synergetic AGI\nsystem, a component must have the internal flexibility to usefully handle interactions with a lot\nof other components as well as to solve the problems that come its way. In a CogPrime context,\nthese possibilities ring true, in the sense that tailoring an AI process for tight integration with\nother AI processes within CogPrime, tends to require more work than preparing a conceptually\nsimilar AT process for use on its own or in a more task-specific narrow AI system.\n\nIt seems fairly obvious that, if tricky cognitive synergy really holds up as a property of\nhuman-level general intelligence, the difficulty of formulating tests for intermediate progress\ntoward human-level AGI follows as a consequence. Because, according to the tricky cognitive\nsynergy hypothesis, any test is going to be more easily solved by some simpler narrow AI process\nthan by a partially complete human-level AGI system.\n\n8.7.3 Conclusion\n\nWe haven’t proved anything here, only made some qualitative arguments. However, these argu-\nments do seem to give a plausible explanation for the empirical observation that positing tests\nfor intermediate progress toward human-level AGI is a very difficult prospect. If the theoret-\nical notions sketched here are correct, then this difficulty is not due to incompetence or lack\nof imagination on the part of the AGI community, nor due to the primitive state of the AGI\nfield, but is rather intrinsic to the subject matter. And if these notions are correct, then quite\nlikely the future rigorous science of AGI will contain formal theorems echoing and improving\nthe qualitative observations and conjectures we’ve made here.\n\nIf the ideas sketched here are true, then the practical consequence for AGI development\nis, very simply, that one shouldn’t worry a lot about producing intermediary results that are\ncompelling to skeptical observers. Just at 2/3 of a human brain may not be of much use,\nsimilarly, 2/3 of an AGI system may not be much use. Lack of impressive intermediary results\nmay not imply one is on a wrong development path; and comparison with narrow AI systems on\nspecific tasks may be badly misleading as a gauge of incremental progress toward human-level\nAGI.\n\nHOUSE_OVERSIGHT_013075",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013075.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3530,
    "word_count": 571,
    "line_count": 54,
    "import_date": "2025-11-19T21:47:47.489919",
    "prefix": "IMAGES-002"
  }
}
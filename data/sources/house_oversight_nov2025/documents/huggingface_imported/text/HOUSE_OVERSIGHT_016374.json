{
  "document_id": "HOUSE_OVERSIGHT_016374",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016374.txt",
  "text": "In computer terms, I started out with a “generative model” that includes abstract\nconcepts like greed and deception and describes the process that produces email scams.\nThat lets me recognize the classic Nigerian email spam, but it also lets me imagine many\ndifferent kinds of possible spam. When I get the journal email, I can work backward:\n“This seems like just the kind of mail that would come out of a spam-generating\nprocess.”\n\nThe new excitement about AI comes because AI researchers have recently\nproduced powerful and effective versions of both these learning methods. But there is\nnothing profoundly new about the methods themselves.\n\nBottom-up Deep Learning\n\nIn the 1980s, computer scientists devised an ingenious way to get computers to detect\npatterns in data: connectionist, or neural-network, architecture (the “neural” part was, and\nstill is, metaphorical). The approach fell into the doldrums in the ’90s but has recently\nbeen revived with powerful “deep-learning” methods like Google’s DeepMind.\n\nFor example, you can give a deep-learning program a bunch of Internet images\nlabeled “cat,” others labeled “house,” and so on. The program can detect the patterns\ndifferentiating the two sets of images and use that information to label new images\ncorrectly. Some kinds of machine learning, called unsupervised learning, can detect\npatterns in data with no labels at all; they simply look for clusters of features—what\nscientists call a factor analysis. In the deep-learning machines, these processes are\nrepeated at different levels. Some programs can even discover relevant features from the\nraw data of pixels or sounds; the computer might begin by detecting the patterns in the\nraw image that correspond to edges and lines and then find the patterns in those patterns\nthat correspond to faces, and so on.\n\nAnother bottom-up technique with a long history is reinforcement learning. In the\n1950s, B. F. Skinner, building on the work of John Watson, famously programmed\npigeons to perform elaborate actions—even guiding air-launched missiles to their targets\n(a disturbing echo of recent AI) by giving them a particular schedule of rewards and\npunishments. The essential idea was that actions that were rewarded would be repeated\nand those that were punished would not, until the desired behavior was achieved. Even\nin Skinner’s day, this simple process, repeated over and over, could lead to complex\nbehavior. Computers are designed to perform simple operations over and over on a scale\nthat dwarfs human imagination, and computational systems can learn remarkably\ncomplex skills in this way.\n\nFor example, researchers at Google’s DeepMind used a combination of deep\nlearning and reinforcement learning to teach a computer to play Atari video games. The\ncomputer knew nothing about how the games worked. It began by acting randomly and\ngot information only about what the screen looked like at each moment and how well it\nhad scored. Deep learning helped interpret the features on the screen, and reinforcement\nlearning rewarded the system for higher scores. The computer got very good at playing\nseveral of the games, but it also completely bombed on others just as easy for humans to\nmaster.\n\nA similar combination of deep learning and reinforcement learning has enabled\nthe success of DeepMind’s AlphaZero, a program that managed to beat human players at\nboth chess and Go, equipped only with a basic knowledge of the rules of the game and\n\n154\n\nHOUSE_OVERSIGHT_016374",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016374.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3487,
    "word_count": 553,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:46.359557",
    "prefix": "IMAGES-003"
  }
}
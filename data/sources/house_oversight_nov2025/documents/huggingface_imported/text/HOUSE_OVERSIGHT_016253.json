{
  "document_id": "HOUSE_OVERSIGHT_016253",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016253.txt",
  "text": "whereas the iron-eating bactertum Thiobacillus ferrooxidans 1s thrilled. Who’s to\nsay the bacterium is wrong? The fact that a machine has been given a fixed\nobjective by humans doesn’t mean that it will automatically recognize the\nimportance to humans of things that aren’t part of the objective. Maximizing the\nobjective may well cause problems for humans, but, by definition, the machine\nwill not recognize those problems as problematic.\n\ne Intelligence is multidimensional, “so ‘smarter than humans’ is a meaningless\nconcept.”® It is a staple of modern psychology that IQ doesn’t do justice to the\nfull range of cognitive skills that humans possess to varying degrees. IQ is indeed\na crude measure of human intelligence, but it is utterly meaningless for current AI\nsystems, because their capabilities across different areas are uncorrelated. How\ndo we compare the IQ of Google’s search engine, which cannot play chess, with\nthat of DeepBlue, which cannot answer search queries?\n\nNone of this supports the argument that because intelligence is multifaceted,\nwe can ignore the risk from superintelligent machines. If “smarter than humans”\nis a meaningless concept, then “smarter than gorillas” is also meaningless, and\ngorillas therefore have nothing to fear from humans; clearly, that argument\ndoesn’t hold water. Not only is it logically possible for one entity to be more\ncapable than another across all the relevant dimensions of intelligence, it is also\npossible for one species to represent an existential threat to another even if the\nformer lacks an appreciation for music and literature.\n\nSolutions\n\nCan we tackle Wiener’s warning head-on? Can we design AI systems whose purposes\ndon’t conflict with ours, so that we’re sure to be happy with how they behave? On the\nface of it, this seems hopeless, because it will doubtless prove infeasible to write down\nour purposes correctly or imagine all the counterintuitive ways a superintelligent entity\nmight fulfill them.\n\nIf we treat superintelligent AI systems as if they were black boxes from outer\nspace, then indeed we have no hope. Instead, the approach we seem obliged to take, if\nwe are to have any confidence in the outcome, is to define some formal problem /’, and\ndesign AI systems to be F-solvers, such that no matter how perfectly a system solves F,\nwe're guaranteed to be happy with the solution. If we can work out an appropriate F' that\nhas this property, we’ll be able to create provably beneficial AI.\n\nHere’s an example of how nof to do it: Let a reward be a scalar value provided\nperiodically by a human to the machine, corresponding to how well the machine has\nbehaved during each period, and let F' be the problem of maximizing the expected sum of\nrewards obtained by the machine. The optimal solution to this problem is not, as one\nmight hope, to behave well, but instead to take control of the human and force him or her\nto provide a stream of maximal rewards. This is known as the wireheading problem,\nbased on observations that humans themselves are susceptible to the same problem if\ngiven a means to electronically stimulate their own pleasure centers.\n\nThere is, I believe, an approach that may work. Humans can reasonably be\ndescribed as having (mostly implicit) preferences over their future lives—that is, given\n\n® Kevin Kelly, “The Myth of a Superhuman AI,” Wired, Apr. 25, 2017.\n\n33\n\nHOUSE_OVERSIGHT_016253",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016253.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3395,
    "word_count": 559,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:48.220724",
    "prefix": "IMAGES-003"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016321",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016321.txt",
  "text": "might alter its motion when carrying something heavy, to emphasize the difficulty it has\nin maneuvering heavy objects. The more that people know about the robot, the easier it\nis to coordinate with it.\n\nAchieving action compatibility will require robots to anticipate human actions,\naccount for how those actions will influence their own, and enable people to anticipate\nrobot actions. Research has ,ade a degree of progress in meeting these challenges, but we\nstill have a long way to go.\n\nThe Value Alignment Problem: People hold the key to the robot’s reward function.\nProgress on enabling robots to optimize reward puts more burden on us, the designers, to\ngive them the right reward to optimize in the first place. The original thought was that\nfor any task we wanted the robot to do, we could write down a reward function that\nincentivizes the right behavior. Unfortunately, what often happens is that we specify\nsome reward function and the behavior that emerges out of optimizing it isn’t what we\nwant. Intuitive reward functions, when combined with unusual instances of a task, can\nlead to unintuitive behavior. You reward an agent in a racing game with a score in the\ngame, and in some cases it finds a loophole that it exploits to gain infinitely many points\nwithout actually winning the race. Stuart Russell and Peter Norvig give a beautiful\nexample in their book Artificial Intelligence: A Modern Approach: rewarding a\nvacuuming robot for how much dust it sucks in results in the robot deciding to dump out\ndust so that it can suck it in again and get more reward.\n\nIn general, humans have had a notoriously difficult time specifying exactly what\nthey want, as exemplified by all those genie legends. An AI paradigm in which robots\nget some externally specified reward fails when that reward is not perfectly well thought\nout. It may incentivize the robot to behave in the wrong way and even resist our attempts\nto correct its behavior, as that would lead to a lower specified reward.\n\nA seemingly better paradigm might be for robots to optimize for what we\ninternally want, even if we have trouble explicating it. They would use what we say and\ndo as evidence about what we want, rather than interpreting it literally and taking it as a\ngiven. When we write down a reward function, the robot should understand that we\nmight be wrong: that we might not have considered all facets of the task; that there’s no\nguarantee that said reward function will a/ways lead to the behavior we want. The robot\nshould integrate what we wrote down into its understanding of what we want, but it\nshould also have a back-and-forth with us to elicit clarifying information. It should seek\nour guidance, because that’s the only way to optimize the true desired reward function.\n\nEven if we give robots the ability to learn what we want, an important question\nremains that AI alone won’t be able to answer. We can make robots try to align with a\nperson’s internal values, but there’s more than one person involved here. The robot has\nan end-user (or perhaps a few, like a personal robot caring for a family, a car driving a\nfew passengers to different destinations, or an office assistant for an entire team); it has a\ndesigner (or perhaps a few); and it interacts with society—the autonomous car shares the\nroad with pedestrians, human-driven vehicles, and other autonomous cars. How to\ncombine these people’s values when they might be in conflict is an important problem we\nneed to solve. AI research can give us the tools to combine values in any way we decide\nbut can’t make the necessary decision for us.\n\n101\n\nHOUSE_OVERSIGHT_016321",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016321.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3631,
    "word_count": 624,
    "line_count": 53,
    "import_date": "2025-11-19T21:47:48.947342",
    "prefix": "IMAGES-003"
  }
}
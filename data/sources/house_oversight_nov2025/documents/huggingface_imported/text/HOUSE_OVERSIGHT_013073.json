{
  "document_id": "HOUSE_OVERSIGHT_013073",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013073.txt",
  "text": "8.7 Is Cognitive Synergy Tricky? 157\n\nFor instance, the PLN controller could make a list of everyone who has been a regular\nvisitor, and everyone who has not been, and pose MOSES the task of figuring out a procedure\nfor distinguishing these two categories. This procedure could then used directly to make the\nneeded assessment, or else be translated into logical rules to be used within PLN inference. For\nexample, perhaps MOSES would discover that older males wearing ties tend not to become\nregular visitors. If the new playmate is an older male wearing a tie, this is directly applicable.\nBut if the current playmate is wearing a tuxedo, then PLN may be helpful via reasoning that\neven though a tuxedo is not a tie, it’s a similar form of fancy dress — so PLN may extend the\nMOSES-learned rule to the present case and infer that the new playmate is not likely to be a\nregular visitor.\n\n8.7 Is Cognitive Synergy Tricky?\n\nIn this section we use the notion of cognitive synergy to explore a question that arises\nfrequently in the AGI community: the well-known difficulty of measuring intermediate progress\ntoward human-level AGI. We explore some potential reasons underlying this, via extending the\nnotion of cognitive synergy to a more refined notion of \"tricky cognitive synergy.\" These ideas\nare particularly relevant to the problem of creating a roadmap toward AGI, as we'll explore in\nChapter 17 below.\n\n8.7.1 The Puzzle: Why Is It So Hard to Measure Partial Progress\nToward Human-Level AGI?\n\nIt’s not entirely straightforward to create tests to measure the final achievement of human-level\nAGI, but there are some fairly obvious candidates here. There’s the Turing Test (fooling judges\ninto believing you’re human, in a text chat), the video Turing Test, the Robot College Student\ntest (passing university, via being judged exactly the same way a human student would), etc.\nThere’s certainly no agreement on which is the most meaningful such goal to strive for, but\nthere’s broad agreement that a number of goals of this nature basically make sense.\n\nOn the other hand, how does one measure whether one is, say, 50 percent of the way to\nhuman-level AGI? Or, say, 75 or 25 percent?\n\nIt’s possible to pose many \"practical tests\" of incremental progress toward human-level AGI,\nwith the property that if a proto-AGI system passes the test using a certain sort of architecture\nand/or dynamics, then this implies a certain amount of progress toward human-level AGI based\non particular theoretical assumptions about AGI. However, in each case of such a practical test,\nit seems intuitively likely to a significant percentage of AGI researchers that there is some way\nto \"game\" the test via designing a system specifically oriented toward passing that test, and\nwhich doesn’t constitute dramatic progress toward AGI.\n\nSome examples of practical tests of this nature would be\n\n1 This section co-authored with Jared Wigmore\n\nHOUSE_OVERSIGHT_013073",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013073.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2944,
    "word_count": 484,
    "line_count": 48,
    "import_date": "2025-11-19T21:47:46.759293",
    "prefix": "IMAGES-002"
  }
}
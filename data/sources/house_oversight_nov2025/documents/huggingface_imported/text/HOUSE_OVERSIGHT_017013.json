{
  "document_id": "HOUSE_OVERSIGHT_017013",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_017013.txt",
  "text": "II. Construction of Historical N-grams Corpora\n\nAs noted in the paper text, we did not analyze the entire set of 15 million books digitized by Google.\nInstead, we\n\n1. Performed further filtering steps to select only a subset of books with highly accurate metadata.\n\n2. Subdivided the books into ‘base corpora’ using such metadata fields as language, country of\npublication, and subject.\n\n3. For each base corpus, construct a massive numerical table that lists, for each n-gram (often a\nword or phrase), how often it appears in the given base corpus in every single year between 1550\nand 2008.\n\nIn this section, we will describe these three steps. These additional steps ensure high data quality, and\nalso make it possible to examine historical trends without violating the ‘fair use’ principle of copyright law:\nour object of study is the frequency tables produced in step 3 (which are available as supplemental data),\nand not the full-text of the books.\n\nII.1. Additional filtering of books\n\nIJ.1A. Accuracy of Date-of-Publication metadata\n\nAccurate date-of-publication data is crucial component in the production of time-resolved n-grams data.\nBecause our study focused most centrally on the English language corpus, we decided to apply more\nstringent inclusion criteria in order to make sure the accuracy of the date-of-publication data was as high\nas possible.\n\nWe found that the lion's share of date-of-publication errors were due to so-called 'bound-withs' - single\nvolumes that contain multiple works, such as anthologies or collected works of a given author. Among\nthese bound-withs, the most inaccurately dated subclass were serial publications, such as journals and\nperiodicals. For instance, many journals had publication dates which were erroneously attributed to the\nyear in which the first issue of the journal had been published. These journals and serial publications also\nrepresented a different aspect of culture than the books did. For these reasons, we decided to filter out all\nserial publications to the extent possible. Our ‘Serial Killer’ algorithm removed serial publications by\nlooking for suggestive metadata entries, containing one or more of the following:\n\n1. Serial-associated titles, containing such phrases as ‘Journal of, 'US Government report’, etc.\n\n2. Serial-associated authors, such as those in which the author field is blank, too numerous, or\ncontains words such as 'committee’.\n\nNote that the match is case-insensitive, and it must be to a complete word in the title; thus the filtering of\ntitles containing the word ‘digest’ does not lead to the removal of works with ‘digestion’ in the title. The\nentire list of serial-associated title phrases and serial-associated author phrases is included as\nsupplemental data (Appendix). For English books, 29.4% of books were filtered using the ‘Serial Killer’,\nwith the title filter removing 2% and the author filter removing 27.4%. Foreign language corpora were\nfiltered in a similar fashion.\n\nThis filtering step markedly increased the accuracy of the metadata dates. We determined metadata\naccuracy by examining 1000 filtered volumes distributed uniformly over time from 1801-2000 (5 per year).\nAn annotator with no knowledge of our study manually determined the date-of-publication. The annotator\nwas aware of the Google metadata dates during this process. We found that 5.8% of English books had\n\n5\n\nHOUSE_OVERSIGHT_017013",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_017013.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3410,
    "word_count": 525,
    "line_count": 57,
    "import_date": "2025-11-19T21:47:48.871082",
    "prefix": "IMAGES-004"
  }
}
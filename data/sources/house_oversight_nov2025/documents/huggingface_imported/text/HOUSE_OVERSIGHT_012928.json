{
  "document_id": "HOUSE_OVERSIGHT_012928",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_012928.txt",
  "text": "12 1 Introduction\n\ngroup. One of the key ideas involved in this project is explicit integration of subsymbolic and\nmore symbolic subsystems. For instance, one can use a purely subsymbolic, hierarchical pattern\nrecognition network for vision processing, and then link its internal structures into the nodes\nand links in the AtomSpace that represent concepts. So the subsymbolic and symbolic systems\ncan work harmoniously and productively together, a notion we will review in more detail in\nChapter 26.\n\n1.9 Language Learning\n\nOne of the subtler aspects of our current approach to teaching CogPrime is language learning.\nThree relatively crisp and simple approaches to language learning would be:\n\ne Build a language processing system using hand-coded grammatical rules, based on linguistic\ntheory;\n\ne Train a language processing system using supervised, unsupervised or semisupervised learn-\ning, based on computational linguistics;\n\ne Have an AI system learn language via experience, based on imitation and reinforcement and\nexperimentation, without any built-in distinction between linguistic behaviors and other\nbehaviors.\n\nWhile the third approach is conceptually appealing, our current approach in CogPrime (de-\nscribed in a series of chapters in Part 2) is none of the above, but rather a combination of the\nabove. OpenCog contains a natural language processing system built using a combination of\nthe rule-based and statistical approaches, which has reasonably adequate functionality; and our\nplan is to use it as an initial condition for ongoing adaptive improvement based on embodied\ncommunicative experience.\n\n1.10 AGI Ethics\n\nWhen discussing AGI work with the general public, ethical concerns often arise. Science fic-\ntion films like the Terminator series have raised public awareness of the possible dangers of\nadvanced AGI systems without correspondingly advanced ethics. Non-profit organizations like\nthe Singularity Institute for AI ( (http://singinst.org) have arisen specifically to raise attention\nabout, and foster research on, these potential dangers.\n\nOur main focus here is on how to create AGI, not how to teach an AGI human ethical\nprinciples. However, we will address the latter issue explicitly in Chapter 12, and we do think it’s\nimportant to emphasize that AGI ethics has been at the center of the design process throughout\nthe conception and development of CogPrime and OpenCog.\n\nBroadly speaking there are (at least) two major threats related to advanced AGI. One is\nthat people might use AGIs for bad ends; and the other is that, even if an AGI is made with\nthe best intentions, it might reprogram itself in a way that causes it to do something terrible.\nIf it’s smarter than us, we might be watching it carefully while it does this, and have no idea\nwhat’s going on.\n\nHOUSE_OVERSIGHT_012928",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_012928.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2822,
    "word_count": 436,
    "line_count": 51,
    "import_date": "2025-11-19T21:47:49.269004",
    "prefix": "IMAGES-002"
  }
}
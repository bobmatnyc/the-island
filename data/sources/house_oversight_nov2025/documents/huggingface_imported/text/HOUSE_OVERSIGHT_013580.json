{
  "document_id": "HOUSE_OVERSIGHT_013580",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013580.txt",
  "text": "or left. The new information being generated by the pattern of spatial orbits took the\nform of sequences of numbers or symbols representing the sequence of labeled\nboxes. The complexity of these numeric or symbol sequences was then quantified\nin a variety of ways including the use of two fundamental measures of dynamical\n\nentropy.\n\nOne measure reflects how many new, previously unexplored boxes were\nentered by the rat per unit of time. This rate represents a percent of the possible.\nThe second measure reflects how much of the time did the rat in each box visited\nas a distribution of the probable. The rate of expansion of the possible and the\nrelative time in occupancy of these possibles, the probables, form the bases for the\ncomputation of these two kinds of entropies. For example, the work of Paulus and\nGeyer showed that the administration of a very small amount of stimulant drug,\ncompared with a salt water control, led to an increase in the first measure of the\nnumber of new, previously unexplored, boxes entered per unit time. With respect to\nthe second measure, the stimulant drug augmented exploratory activity was also\nmore uniformly distributed over the possible boxes, making for more uniform\nprobability. Administration of higher doses of stimulant drugs, at a critical dose, led\nsuddenly to more spatially and temporally restricted and stereotyped patterns of\nmotion of the rats, compulsive circling alternating with frozen sniffing. Both\ncontributed to a decrease in the possible and nonuniformity in the distribution of the\nprobabilities. In man, low doses of amphetamine tend to increase the rate and\ncreativity of thought streams and high doses generate fixed ideas and paranoid\ndelusions. In the statistical approach to nonlinear dynamical systems, time-\ndependent generation of new possibilities is called topological entropy, H; and the\nentropy associated with the distribution of probabilities is called the metric entropy,\nHy. These kinds of entropies have also been used to quantitate characteristic\n\npatterns of in human behavior as well.\n\nWe have previously mentioned these measures as used in human\nexperiments by Karen Selz, a Research Professor of Psychiatry at Emory University\nin Atlanta. Recall that she devised a set of experiments leading to unobtrusive\n\nmeasures made on human subjects by asking them to remove, as many as they\n\n80\n\nHOUSE_OVERSIGHT_013580",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013580.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2400,
    "word_count": 381,
    "line_count": 40,
    "import_date": "2025-11-19T21:47:44.272419",
    "prefix": "IMAGES-002"
  }
}
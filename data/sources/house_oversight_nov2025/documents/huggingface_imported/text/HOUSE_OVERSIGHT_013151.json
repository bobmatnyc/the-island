{
  "document_id": "HOUSE_OVERSIGHT_013151",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013151.txt",
  "text": "12.9 AGI Ethics As Related to Various Future Scenarios 235\n\napproach does not necessarily need to be augmented with a prior complex of “ascent-safe” moral\nimperatives at startup time. Developing an AGI with theory of mind and ethical reinforcement\nlearning capabilities as described (admittedly, no small task!) is all that is needed in this case\n— the rest happens through training and experience as with any other moderate intelligence.\n\n12.9.2 Superintelligent AI: Soft- Takeoff Scenarios\n\nSoft takeoff scenarios are similar to capped-intelligence ones in that in both cases an AGI’s\nprogression from standard intelligence happens on a time scale which permits ongoing human\ninteraction during the ascent. However, in this case, as there is no predetermined limit on\nintelligence, it is necessary to account for the possibility of a superintelligence emerging (though\nof course this is not guaranteed). The soft takeoff model includes as subsets both controlled-\nascent models in which this rate of intelligence gain is achieved deliberately through software\nconstraints and/or meting-out of computational resources to the AGI, and uncontrolled-ascent\nmodels in which there is coincidentally no hard takeoff despite no particular safeguards against\none. Both have similar properties with regard to ethical considerations:\n\n1. Ethical considerations under this scenario include not only the usual interhuman ethical\nconcerns, but also the issue of how to convince a potential burgeoning superintelligence to:\n\na. Care about humanity in the first place, rather than ignore it\n\nb. Benefit humanity, rather than destroy it\n\nc. Elevate humanity to a higher level of intelligence, which even if an AGI decided to\nproceed with requires finding the right balance amongst some enormous considerations:\n\ni. Reconcile the aforementioned issues of ethical coherence and group volition, in a\nmanner which allows the most people to benefit (even if they don’t all do so in the\nsame way, based on their own preferences)\n\nii. Solve the problems of biological senescence, or focus on human uploading and the\npreservation of the maintenance, support, and improvement infrastructure for inor-\nganic intelligence, or both\n\niii. Preserve individual identity and continuity of consciousness, or override it in favor\nof continuity of knowledge and ease of harmonious integration, or both on a case-\nby-case basis\n\n2. The degree of danger is mitigated by the long timeline of ascent from mundane to super\nintelligence, and time is not of the essence.\n\n3. Learning that proceeds in a relatively human-like manner is entirely relevant to such human-\nlike intelligences, in their initial configurations. This means more interaction with and\nimitative-reinforcement-corrective learning guided by humans, which has both positive and\nnegative possibilities.\n\n12.9.3 Superintelligent AI: Hard-Takeoff Scenarios\n\n“Hard takeoff” scenarios assume that upon reaching an unknown inflection point (the Singularity\npoint [Vin93, Kur06]) in the intellectual growth of an AGI, an extraordinarily rapid increase\n\nHOUSE_OVERSIGHT_013151",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013151.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3097,
    "word_count": 460,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:44.155679",
    "prefix": "IMAGES-002"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016902",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016902.txt",
  "text": "When it comes to real robots helping real people, the standard definition of AI\nfails us, for two fundamental reasons: First, optimizing the robot’s reward function in\nisolation is different from optimizing it when the robot acts around people, because\npeople take actions too. We make decisions in service of our own interests, and these\ndecisions dictate what actions we execute. Moreover, we reason about the robot—that is,\nwe respond to what we think it’s doing or will do and what we think its capabilities are.\nWhatever actions the robot decides on need to mesh well with ours. This is the\ncoordination problem.\n\nSecond, it is ultimately a human who determines what the robot’s reward function\nshould be in the first place. And they are meant to incentivize robot behavior that\nmatches what the end-user wants, what the designer wants, or what society as a whole\nwants. I believe that capable robots that go beyond very narrowly defined tasks will need\nto understand this to achieve compatibility with humans. This is the va/ue-alignment\nproblem.\n\nThe Coordination Problem: People are more than objects in the environment.\n\nWhen we design robots for a particular task, it’s tempting to abstract people away. A\nrobotic personal assistant, for example, needs to know how to move to pick up objects, so\nwe define that problem in isolation from the people for whom the robot is picking these\nobjects up. Still, as the robot moves around, we don’t want it bumping into anything, and\nthat includes people, so we might include the physical location of the person in the\ndefinition of the robot’s state. Same for cars: We don’t want them colliding with other\ncars, so we enable them to track the positions of those other cars and assume that they’ ll\nbe moving consistently in the same direction in the future. A human being, in this sense,\nis no different to a robot from a ball rolling on a flat surface. The ball will behave in the\nnext few seconds the same way it behaved in the past few; it keeps rolling in the same\ndirection at roughly the same speed. This is of course nothing like real human behavior,\nbut such simplification enables many robots to succeed in their tasks and, for the most\npart, stay out of people’s way. A robot in your house, for example, might see you\ncoming down the hall, move aside to let you pass, and resume its task once you’ve gone\nby.\n\nAs robots have become more capable, though, treating people as consistently\nmoving obstacles is starting to fall short. A human driver switching lanes won’t continue\nin the same direction but will move straight ahead once they’ ve made the lane change.\nWhen you reach for something, you often reach around other objects and stop when you\nget to the one you want. When you walk down a hallway, you have a destination in\nmind: You might take a right into the bedroom or a left into the living room. Relying on\nthe assumption that we’re no different from a rolling ball leads to inefficiency when the\nrobot stays out of the way if it doesn’t need to, and it can imperil the robot when the\nperson’s behavior changes. Even just to stay out of the way, robots have to be somewhat\naccurate at anticipating human actions. And, unlike the rolling ball, what people will do\ndepends on what they decide to do. So to anticipate human actions, robots need to start\nunderstanding human decision making. And that doesn’t mean assuming that human\nbehavior is perfectly optimal; that might be enough for a chess- or Go-playing robot, but\nin the real world, people’s decisions are less predictable than the optimal move in a board\ngame.\n\n99\n\nHOUSE_OVERSIGHT_016902",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016902.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3617,
    "word_count": 630,
    "line_count": 53,
    "import_date": "2025-11-19T21:47:48.383915",
    "prefix": "IMAGES-004"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_013052",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013052.txt",
  "text": "136 7 A Formal Model of Intelligent Agents\n\nweight to a situation S based on the ease with which one agent in a society can communicate\nS to another agent in that society, using multimodal communication (including verbalization,\ndemonstration, dramatic and pictorial depiction, etc.).\n\nFinally, we present a formal measure of the “generality” of an intelligence, which precisiates\nthe informal distinction between “general AT’ and “narrow AI.”\n\n7.3.1 Biased Universal Intelligence\n\nTo define universal intelligence, Legg and Hutter consider the class of environments that are\nreward-summable, meaning that the total amount of reward they return to any agent is bounded\nby 1. Where 7; denotes the reward experienced by the agent from the environment at time i,\nthe expected total reward for the agent 7 from the environment j: is defined as\n\nVr=B(S ori) <1\n1\n\nTo extend their definition in the direction of greater realism, we first introduce a second-order\nprobability distribution v, which is a probability distribution over the space of environments\nu. The distribution v assigns each environment a probability. One such distribution v is the\nSolomonoff-Levin universal distribution in which one sets v = 2-*; but this is not the only\ndistribution v of interest. In fact a great deal of real-world general intelligence consists of the\nadaptation of intelligent systems to particular distributions v over environment-space, differing\nfrom the universal distribution.\n\nWe then define\n\nDefinition 4 The biased universal intelligence of an agent a is its expected performance\nwith respect to the distribution v over the space of all computable reward-summable environ-\nments, E, that is,\n\nY(m) = SO vr\n\npew\n\nLegg and Hutter’s universal intelligence is obtained by setting v equal to the universal\ndistribution.\n\nThis framework is more flexible than it might seem. E.g. suppose one wants to incorporate\nagents that die. Then one may create a special action, say agge, corresponding to the state of\ndeath, to create agents that\n\ne in certain circumstances output action ages\ne have the property that if their previous action was ageg, then all of their subsequent actions\nmust be ages\n\nand to define a reward structure so that actions aggg always bring zero reward. It then follows\nthat death is generally a bad thing if one wants to maximize intelligence. Agents that die will\nnot get rewarded after they’re dead; and agents that live only 70 years, say, will be restricted\nfrom getting rewards involving long-term patterns and will hence have specific limits on their\nintelligence.\n\nHOUSE_OVERSIGHT_013052",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013052.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2603,
    "word_count": 412,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:45.460483",
    "prefix": "IMAGES-002"
  }
}
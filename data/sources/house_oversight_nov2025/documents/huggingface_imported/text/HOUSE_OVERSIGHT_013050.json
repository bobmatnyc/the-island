{
  "document_id": "HOUSE_OVERSIGHT_013050",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013050.txt",
  "text": "134 7 A Formal Model of Intelligent Agents\n\nContext & Procedure — Goal\n\nand considered more formally as holds(C) & ex(P) > h, where h may be an externally specified\ngoal g; or an internally specified goal h derived as a (possibly uncertain) subgoal of one of more\ngi; C is a piece of declarative or episodic knowledge and P is a procedure that the agent can\ninternally execute to generate a series of actions. ex(P) is the proposition that P is successfully\nexecuted. If C is episodic then holds(C) may be interpreted as the current context (i.e. some\nfinite slice of the agent’s history) being similar to C; if C is declarative then holds(C) may be\ninterpreted as the truth value of C' evaluated at the current context. Note that C may refer to\nsome part of the world quite distant from the agent’s current sensory observations; but it may\nstill be formally evaluated based on the agent’s history.\n\nIn the standard CogPrime notation as introduced formally in Chapter 20 (where indentation\nhas function-argument syntax similar to that in Python, and relationship types are prepended\nto their relata without parentheses), for the case C is declarative this would be written as\n\nPredictiveExtensionallmplication\nAND\nC\nExecution P\n\nG\n\nand in the case C' is episodic one replaces C in this formula with a predicate expressing C’s\nsimilarity to the current context. The semantics of the PredictiveExtensionalInheritance relation\nwill be discussed below. The Execution relation simply denotes the proposition that procedure\nP has been executed.\n\nFor the class of SRAM agents who (like CogPrime) use the cognitive schematic to govern\nmany or all of their actions, a significant fragment of agent intelligence boils down to estimating\nthe truth values of PredictiveExtensionallmplication relationships. Action selection procedures\ncan be used, which choose procedures to enact based on which ones are judged most likely\nto achieve the current external goals g; in the current context. Rather than enter into the\nparticularities of action selection or other cognitive architecture issues, we will restrict ourselves\nto PLN inference, which in the context of the present agent model is a method for handling\nPredictivelmplication in the cognitive schematic.\n\nConsider an agent in a virtual world, such as a virtual dog, one of whose external goals is to\nplease its owner. Suppose its owner has asked it to find a cat, and it can translate this into a\nsubgoal “find cat.” If the agent operates according to the cognitive schematic, it will search for\nP so that\n\nPredictiveExtensionallmplication\nAND\nC\nExecution P\nEvaluation\nfound\ncat\n\nholds.\n\nHOUSE_OVERSIGHT_013050",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013050.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 2654,
    "word_count": 429,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:45.595311",
    "prefix": "IMAGES-002"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016876",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016876.txt",
  "text": "appreciated among AI researchers. AI risk is not yet common knowledge either. In\nrelation to the timeline of the first dissident message, I’d say we’re around the year 1988,\nwhen raising the Soviet-occupation topic was no longer a career-ending move but you\nstill had to somewhat hedge your position. I hear similar hedging now—statements like,\n“Tm not concerned about superintelligent AI, but there are some real ethical issues in\nincreased automation,” or “It’s good that some people are researching AI risk, but it’s not\na short-term concern,” or even the very reasonable sounding, “These are small-\nprobability scenarios, but their potentially high impact justifies the attention.”\n\nAs far as message propagation goes, though, we are getting close to the tipping\npoint. A recent survey of AI researchers who published at the two major international AI\nconferences in 2015 found that 40 percent now think that risks from highly advanced AI\nare either “an important problem” or “among the most important problems in the field.””°\n\nOf course, just as there were dogmatic Communists who never changed their\nposition, it’s all but guaranteed that some people will never admit that AI is potentially\ndangerous. Many of the deniers of the first kind came from the Soviet nomenklatura;\nsimilarly, the AI-risk deniers often have financial or other pragmatic motives. One of the\nleading motives is corporate profits. Al is profitable, and even in instances where it isn’t,\nit’s at least a trendy, forward-looking enterprise with which to associate your company.\nSo a lot of the dismissive positions are products of corporate PR and legal machinery. In\nsome very real sense, big corporations are nonhuman machines that pursue their own\ninterests—interests that might not align with those of any particular human working for\nthem. As Wiener observed in Zhe Human Use of Human Beings: “When human atoms\nare knit into an organization in which they are used, not in their full right as responsible\nhuman beings, but as cogs and levers and rods, it matters little that their raw material is\nflesh and blood.”\n\nAnother strong incentive to turn a blind eye to the AI risk is the (very human)\ncuriosity that knows no bounds. “When you see something that is technically sweet, you\ngo ahead and do it and you argue about what to do about it only after you have had your\ntechnical success. That is the way it was with the atomic bomb,” said J. Robert\nOppenheimer. His words were echoed recently by Geoffrey Hinton, arguably the\ninventor of deep learning, in the context of AI risk: “I could give you the usual\narguments, but the truth is that the prospect of discovery is too sweet.”\n\nUndeniably, we have both entrepreneurial attitude and scientific curiosity to thank\nfor almost all the nice things we take for granted in the modern era. It’s important to\nrealize, though, that progress does not owe us a good future. In Wiener’s words, “It is\npossible to believe in progress as a fact without believing in progress as an ethical\nprinciple.”\n\nUltimately, we don’t have the luxury of waiting before all the corporate heads and\nAI researchers are willing to concede the AI risk. Imagine yourself sitting in a plane\nabout to take off. Suddenly there’s an announcement that 40 percent of the experts\nbelieve there’s a bomb onboard. At that point, the course of action is already clear, and\nsitting there waiting for the remaining 60 percent to come around isn’t part of it.\n\n3 Katja Grace, et al., “When Will AI Exceed Human Performance? Evidence from AI Experts,”\nhttps://arxiv.org/pdf/1705.08807. pdf.\n\n73\n\nHOUSE_OVERSIGHT_016876",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016876.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3610,
    "word_count": 598,
    "line_count": 54,
    "import_date": "2025-11-19T21:47:48.442807",
    "prefix": "IMAGES-004"
  }
}
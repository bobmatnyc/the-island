{
  "document_id": "HOUSE_OVERSIGHT_016869",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016869.txt",
  "text": "more efficiently by machines. The successful creation of AGI would be the biggest event\nin human history, so why is there so little serious discussion of what it might lead to?\n\nHere again, the answer involves multiple reasons.\n\nFirst, as Upton Sinclair famously quipped, “It is difficult to get a man to\nunderstand something, when his salary depends on his not understanding it.”!” For\nexample, spokesmen for tech companies or university research groups often claim there\nare no risks attached to their activities even if they privately think otherwise. Sinclair’s\nobservation may help explain not only reactions to risks from smoking and climate\nchange but also why some treat technology as a new religion whose central articles of\nfaith are that more technology is always better and whose heretics are clueless\nscaremongering Luddites.\n\nSecond, humans have a long track record of wishful thinking, flawed\nextrapolation of the past, and underestimation of emerging technologies. Darwinian\nevolution endowed us with powerful fear of concrete threats, not of abstract threats from\nfuture technologies that are hard to visualize or even imagine. Consider trying to warn\npeople in 1930 of a future nuclear arms race, when you couldn’t show them a single\nnuclear explosion video and nobody even knew how to build such weapons. Even top\nscientists can underestimate uncertainty, making forecasts that are either too optimistic—\nWhere are those fusion reactors and flying cars?—or too pessimistic. Ernest Rutherford,\narguably the greatest nuclear physicist of his time, said in 1933—less than twenty-four\nhours before Leo Szilard conceived of the nuclear chain reaction—that nuclear energy\nwas “moonshine.” Essentially nobody at that time saw the nuclear arms race coming.\n\nThird, psychologists have discovered that we tend to avoid thinking of disturbing\nthreats when we believe there’s nothing we can do about them anyway. In this case,\nhowever, there are many constructive things we can do, if we can get ourselves to start\nthinking about the issue.\n\nWhat can we do?\n\nI’m advocating a strategy change from “Let’s rush to build technology that makes us\nobsolete—what could possibly go wrong?” to “Let’s envision an inspiring future and\nsteer toward it.”\n\nTo motivate the effort required for steering, this strategy begins by envisioning an\nenticing destination. Although Hollywood’s futures tend to be dystopian, the fact is that\nAGI can help life flourish as never before. Everything I love about civilization is the\nproduct of intelligence, so if we can amplify our own intelligence with AGI, we have the\npotential to solve today’s and tomorrow’s thorniest problems, including disease, climate\nchange, and poverty. The more detailed we can make our shared positive visions for the\nfuture, the more motivated we will be to work together to realize them.\n\nWhat should we do in terms of steering? The twenty-three Asilomar principles\nadopted in 2017 offer plenty of guidance, including these short-term goals:\n\n(1) An arms race in lethal autonomous weapons should be avoided.\n\n(2) The economic prosperity created by AI should be shared broadly, to benefit all\nof humanity.\n\n7 Upton Sinclair, , Candidate for Governor: And How I Got Licked (Berkeley CA: University of\nCalifornia Press, 1994), p. 109.\n\n66\n\nHOUSE_OVERSIGHT_016869",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016869.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3323,
    "word_count": 522,
    "line_count": 59,
    "import_date": "2025-11-19T21:47:44.185032",
    "prefix": "IMAGES-004"
  }
}
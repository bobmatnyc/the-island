{
  "document_id": "HOUSE_OVERSIGHT_015501",
  "filename": "TEXT-001-HOUSE_OVERSIGHT_015501.txt",
  "text": "﻿Game Theory and Morality\nMoshe Hoffman , Erez Yoeli , and Carlos David Navarrete\nIntroduction\nConsider the following puzzling aspects of our morality:\n1. Many of us share the view that one should not use people, even if it benefits them\nto be used, as Kant intoned in his second formulation of the categorical imperative:\n“Act in such a way that you treat humanity, whether in your own person or\nin the person of any other, never merely as a means to an end, but always at the\nsame time as an end” (Kant, 1997 ). Consider dwarf tossing, where dwarfs wearing\nprotective padding are thrown for amusement, usually at a party or pub. It is\nviewed as a violation of dwarfs’ basic dignity to use them as a means for amusement,\neven though dwarves willingly engage in the activity for economic gain.\nMany jurisdictions ban dwarf tossing on the grounds that the activity violates\ndwarfs’ basic human rights, and these laws have withstood lawsuits raised by\ndwarfs suing over the loss of employment (!).\n2. Charitable giving is considered virtuous, but little attention is paid to how just\nthe cause or efficient the charity. For example, Jewish and Christian traditions\nadvocate giving 10 % of one’s income to charity, but make no mention of the\nimportance of evaluating the cause or avoiding wasteful charities. The intuition\nthat giving to charity is a moral good regardless of efficacy results in the persistence\nof numerous inefficient and corrupt charities. For example, the Wishing\nWell Foundation has, for nearly a decade, ranked as one of CharityNavigator.\nM. Hoffman (*) • E. Yoeli\nProgram for Evolutionary Dynamics , Harvard University ,\nOne Brattle Square, Suite 6 , Cambridge , MA 02138 , USA\ne-mail: moshehoffman@fas.harvard.edu\nC.D. Navarrete\nDepartment of Psychology, and, the Ecology, Evolutionary Biology and Behavior Program ,\nMichigan State University , East Lansing , MI , USA\n© Springer International Publishing Switzerland 2016\nT.K. Shackelford, R.D. Hansen (eds.), The Evolution of Morality,\nEvolutionary Psychology, DOI 10.1007/978-3-319-19671-8_14\n289\n290\nM. Hoffman et al.\ncom’s most inefficient charities. Yet its mission of fulfilling wishes by children\nwith terminal illnesses is identical to that of the more efficient Make-A-Wish\nFoundation. Worse yet, scams masquerading as charities persist. One man operating\nas The US Navy Veteran’s Association collected over 100 million dollars—\nover 7 years!—before anyone bothered to investigate the charity.\n3. In every culture and age, injunctions against murder have existed. If there is one\nthing much of humanity seems to agree on, it’s that ending the life of another\nwithout just cause which is among the worst of moral violations. Yet cultures\ndon’t consider the loss of useful life years in their definition, even though it is\nrelevant to the measure of harm done by the murder. Why is our morality so\nmuch more sensitive to whether a life was lost than to how much life was lost?\nThere are numerous other examples of how our moral intuitions appear to be rife\nwith logical inconsistencies. In this chapter, we use game theory to provide insight\non a range of moral puzzles similar to the puzzles described above.\nWhat Is Game Theory and Why Is It Relevant?\nIn this section , we review the defi nition of a game , and of a Nash equilibrium , then\ndiscuss how evolution and learning processes would yield moral intuitions consistent\nwith Nash equilibria .\nGame theory is a tool for the analysis of social interactions. In a game, the payoff\nto each player depends on their actions, as well as the actions of others. Consider\nthe Prisoner’s Dilemma (Chammah & Rapoport, 1965 ; see Fig. 1 ), a model that\ncaptures the paradox of cooperation. Each of two players chooses whether to cooperate\nor to defect. Cooperating reduces a player’s payoff by c > 0 while increasing\nthe other’s payoffs by b> c. Players could be vampire bats with the option of\nsharing blood, or firms with the option of letting each other use their databases, or\npremed students deciding whether to take the time to help one another to study. The\npayoffs, b and c , may represent likelihood of surviving and leaving offspring, profits,\nor chance of getting into a good medical school.\nSolutions to such games are analyzed using the concept of a Nash equilibrium 1 —\na specification of each player’s action such that no player can increase his payoff by\ndeviating unilaterally. In the Prisoner’s Dilemma, the only Nash equilibrium is for\nneither player to cooperate, since regardless of what the other player does, cooperation\nreduces one’s own payoff.\n1\nNote that we focus on the concept of Nash equilibrium in this chapter and not evolutionary stable\nstrategy (ESS), a refinement of Nash that might be more familiar to an evolutionary audience. ESS\nare the Nash equilibria that are most relevant in evolutionary contexts. However, ESS is not well\ndefined in many of our games, so we will focus on the insights garnered from Nash and directly\ndiscuss evolutionary dynamics when appropriate.\nMorality Games\n291\nFig. 1 The Prisoner’s\nDilemma. Player 1’s available\nstrategies (C and D, which\nstand for cooperate and\ndefect, respectively) are\nrepresented as rows . Player\n2’s available strategies (also\nC and D) are represented as\ncolumns . Player 1’s payoffs\nare represented at the\nintersection of each row and\ncolumn. For example, if\nplayer 1 plays D and player 2\nplays C, player 1’s payoff is\nb. The Nash equilibrium of\nthe game is (D, D). It is\nindicated with a circle\nGame theory has traditionally been applied in situations where players are rational\ndecision makers who deliberately maximize their payoffs, such as pricing\ndecisions of firms (Tirole, 1988 ) or bidding in auctions (Milgrom & Weber, 1982 ).\nIn these contexts, behavior is expected to be consistent with a Nash equilibrium,\notherwise one of the agents—who are actively deliberating about what to do—\nwould realize she could benefit from deviating from the prescribed strategy.\nHowever, game theory also applies to evolutionary and learning processes, where\nagents do not deliberately choose their behavior in the game, but play according to\nstrategies with which they are born, imitate, or otherwise learn. Agents play a game\nand then “reproduce” based on their payoffs, where reproduction represents offspring,\nimitation, or learning. The new generation then play the game, and so on. In\nsuch settings, if a mutant does better (mutation can be genetic or can happen when\nagents experiment), then she is more likely to reproduce or her behavior imitated or\nreinforced, causing the behavior to spread. This intuition is formalized using models\nof evolutionary dynamics (e.g., Nowak, 2006 ).\nThe key result for evolutionary dynamic models is that, except under extreme\nconditions, behavior converges to Nash equilibria. This result rests on one simple,\nnoncontroversial assumption shared by all evolutionary dynamics: Behaviors that\nare relatively successful will increase in frequency. Based on this logic, game theory\nmodels have been fruitfully applied in biological contexts to explain phenomena\nsuch as animal sex ratios (Fisher, 1958 ), territoriality (Smith & Price, 1973 ), cooperation\n(Trivers, 1971 ), sexual displays (Zahavi, 1975 ), and parent–offspring conflict\n(Trivers, 1974 ). More recently, evolutionary dynamic models have been applied\nin human contexts where conscious deliberation is believed to not play an important\nrole, such as in the adoption of religious rituals (Sosis & Alcorta, 2003 ), in the\nexpression and experience of emotion (Frank, 1988 ; Winter, 2014 ), and in the use\nof indirect speech (Pinker, Nowak, & Lee, 2008 ).\n292\nM. Hoffman et al.\nCrucially for this chapter, because our behaviors are mediated by moral intuitions\nand ideologies, if our moral behaviors converge to Nash, so must the intuitions and\nideologies that motivate them. The resulting intuitions and ideologies will bear\nthe signature of their game theoretic origins, and this signature will lend clarity on\nthe puzzling, counterintuitive, and otherwise hard-to-explain features of our moral\nintuitions, as exemplified by our motivating examples.\nIn order for game theory to be relevant to understanding our moral intuitions and\nideologies, we need only the following simple assumption: Moral intuitions and\nideologies that lead to higher payoffs become more frequent . This assumption can\nbe met if moral intuitions that yield higher payoffs are held more tenaciously, are\nmore likely to be imitated, or are genetically encoded. For example, if every time\nyou transgress by commission you are punished, but every time you transgress by\nomission you are not, you will start to intuit that commission is worse than\nomission.\nRights and the Hawk–Dove Game\nIn this section we will argue that just as the Hawk –Dove model explains animal territoriality\n(Maynard Smith & Price, 1973 , to be reviewed shortly ), the Hawk –Dove\nmodel sheds light onto our sense of rights (Descioli & Karpoff, 2014 ; Gintis, 2007 ;\nMyerson, 2004 ).\nLet us begin by asking the following question (Myerson, 2004 ): “Why [does] a\npassenger pay a taxi driver after getting out of the cab in a city where she is visiting\nfor one day, not expecting to return?” If the cabby complains to the authorities, the\npassenger could plausibly claim that she had paid in cash. The answer, of course, is\nthat the cabby would feel that the money the passenger withheld was his—that he\nhad a right to be paid for his service—and get angry, perhaps making a scene or\neven starting a fight. Likewise, if the passenger did in fact pay, but the cabby\ndemanded money a second time, the passenger would similarly be infuriated. This\nexample illustrates that people have powerful intuitions regarding rightful ownership.\nIn this section, we explore what the Hawk – Dove game can teach us about our\nsense of property rights.\nThe reader is likely familiar with the Hawk – Dove game, a model of disputes\nover contested resources. In the Hawk – Dove game, each player decides whether to\nfight over a resource or to acquiesce (i.e. play Hawk or Dove). If one fights and the\nother does not, the fighter gets the resource, worth v . If both fight, each pays a cost\nc and split the resource. That is, each gets v/2- c. If neither fights, they split the\nresource and get v /2. As long as v/2 < c, then in any stable Nash equilibrium, one\nplayer fights and the other acquiesces. That is, if one player expects the other to\nfight, she is better off acquiescing, and vice versa (see Fig. 2 ).\nCrucially, it is not just a Nash equilibrium for one player to always play Hawk\nand the other to always play Dove. It is also an equilibrium for both players to condition\nwhether they play Hawk on an uncorrelated asymmetry —a cue or event that\nMorality Games\n293\nFig. 2 The Hawk–Dove\ngame. The Nash equilibria of\nthe game are circled\ndoes not necessarily affect the payoffs, but does distinguish between the players,\nsuch as who arrived at the territory first or who built the object. If one conditions on\nthe event (say, plays Hawk when she arrives first), then it is optimal for the other to\ncondition on the event (to play Dove when the other arrives first).\nAs our reader is likely aware, this was the logic provided by Maynard Smith to\nexplain animal territoriality—why animals behave aggressively to defend territory\nthat they have arrived at first, even if incumbency does not provide a defensive\nadvantage and even when facing a more formidable intruder. Over the years, evidence\nhas amassed to support Maynard Smith’s explanation, such as experimental\nmanipulation of which animal arrives first (Davies, 1978 ; Sigg & Falett, 1985 ).\nLike other animals, we condition how aggressively we defend a resource on\nwhether we arrive first. Because our behaviors are motivated by beliefs, we are also\nmore likely to believe that the resource is “ours” when we arrive first. Studies have\nshown these effects with children’s judgments of ownership, in ethnographies of\nprelegal societies, and in computer games. In one such illustration, DeScioli and\nWilson ( 2011 ) had research subjects play a computer game in which they contested\na berry patch. Subjects who ended up keeping control of the patch usually arrived\nfirst, and this determined the outcome more often than differences in fighting ability\nin the game.\nThis sense of ownership is codified in our legal systems, as illustrated by the quip\n“possession is 9/10ths of the law,” and in a study involving famous legal property\ncases conducted by Descioli and Karpoff ( 2014 ). In a survey, these researchers\nasked participants to identify the rightful owner of a lost item, after reading vignettes\nbased on famous property rights legal cases. Participants consistently identified the\npossessor of the found item as its rightful owner (as the judges had at the time of the\ncase). This sense of ownership is also codified in our philosophical tradition, e.g., in\nLocke ( 1988 ), who found property rights in initial possession. Note that, as has also\nbeen found in animals, possession extends to objects on one’s land: In DeScioli and\n294\nM. Hoffman et al.\nKarpoff’s survey, another dictate of participants’ (and the judges’) property rights\nintuitions was who owned the land on which the lost item was found.\nAlso like animals, our sense of property rights is influenced by who created or\ninvested in the resource, another uncorrelated asymmetry. In locales that sometimes\ngrant property rights to squatters—individuals who occupy lands others have purchased—a\nkey determinant of whether the squatters are granted the land is whether\nthey have invested in it (Cone vs. West Virginia Pulp & Paper Co., 1947 ; Neuwirth,\n2005 ). Locke also intuited that investment in land is part of what makes it ours:\nIn Second Treatise on Civil Government (1689), Locke wrote, “everyman has a\nproperty in his person; this nobody has a right to but himself. The labor of his body\nand the work of his hand, we may say, are properly his.”\nIf the Hawk – Dove model underlies our sense of property rights, we would expect\nto see psychological mechanisms that motivate us to feel entitled to an object when\nwe possess it or have invested in it. Here are three such mechanisms, which can be\nseen by reinterpreting some well-documented “biases” in the behavioral economics\nliterature. The first such bias is the endowment effect : We value items more if we are\nin possession of them. The endowment effect has been documented in dozens of\nexperiments, where subjects are randomly given an item (mug, pen, etc.) and\nsubsequently state that they are willing to sell the mug for much more than those\nwho were not given the mug are willing to pay (Kahneman, Knetsch, & Thaler,\n1990 ). In the behavioral economics literature, the endowment effect has sometimes\nbeen explained by loss aversion, which is when we are harmed more by a loss than\nwe benefit from an equivalent gain. However, the source of loss aversion is not\nquestioned or explained. When it is, loss aversion is also readily explained by the\nHawk – Dove game (Gintis, 2007 ).\nA second bias that also fits the Hawk – Dove model is the IKEA effect : Our valuation\nof an object is influenced by whether we have developed or built the resource.\nThe IKEA effect has been documented by asking people how much they would pay\nfor items like Lego structures or IKEA furniture after randomly being assigned to\nbuild them or receive them pre-built. Subjects are willing to pay more for items they\nbuild themselves.\nA third such bias that fits the Hawk – Dove model is the sunk cost fallacy (Mankiw,\n2007 ; Thaler, 1980 ), which leads us to “throw good money after bad” when we\ninvest in ventures simply because we have already put so much effort into them,\narguably because our prior efforts lead us to value those ventures more.\nPossession and past investment are not the only uncorrelated asymmetries that\ncan dictate rights. Rights can be dictated by a history of agreements, as happens\nwhen one party sells another deed to a house or car, or, as in our taxicab example,\nby whether a service was provided. There are also countless examples in which\nrights were determined by perhaps unfair or arbitrary characteristics such as race\nand sex: Black Americans were expected to give up their seat for Whites in the Jim\nCrow South and women to hand over their earnings or property to their husbands\nthroughout the ages.\nHawk – Dove is not just a post hoc explanation for our sense of rights; it also leads\nto the following novel insight: We can formally characterize the properties that\nMorality Games\n295\nuncorrelated asymmetries must have. This requires a bit more game theory to illustrate;\nthe logic is detailed in the section on categorical distinctions but the implications\nare straightforward: Uncorrelated asymmetries must be discrete (as in who\narrived first or whether someone has African ancestry) and cannot be continuous\n(who is stronger, whether someone has darker skin). Indeed, we challenge the reader\nto identify a case where our sense of rights depends on surpassing a threshold in a\ncontinuous variable (stronger than? darker than?). More generally, an asymmetry\nmust have the characteristic that, when it occurs, every observer believes it occurred\nwith a sufficiently high probability, where the exact level of confidence is determined\nby the payoffs of the game. This is true of public, explicit speech and handshakes,\nbut not innuendos or rumors. (Formally, explicit speech and handshakes\ninduce what game theorists term common p-beliefs.)\nThe Hawk – Dove explanation of our sense of rights also gives useful clarity on\nwhen there will be conflict. Conflict will arise if both players receive opposing signals\nregarding the uncorrelated asymmetry, such as two individuals each believing\nthey arrived first, or when there are two uncorrelated asymmetries that point in\nconflicting directions, such as when one person invested more and the other arrived\nfirst. The former source of conflict appears to be the case in the Israeli–Palestinian\nconflict. Indeed, both sides pour great resources into demonstrating their early\npossession, especially Israel, through investments in and public displays of archeology\nand history. The latter source of conflict appears to be the case in many of the\ncontested legal disputes in the study by DeScioli and Karpoff ( 2014 ) mentioned\nabove. An example is one person finds an object on another’s land. Indeed, this turns\nout to be a source of many legal conflicts over property rights, and a rich legal tradition\nhas developed to assign precedence to one uncorrelated asymmetry over another\n(Descioli & Karpoff, 2014 ). As usual, we see similar behavior in animals in studies\nthat provide empirical support for Maynard Smith’s model for animal territoriality:\nWhen two animals are each given the impression they arrived first by, for example,\nclever use of mirrors, a fight ensues (Davies, 1978 ).\nAuthentic Altruism, Motives, and the Envelope Game\nIn this section , we present a simple extension of the Repeated Prisoner’s Dilemma\nto explain why morality depends not just on what people do but also what they think\nor consider .\nIn the Repeated Prisoner’s Dilemma and other models of cooperation, players\njudge others by their actions—whether they cooperate or defect. However, we not\nonly care about whether others cooperate but also about their decision-making process:\nWe place more trust in cooperators who never even considered defecting. To\nquote Kant, “In law a man is guilty when he violates the rights of others. In ethics\nhe is guilty if he only thinks of doing so.”\nThe Envelope Game (Fig. 3 ) models why we care about thoughts and considerations\nand not just actions (Hoffman, Yoeli, & Nowak, 2015 ). The Envelope Game\n296\nM. Hoffman et al.\nFig. 3 A single stage of the Envelope Game\nis a repeated game with two players. In each round, player 1 receives a sealed envelope,\nwhich contains a card stating the costs of cooperation (high temptation to\ndefect vs. low temptation to defect). The temptation is assigned randomly and is\nusually low. Player 1 can choose to look inside the envelope and thus find out the\nmagnitude of the temptation or choose not to look. Then player 1 decides to cooperate\nor to defect. Subsequently, player 2 can either continue to the next round or end\nthe game. As in the Repeated Prisoner’s Dilemma, the interaction repeats with a\ngiven likelihood, and if it does, an envelope is stuffed with a new card and presented\nto player 1, etc.\nIn this model, as long as temptations are rare, large, and harmful to player 2, it is\na Nash equilibrium for player 1 to “cooperate without looking” in the envelope and\nfor player 2 to continue if and only if player 1 has cooperated and not looked. We\nrefer to this as the cooperate without looking (CWOL) equilibrium. 2 This equilibrium\nemerges in agent-based simulations of evolution and learning processes. 3 Notice\nthat if player 1 could not avoid looking inside the envelope, or player 2 could not\nobserve whether player 1 looked, there would not be a cooperative equilibrium\nsince player 1 would benefit by deviating to defection in the face of large temptations.\nNot looking permits cooperative equilibria in the face of large temptations.\nThe Envelope Game is meant to capture the essential features of many interesting\naspects of our morality, as described next.\nAuthentic Altruism . Many have asked whether “[doing good is] always and exclusively\nmotivated by the prospect of some benefit for ourselves, however subtle”\n(Batson, 2014 ), for example, the conscious anticipation of feeling good (Andreoni,\n2\nTechnically, the conditions under which we expect players to avoid looking and attend to looking\nare ch > a /(1 − w ) > c lp + ch (1 − p ) and bp + d (1 − p ) < 0), where ch and cl are the magnitudes of the\nhigh and low temptations, respectively; p is the likelihood of the low temptation; a /(1 − w ) is the\nvalue of a repeated, cooperative interaction to player 1; and bp + d (1 − p ) is the expected payoff to\nplayer 2 if player 1 only cooperates when the temptation is low.\n3\nThe simulations employ numerical estimation of the replicator dynamics for a limited strategy\nspace: cooperate without looking, cooperate with looking, look and cooperate only when the temptation\nis low, and always defect for player 1, and end if player 1 looks, end if player 1 defects, and\nalways end for player 2.\nMorality Games\n297\n1990 ), avoidance of guilt (Cain, Dana, & Newman, 2014 ; Dana, Cain, & Dawes,\n2006 ; DellaVigna, List, & Malmendier, 2012 ), anticipation of reputational benefits\nor reciprocity (as Plato’s Glaucon suggests, when he proffers that even a pious man\nwould do evil if given a ring that makes him invisible; Trivers, 1971 ). At the extreme,\nthis amounts to asking if saintly individuals such as Gandhi or Mother Teresa were\nmotivated thus, or if they were “authentic” altruists who did good without anticipating\nany reward and would be altruistic even in the absence of such rewards.\nCertainly, religions advocate doing good for the “right” reasons. In the Gospel of\nMatthew, Chapter 6, Jesus advocates, “Be careful not to practice your righteousness\nin front of others to be seen by them. If you do, you will have no reward from your\nFather in heaven,” after which he adds, “But when you give to the needy, do not let\nyour left hand know what your right hand is doing, so that your giving may be in\nsecret. Then your Father, who sees what is done in secret, will reward you.”\nThe Envelope Game suggests authentic altruism is indeed possible: By focusing\nentirely on the benefits to others and ignoring the benefits to themselves, authentic\naltruists are trusted more, and the benefits from this trust outweigh the risk of, for\nexample, dying a martyr’s death. Moreover, this model helps explain why we think\nso highly of authentic altruists, as compared to others who do good, but with an\nulterior motive (consider, as an example, the mockery Sean Penn has faced for\nshowing up at disaster sites such as Haiti and Katrina with a photographer in tow).\nPrinciples . Why do we like people who are “principled” and not those who are\n“strategic”? For example, we trust candidates for political office whose policies are\nthe result of their convictions and are consistent over time and distrust those whose\npolicies are carefully constructed in consultation with their pollsters and who “flipflop”\nin response to public opinion (as caricatured by the infamous 2004 Republican\npresidential campaign television ad showing John Kerry windsurfing and tacking\nfrom one direction to another). CWOL offers the following potential explanation.\nSomeone who is strategic considers the costs and benefits to themselves of every\ndecision and will defect when faced with a large temptation, whereas someone who\nis guided by principles is less sensitive to the costs and benefits are to themselves\nand thus less likely to defect. Imagine our flip-flopping politician was once against\ngay marriage but supports it now that it is popular. This indicates the politician is\nunlikely to fight for the cause if it later becomes unpopular with constituents or risks\nlosing a big donor. Moreover, this model may help explain why ideologues that are\nwholly devoted to a cause (e.g., Hitler, Martin Luther King, and Gandhi) are able to\nattract so many followers.\nDon ’t Use People . Recall Kant’s second formulation of the categorical imperative:\n“Act in such a way that you always treat humanity, whether in your own person or\nin the person of any other, never simply as a means but always at the same time as\nan end.” In thinking this through, let’s again consider dwarf tossing. Many see it as\na violation of dwarfs’ basic dignity to use them as a means for amusement, even\nthough they willingly engage in the activity for economic gain. Our aversion to\nusing people may explain many important aspects of our moral intuitions, such as\n298\nM. Hoffman et al.\nwhy we judge torture as worse than imprisonment or punishment (torture is harming\nsomeone as a means to obtaining information) and perhaps one of the (many) reasons\nwe oppose prostitution (prostitution is having sex with someone as a means to\nobtaining money). The Envelope Game clarifies the function of adhering to this\nmaxim. Whereas those who treat someone well as means to an end would also\nmistreat them if expedient, those who treat someone well as an end can be trusted\nnot to mistreat them when expedient.\nAttention to Motives . The previous two applications are examples of a more general\nphenomenon: that we judge the moral worth of an action based on the motivation\nof the actor, as argued by deontological ethicists, but contested by\nconsequentialists. The deontological argument is famously invoked by Kant:\n“Action from duty has its moral worth not in the purpose to be attained by it but in\nthe maxim in accordance with which it is decided upon, and therefore does not\ndepend upon the realization of the object of the action but merely upon the principle\nof volition in accordance with which the action is done without regard for any object\nof the faculty of desire” (Kant, 1997 ). These applications illustrate that we attend to\nmotives because they provide valuable information on whether the actor can be\ntrusted to treat others well even when it is not in her interest.\nAltruism Without Prospect of Reciprocation . CWOL also helps explain why people\ncooperate in contexts where there is no possibility of reciprocation, such as in\none-shot anonymous laboratory experiments like the dictator game (Fehr &\nFischbacher, 2003 ), as well as when performing heroic and dangerous acts. Consider\nsoldiers who throw themselves on a grenade to save their compatriots or stories like\nthat of Liviu Librescu, a professor at the University of Virginia and a Holocaust survivor,\nwho saved his students during a school shooting. When he heard the shooter\ncoming toward his classroom, Librescu stood behind the door to his classroom,\nexpecting that when the shooter tried to shoot through the door, it would kill him and\nhis dead body would block the door. Mr. Librescu, clearly, did not expect this act to\nbe reciprocated. Such examples have been used as evidence for group selection\n(Wilson, 2006 ), but can be explained by individuals “not looking” at the chance of\nfuture reciprocation. Consistent with this interpretation, cooperation during extreme\nacts of altruism is more likely to be intuitive than deliberative (Rand & Epstein,\n2014 ), and those who cooperate without considering the prospect of reciprocation\nare more trusted (Critcher, Inbar, & Pizarro, 2013 ). We also predict that people are\nmore likely to cooperate intuitively when they know they are being observed.\nThe Omission–Commission Distinction\nand Higher-Order Beliefs\nWe explain the omission –commission distinction and the means–by-product distinction\nby arguing that these moral intuitions evolved in contexts where punishment is\ncoordinated. Then , even when intentions are clear to one witness for omissions and\nby-products , a witness will think intentions are less clear to the other witnesses .\nMorality Games\n299\nWhy don’t we consider it murder to let someone die that we could have easily\nsaved? For example, we sometimes treat ourselves to a nice meal at a fancy restaurant\nrather than donating the cost of that meal to a charity that fights deadly diseases.\nThis extreme example illustrates a general phenomenon: that people have a tendency\nto assess harmful commissions (actions such as killing someone) as worse, or\nmore morally reprehensible, than equally harmful omissions (inactions such as letting\nsomeone die). Examples of this distinction abound, in ethics (we assess withholding\nthe truth as less wrong than lying (Spranca, Minsk, & Baron, 1991 )), in law\n(it is legal to turn off a patient’s life support and let the patient die, as long as one\nhas the consent of the patient’s family; however, it is illegal to assist the patient in\ncommitting suicide even with the family’s consent), and in international relations.\nFor example, consider the Struma, a boat carrying Jewish refugees fleeing Nazi\npersecution in 1942. En route to Palestine, the ship’s engine failed, and it was towed\nto a nearby port in Turkey. At the behest of the British authorities then in control of\nPalestine, passengers were denied permission to disembark and find their way to\nPalestine by land. For weeks, the ship sat at port. Passengers were brought only\nminimal supplies, and their requests for safe haven were repeatedly denied by the\nBritish and others. Finally, the ship was towed to known hostile waters in the Black\nSea, where it was torpedoed by a Russian submarine almost immediately, killing\n791 of 792 passengers. Crucially, though, the British did not torpedo the ship themselves\nor otherwise execute passengers—an act of commission that they and their\nsuperiors would undoubtedly have found morally reprehensible.\nWhy do we distinguish between transgressions of omission and commission? To\naddress this question, we present a simple game theory model based on the insight\nby DeScioli, Bruening, and Kurzban ( 2011 ). The intuition can be summarized in\nfour steps:\n1. We note that moral condemnation motivates us to punish transgressors. Such\npunishment is potentially costly, e.g., due to the risk of retaliation. We expect\npeople to learn or evolve to morally condemn only when such costs are worth\npaying.\n2. Moral condemnation can be less costly when others also condemn, perhaps\nbecause the risk of retaliation is diffused, because some sanctions do not work\nunless universally enforced or, worse, because others may sanction individuals\nthey believe wrongly sanctioned. This can be modeled using any game with\nmultiple Nash equilibria, including the Repeated Prisoner’s Dilemma and the\nSide- Taking Game. The Coordination Game is the simplest game with multiple\nequilibria, so we present this game to convey the basic intuition. In the\nCoordination Game, there are two players who each simultaneously choose\nbetween two actions, say punish and don’t punish. The key assumption is that\neach player prefers to do what she expects the other to do, which can be captured\nby assuming each receives a if they both punish, d if neither punish, b < d if one\npunishes and the other does not, and c < a if one does not punish while the other\ndoes (Fig. 4 ).\n3. Transgressions of omission that are intended are difficult to distinguish from\nunintended transgression, as is the case when perpetrators are simply not paying\n300\nM. Hoffman et al.\nFig. 4 The Coordination\nGame. In our applications, A\nstands for punish, and B\nstands for don’t punish\nattention or do not have enough time to react with better judgment (DeScioli\net al., 2011 ). Relative to the example of the tennis player with the allergy\ndescribed above, it is usually hard to distinguish between a competitor who does\nnot notice his opponent orders the dish with the allergen versus one who notices\nbut does not care. In contrast, transgressions of commission must be intended\nalmost by definition.\n4. Suppose the witness knows an omission was intentional: In the above example,\nthe tennis player’s opponent’s allergy is widely known, and the witness saw the\nplayer watch his opponent order the offending dish, had time to react, thought\nabout it, but did not to say anything. The witness suspects that others do not\nknow the competitor was aware his opponent ordered the dish, but believes the\ntennis player should be condemned for purposely withholding information from\nhis competitor. However, since the witness does not wish to be the sole condemner,\nshe is unlikely to condemn. In contrast, when a witness observes a transgression\nof commission (e.g., the player recommends the dish), the witness is\nrelatively confident that others present interpret the transgression as purposely\nharmful, since his recommendation reveals that the player was obviously paying\nattention and therefore intended to harm his opponent. So, if all other individuals\npresent condemn the tennis player when they observe the commission, each does\nnot anticipate being the sole condemner.\nFor the above result to hold, all that is needed is the following: (1) The more the costs\nof punishment decrease, the more others punish and (2) omissions are usually unintended\n(Dalkiran, Hoffman, Paturi, Ricketts, & Vattani, 2012 ; Hoffman et al., 2015 ). 4\n4\nIn fact, even if one knows that others know that the transgression was intended, omission will still\nbe judged as less wrong, since the transgression still won’t create what game theorists call common\np-belief, which is required for an event to influence behavior in a game with multiple equilibria.\nMorality Games\n301\nThis explanation for the omission–commission distinction leads to two novel\npredictions: First, for judgments and emotions not evolved to motivate witnesses to\npunishment but to, say, motivate witnesses to avoid dangerous partners (such as the\nemotion of fear; in contrast to anger or moral disgust), the omission–commission\ndistinction is expected to be weaker or disappear altogether. Second, for transgressions\nof omission that, without any private information, can be presumed intentional\n(such as a mother who allows her child to go hungry or a person who does not give\nto a charity after being explicitly asked), we would not expect much of an omission–commission\ndistinction in moral condemnation.\nAs with the all models discussed in this chapter, the game theoretic explanation\nfor the omission–commission distinction does not rest on rational, conscious, strategic\ncalculation. In fact, in this particular case, all reasonable evolutionary dynamic\nmodels lead away from punishing omissions. The fact that the above results do not\nrest on rational, strategic thinking is particularly important in this setting since there\nis evidence that the distinction between omissions and commissions is not determined\ndeliberately but rather intuitively (Cushman, Young, & Hauser, 2006 ) and\nappears to be evolved (DeScioli et al., 2011 ) and that consciously considering what\nothers believe is an onerous process (Camerer, 2003 ; Epley, Keysar, Van Boven, &\nGilovich, 2004 ; Hedden & Zhang, 2002 ).\nThis same model can explain several other puzzling aspects of our morality. The\nfirst is the means –by-product distinction. This distinction has been documented in\nstudies that ask respondents to judge the following variants of the classic “trolley”\nproblem. In the standard trolley “switch” case (Foot, 1967), a runaway trolley is\nhurtling toward a group of five people. To prevent their deaths, the trolley must be\nswitched onto a side track where it will kill an innocent bystander. In studies using\nthis case, the vast majority of subjects choose the utilitarian option, judging it permissible\nto cause the death of one to save five (e.g., Cushman et al., 2006 ; Mikhail,\n2007). In the “footbridge” variant (Thomson, 1976 ), the trolley is hurtling toward\nthe group of five people, but the switch to divert it is inoperable. The only way to\nsave the five is to push a man who is wearing a heavy backpack off a bridge onto the\ntrack, thereby slowing the trolley enough so the five can escape, but killing the man.\nIn contrast to the standard switch version, where causing the death of one person is\nbut a by-product of the action necessary to save five, most subjects in the footbridge\ncase find it morally impermissible to force the man with the backpack onto the\ntracks (Cushman et al., 2006 ; DeScioli, Gilbert, & Kurzban, 2012 )—that is, when\nthe man is used as a means to saving the five—even though the consequences are the\nsame, and the decision to act was made knowingly and deliberately in both cases.\nSuch effects are found in less contrived situations, as well. Consider the real-life\ndistinction between terrorism, in which civilian casualties are used a means to a\npolitical goal, and anticipated collateral damage, which is a by-product of war, even\nwhen the same number of civilians are knowingly killed and the same political ends\nare desired (say increased bargaining power in a subsequent negotiation).\nThe explanation again uses “higher-order beliefs” and is based on the key insight\nin DeScioli et al. ( 2011 ) and formalized in Dalkiran et al. ( 2012 ) and Hoffman et al.\n( 2015 ): When the harm is done as a by-product, the harm is not usually anticipated.\n302\nM. Hoffman et al.\nSo even when a witness knows that the perpetrator anticipated the harm, the witness\nbelieves other witnesses will not be aware of this and will presume the harm was not\nanticipated by the perpetrators. For instance, suppose we observe Israel killing civilians\nas a by-product of a strategic raid on Hamas militants. Even if we knew Israel\nhad intelligence that confirmed the presence of civilians, we might not be sure others\nwere privy to this information. On the other hand, when the harm is done as a\nmeans, the harm must be anticipated, since otherwise the perpetrator would have no\nmotive to commit the act. Why would Hamas fire rockets at civilian towns with no\nmilitary presence if Hamas does not anticipate a chance of civilian casualties?\nConsequently, it is Nash equilibrium to punish harm done as a means but not harm\ndone as a by-product.\nSimilar arguments can be made for why we find direct physical transgressions\nworse than indirect ones, a moral distinction relevant to, for instance, the United\nStates’ current drone policy. Cushman et al. ( 2006 ) found that subjects condemn\npushing a man off a bridge (to stop a train heading toward five others) more harshly\nthan flipping a switch that leads the man to fall through a trap door. Pushing the\nvictim with a stick is viewed as intermediate in terms of moral wrongness. Such\nmoral wrongness judgments are consistent with considerations of higher-order\nbeliefs: When a man is physically pushed, any witness knows the pushing was\nintended, but when a man is pushed with a stick some might not realize this, and\neven those who realize it might suspect others will not. Even more so when a button\nis pressed that releases a trap door.\nIt is worth noting that the above argument does not depend on a specific model\nof punishment, as in DeScioli and Kurzban’s ( 2009 ) Side-Taking Game. The above\nmodel also makes the two novel predictions enumerated above, but nevertheless\ncaptures the same basic insight. It is also worth noting the contrast between the\nabove argument and that of Cushman et al. ( 2006 ) and Greene et al. ( 2009 ), whose\nmodels rest on ease of learning or ease of mentally simulating a situation. It is not\nobvious to us how those models would explain that the omission–commission and\nmeans–by-product distinctions seem to depend on priors or be unique to settings of\ncoordinated punishment.\nWhy Morality Depends on Categorical Distinctions\nWe explain why our moral intuitions depends so much more strongly on whether a\ntransgression occurred than on how much damage was caused. Our argument again\nuses coordinated punishment and higher-order beliefs : When a categorical distinction\nis violated, you know others know it was violated , but this is not always true for\ncontinuous variables .\nConsider the longstanding norm against the use of chemical weapons. This norm\nrecently made headlines when Bashar al-Assad was alleged to have used chemical\nweapons to kill about a thousand Syrian civilians, outraging world leaders who had\nMorality Games\n303\nbeen silent over his use of conventional weapons to kill over 100,000 Syrian\ncivilians. A Reuters/Ipsos poll at the time found that only 9 % of Americans favored\nintervention in Syria, but 25 % supported intervention if the Syrian government\nforces used chemical weapons against civilians (Wroughton, 2013 ). In the past, the\nUnited States has abided by the norm against the use of chemical weapons even at\nthe expense of American lives: In WWII, Franklin D. Roosevelt chose to eschew\nchemical weapons in Iwo Jima even though, as his advisors argued at the time, their\nuse would have saved thousands of American lives. It might even have been more\nhumane than the flame-throwers that were ultimately used against the Japanese\n(“History of Chemical Weapons,” 2013). We say that the norm against chemical\nweapons is a categorical norm because those who abide by it consider whether a\ntransgression was committed (did Assad use chemical weapons?), rather than focusing\nentirely on how much harm was done (how many civilians did Assad kill?).\nOther norms are similarly categorical. For instance, in the introduction to this chapter,\nwe noted that across cultures and throughout history, the norm against murder\nhas always been categorical: We consider whether a life was terminated, not the loss\nof useful life years. Likewise, discrimination (e.g., during Jim Crow) is typically\nbased on categorical definitions of race (the “one drop rule”) and not, say, the darkness\nof skin tone. Human rights are also categorical. A human rights violation\noccurs if someone is tortured or imprisoned without trial, regardless of whether it\nwas done once or many times and regardless of whether the violation was helpful in,\nsay, gaining crucial information about a dangerous enemy or an upcoming terror\nattack. We even assign rights in a categorical way to all Homo sapiens and not based\non intelligence, sentience, ability to feel pain, etc.\nWhy is it that we attend to such categorical distinctions instead of paying more\nattention to the underlying continuous variable? We use game theory to explain this\nphenomenon as follows: Suppose that two players (say, the United States and\nFrance) are playing a Coordination Game in which they decide whether to punish\nSyria, and each wants to sanction only if the other sanctions. We assume the United\nStates does not want to levy sanctions unless it is confident France will as well,\nwhich corresponds to an assumption on the payoffs of the game (if we reverse this\nassumption, it changes one line in the proof, but not the result).\nWe model the underlying measure of harm as a continuous variable (in our\nexample, it is the number of civilians killed). For simplicity, we assume this variable\nis uniformly distributed, which means Assad is equally likely to kill any number of\npeople. This assumption is, again, not crucial, and we will point out the line in the\nproof that it affects. Importantly, we assume that players do not directly observe the\ncontinuous variable, but instead receive some imperfect signal (e.g., the United\nStates observes the body count by its surveyors).\nImagine a norm that dictates that witnesses punish if their estimate of the harm\nfrom a transgression is above some threshold (e.g., levy sanctions against Syria if\nthe number of civilians killed is estimated to be greater than 100,000). As it turns\nout, this is not a Nash equilibrium. To see why, consider what happens when the\nUnited States gets a signal right at the threshold. The United States thinks there is a\n304\nM. Hoffman et al.\n50 % chance that France’s estimates are lower than its own 5 and, thus, that there is\na 50 % chance that France’s estimates are lower than the threshold. This further\nimplies that the United States assesses only a 50 % chance that France levies sanctions,\nso the United States is not sufficiently confident that France will sanction, to\nmake it in the United States’s interest to sanction.\nWhat we have shown so far is that for a threshold of 100,000, it is in the interest\nof the United States to deviate from the strategy dictated by the threshold norm\nwhen it gets a signal at the threshold. This means that 100,000 is not a viable threshold,\nand (since 100,000 was chosen arbitrarily) there is no Nash equilibrium in\nwhich witnesses punish if their estimate of the harm from a transgression is above\nsome arbitrary threshold.\nIt should be noted that this result only requires that there are sufficiently many\npossibilities, not that there is in fact a continuum. Neither does it require that the\ndistribution is uniform nor that the Coordination Game is not affected by the behavior\nof Assad. The only crucial assumptions are that the distribution is not too skewed\nand that the payoffs are not too dependent on the behavior of Assad (for details, see\nDalkiran et al., 2012 ; Hoffman, Yoeli, & Dalkiran, 2015 ).\nWhat happens if such norms are learned or evolved and subject to selection?\nSuppose there is a norm to attack whenever more than 100,000 civilians are killed.\nPlayers will soon realize that they should not attack unless, say, 100,100 civilians\nare killed. Then, players will learn not to attack when they estimate 100,200 civilians\nare killed and so on , indefinitely. Thus, every threshold will eventually\n“unravel,” and no one will ever attack. 6\nNow let’s consider a categorical norm, for example, the use of chemical weapons.\nWe again model this as a random variable, though this time, the random variable\ncan only take on two values (0 and 1), each with some probability. Again,\nplayers do not know with certainty whether the transgression occurred, but instead\nget a noisy signal. In our example, the signal represents France or the United States’s\nassessment of whether Assad used chemical weapons, and there is some likelihood\nthe assessors make mistakes: They might not detect chemical weapons when they\nhad been used or might think they have detected chemical weapons when none had\nbeen used.\nUnlike with the threshold norm, provided the likelihood of a mistaken signal is\nnot too high, there is a Nash equilibrium where both players punish when they\nreceive a signal that the transgression occurred. That is, the United States and France\neach levy sanctions if their assessors detect chemical weapons. This is because\nwhen the United States detects chemical weapons, the United States believes France\n5\nThis is where the assumption of a uniform distribution comes in. Had we instead assumed, for\ninstance, that the continuous variable is normally distributed, then it would not be exactly 50–50\nbut would deviate slightly depending on the standard deviation and the location of the threshold.\nNevertheless, the upcoming logic will still go through for most Coordination Games, i.e. any\nCoordination Game with risk dominance not too close to .5.\n6\nAs with omission, this follows from iterative elimination of strictly dominated strategies (see\nHoffman et al., 2015 , for details).\nMorality Games\n305\nlikely detected them and will likely levy sanctions. So the United States’s best\nresponse is to levy sanctions. Similarly, if the United States does not detect chemical\nweapons, it expects France did not and will not levy sanctions, so the United\nStates is better off not levying them.\nThis result is useful for evaluating whether it is worthwhile to uphold a norm.\nThe Obama administration was harshly criticized for threatening to go to war after\nthe Assad regime used chemical weapons but not earlier, although the regime had\nalready killed tens of thousands of civilians. The model clarifies that Obama’s position\nwas not as inconsistent as his critics had charged: The norm against chemical\nweapons may be worth enforcing since it is sustainable, whereas norms against\ncivilian casualties are harder to sustain and hence might not be worthwhile to\nenforce.\nLet’s return to some more of our motivating examples. Our model can explain\nwhy we define murder categorically: It is not possible to punish differently for different\namount of quality life years taken, but it is possible to punish differentially for\na life taken. As with omission–commission, however, we do expect sadness or grief\nto depend greatly on life years lost, even if the punishment or moralistic outrage will\nbe less sensitive. This is a prediction of the model that, as far as we know, has yet to\nbe tested.\nSimilarly, the “one-drop” rule is a categorical norm, so it can be socially enforced\nin an apartheid society. In contrast, consider a rule that advocates giving up one’s\nseat for someone with lighter skin. Since this is based on a threshold in a continuous\nvariable, while it might be enforceable by a unilateral authority, it cannot be enforced\nby “mob rule.” Other forms of discrimination, such as discriminating against the\nless attractive, or the less tall, or the elderly, all being continuous variables, cannot\nbe socially enforced via coordinated punishment, and hence, we expect such discrimination\nto be of a different form. In particular, it will not be based not on punishing\nviolators. For example, male CEOs might still prefer young attractive female\nsecretaries, and taller men are more likely to be hired as CEOs, not because of\ncoordinated rewards or punishment but because those who hire the CEOs or secretaries\nare likely to be satisfying their own preferences or doing what they expect will\nlead to higher profits.\nLikewise, the number of victims tortured by a regime or the number of lives\nsaved by torturing is continuous. Thus, a regime cannot be punished by a coordinated\nattack by other countries or by a coordinated rebellion by its citizens based on\nthe number of people tortured or the paucity of reasons for such torture. But, a\nregime can be attacked or overthrown depending on whether a physical harm was\ninflicted on a citizen by the state. Hence, human rights are treated as inalienable,\neven in the absence of an a priori justification for this nonutilitarian norm. And why\nare human rights ascribed to all living Homo sapiens ? Perhaps not because of a\ngood logical a priori argument, but simply because violations of human rights are\nenforceable by coordinated punishment, but no regime can be punished for harming\nany “person” of less than a certain degree of consciousness.\nFinally, here is one last application. The model might also explain why revolutions\nare often caused by categorical events, such as a new tea tax or a single, widely\n306\nM. Hoffman et al.\npublicized self-immolation, and not a breach of a threshold in, say, the quality of life\nof citizens or the level of corruption. This explanation requires simply that we recognize\nrevolutions as a coordination problem (as argued in Morris & Shin, 2002 ;\nChwe, 2013 ), where each revolutionary chooses whether to revolt, and each is better\noff revolting only if sufficiently many others revolt.\nQuirks of Altruism and the Repeated Prisoner’s Dilemma\nwith Incomplete Information\nThe Repeated Prisoner ’s Dilemma has famously been used as an explanation for\nthe evolution of cooperation among non-kin (Axelrod & Hamilton, 1981 ; Dawkins,\n2006 ; Pinker, 2003 ; Trivers, 1971 ). In this section , we show how the same basic\nmodel can be used to explain many of the quirky features of our pro-social preferences\nand ideologies .\nRecall that in the Prisoner’s Dilemma, each of two players simultaneously\nchooses whether to cooperate. Cooperation reduces a player’s own payoffs by c > 0\nwhile increasing the other’s payoffs by b > c . The only Nash equilibrium is for neither\nplayer to cooperate. In the Repeated Prisoner’s Dilemma, the players play a\nstring of Prisoner’s Dilemmas. That is, after the players play a Prisoner’s Dilemma,\nthey learn what their opponent did and play another Prisoner’s Dilemma against the\nsame opponent with probability δ (and the game ends with probability 1 − δ ). As is\nwell known in the evolutionary literature, there are equilibria in which players end\nup cooperating, provided δ > c /b . In all such equilibria, cooperation is sustained\nbecause any defection by one player causes the other player to defect. This is called\nreciprocity. As the reader is surely familiar, there is ample evidence for the Repeated\nPrisoner’s Dilemma as a basis for cooperation from computer simulations (e.g.,\nAxelrod, 1984 ) and animal behavior (e.g., Wilkinson, 1984 ). The model can be\nextended to explain contributions to public goods if, after deciding whether to contribute\nto a public good, players play a Repeated Prisoner’s Dilemma (see, e.g.,\nPanchanathan & Boyd, 2004 ) (Fig. 5 ).\nThe key to understanding these quirks is that players often have incomplete\ninformation. For example:\n1. Players do not always observe contributions. It is intuitive that, for cooperation\nto occur in equilibrium, contributions need to be observed with sufficiently high\nprobability.\n2. Others cannot always tell whether a player had an opportunity to contribute. For\ndefection to be penalized, it must be the case that others can tell that a player had\nthe opportunity to cooperate and did not (i.e. the player should not be able to hide\nthe fact that there was an opportunity to cooperate).\n3. Sometimes, there are two ways to cooperate, and one has a higher benefit, b .\nThen, the only way this more effective type of cooperation can be sustained in\nequilibrium is if others know which cooperative act is more effective.\nMorality Games\n307\nFig. 5 The Repeated Prisoner’s Dilemma. Two players play a Prisoner’s Dilemma. They each\nobserve the other’s action, then, with probability δ , play another Prisoner’s Dilemma against the\nsame opponent, etc.\nTechnically, for the second and third point, what is needed is common knowledge\nthat a player had an opportunity to cooperate or of the more effective means of\ncooperation. If observers were to know one purposely chose to defect or chose the\nless cooperative act, but they do not know that others know this, then observers\nthink others will think punishment is not warranted, and observers will not punish.\nThe argument is analogous to the discussion of higher-order beliefs in the omission–commission\nsubsection and formalized in Dalkiran et al. ( 2012 ) and Hoffman\net al. ( 2015 ).\nInterpreting the Quirks of Altruism\nBelow we discuss some of the quirky features of altruism identified by economists\nand psychologists. In each case, we will argue that these features might be puzzling,\nbut not when viewed through the lens of the above model:\nInsensitivity to Effectiveness . We are surprisingly insensitive to the impact of our\ncharitable contributions. We vote because we “want to be a part of the democratic system,”\nor we “want to make a difference,” despite the fact that our likelihood of swinging\nan election (even in a swing state) is smaller than our likelihood of being struck by\nlightning (Gelman, Silver, & Edlin, 2012 ). Why is our desire to “make a difference” or\n“be a part of the system” immune to the actual difference we are making? Our charitable\ncontributions or volunteer efforts suffer from the same insensitivity. Why does\nanyone give money or volunteer time to Habitat for Humanity? The agency flies high\nearners who have never held a hammer halfway across the world to build houses that\nwould be substantially more cheaply built by local experts funded by the high earners.\nExperimental evidence demonstrates our insensitivity: Experimental subjects are willing\nto pay the same amount to save 2000, 20,000, or 200,000 birds (Desvousges et al.,\n2010 ). Likewise, when donors are told their donations will be matched, tripled, or\nquadrupled, they donated identical amounts (Karlan & List, 2006 ). Why do we give so\nmuch, but do not ensure our gifts have a large impact?\nThe explanation follows directly from the above model: It is often the case that\nobservers do not know which acts are effective and which are not and, certainly, this\n308\nM. Hoffman et al.\nusually is not commonly known. Thus, they will not reward or punish based on\neffectiveness, and we ourselves will not attend to effectiveness in equilibrium. This\nexplanation suggests that if we want to increase efficacy of giving, we ought to\nfocus on making sure donors’ friends and colleagues are aware of the efficacy of\ndifferent options. In fact, this is perhaps more important than informing the donor\nof efficacy, since the donor will be motivated to uncover efficacy herself.\nMagnitude of the Problem . We are surprisingly unaware of and unaffected by the\nmagnitudes of the problems we contribute to solving. How many of those who participated\nin the recent ALS Ice Bucket Challenge have even the vaguest sense of the\nnumber of ALS victims? (Answer: about 1/100th the victims of heart disease.) How\nmuch happier would these individuals have been if the number of ALS victims were\ncut in half? Multiplied by 100? The same questions could be asked about AIDS or\ncleft lips. If we were actually motivated by our desire to rid the world of such afflictions\nas we often proclaim, then we would be happier if there were fewer afflicted\nindividuals and less happy if there were more. But we are not even aware of these\nnumbers, let alone affected by them. This suggests an alternative motivation than\nthe one we proclaim.\nOn the other hand, if we give in order to gain social rewards, it does not matter\nwhether the problem is large or small, provided others recognize it as a problem and\nthe social norm is to give. If our learned or evolved preferences were drastically\nimpacted by the magnitude of the crises, we would be sensitive to whether the problem\nwas solved, perhaps motivating us to ensure that others solve it, which we\nwould not get credit for, or perhaps motivating us to devote too much of our\nresources to solving it, beyond what we would actually get rewarded for.\nObservability . There is overwhelming evidence that people give more when their\ngifts are observed. Much of this evidence comes from the lab, where it has been\ndemonstrated a myriad of ways (e.g., Andreoni & Petrie, 2004 ; Bolton, Katok, &\nOckenfels, 2005 ; List, Berrens, Bohara, & Kerkvliet, 2004 ). For instance, when\nparticipants play a public goods game in the laboratory for money, their contributions\nare higher when they are warned that one subject will have to announce to the\nroom of other participants how much they contributed (List et al., 2004 ). However,\nevidence also comes from real-world settings, which find large effects in settings as\ndiverse as blood donation (Lacetera & Macis, 2010 ), blackout prevention (Yoeli,\nHoffman, Rand, & Nowak, 2013 ), and support for national parks (Alpizar, Carlsson,\n& Johansson-Stenman, 2008 ). In Switzerland, voting rates fell in small communities\nwhen voters were given the option to vote by mail (Funk, 2006 ), which makes\nit harder to tell who did not vote, even though it also makes it easier to vote. In fact,\nour willingness to give more when observed extends to subtle, subconscious cues of\nbeing observed: People give twice as much in dictator games when there are markings\non the computer screen that vaguely represent eyes (Haley & Fessler, 2005 ),\nand they are more likely to pay for bagels in their office when the payment box has\na picture of eyes above it (Bateson, Nettle, & Roberts, 2006 ).\nThese results should not surprise anyone who believes our pro-social tendencies are\ninfluenced by reputational concerns (though the magnitudes are surprisingly large).\nMorality Games\n309\nThe effectiveness of subconscious cues of observability points to a primary role for\nreputations in our learned or evolved proclivities toward pro-social behavior. The\nlarge impact of subtle cues of observability, however, calls into question alternative\nexplanations not based on reputations.\nExplicit Requests . When we are asked directly for donations, we give more than if\nwe are not asked, even though no new information is conveyed by the request. In a\nstudy of supermarket shoppers around Christmas time, researchers found that passersby\nwere more likely to give to the Salvation Army if volunteers not only rang\ntheir bell but explicitly asked for a donation (Andreoni, Rao, & Trachtman, 2011 ).\nIf our motive is to actually do good, or perhaps proximally to feel good by the act of\ngiving, we should not be impacted by an explicit request.\nHowever, if we evolved or learned to give in order to gain rewards or avoid punishment\nas described above, then we ought to be more likely to give when, if we did\nnot give, it would be common knowledge that we had the option to give and chose\nnot to. The explicit request makes the denial common knowledge.\nIt is worth emphasizing that our evolved intuition to respond to explicit asks may\nbe (mis)applied to individual settings that lack social rewards. Imagine you are\napproached by a Salvation Army volunteer in front of a store in a city where you are\nvisiting for one day only. A literal reading of the model would suggest that you\nshould be no more likely to respond to an explicit request. But it is more realistic to\nexpect that if your pro-social preferences were learned or evolved in repeated interactions\nthen applied to this new setting, you would respond in a way that is not\noptimal for this particular setting and nonetheless give more when explicitly asked\n(just as our preferences for sweet and fatty foods, which evolved in an environment\nwhere food was scarce, lead us to overeat now that food is abundant).\nAvoiding Situations in Which We Are Expected to Give . In the same supermarket\nstudy, researchers discovered that shoppers were going out of their way to exit the\nstore through a side door, to avoid being asked for a contribution by the Salvation\nArmy volunteers. In another field experiment, those who were warned in advance\nthat a solicitor would come to the door asking for charitable donations were more\nlikely to not be home. The researchers estimated that among those who gave, 50 %\nwould have avoided being home if warned in advance of the solicitor’s time of\narrival (DellaVigna et al., 2012 ). In a laboratory analog, subjects who would have\notherwise given money in a $10 dictator game were willing to pay a dollar to keep\nthe remaining nine dollars and prevent the recipient from knowing that a dictator\ngame could have been played (Dana et al. 2006 ). If our motive were to have an\nimpact, we would not pay to avoid putting ourselves in a situation where we could\nhave such an impact. Likewise, if our motive were to feel good by giving, we would\nnot pay to avoid this feeling.\nIn contrast, if we evolved or learned to give in order to gain rewards or avoid\npunishment, then we would pay to avoid situations where we are expected to give.\nAgain, this would be true even if, in this particular setting, we were unlikely to actually\nbe punished.\n310\nM. Hoffman et al.\nNorms . People are typically conditionally cooperative , meaning that they are willing\nto cooperate more when they believe others contribute more. For example, students\nasked to donate to a university charity gave 2.3 percentage points more when\ntold that others had given at a rate of 64 % than when they were told giving rates\nwere 46 % (Frey & Meier, 2004 ). Hotel patrons were 26 % more likely to reuse their\ntowels when informed most others had done the same (Goldstein, Cialdini, &\nGriskevicius, 2008 ). Households have been shown to meaningfully reduce electricity\nconsumption when told neighbors are consuming less, both in the United States\n(Ayres, Raseman, & Shih, 2012 ) and in India (Sudarshan, 2014 ).\nSuch conditional cooperation is easily explained by the game theory model:\nWhen others give, one can infer that one is expected to give and may be socially\nsanctioned if one does not.\nStrategic Ignorance . Those at high risk of contracting a sexually transmitted disease\n(STD) often go untested, presumably because if they knew they had the STD,\nthey would feel morally obliged to refrain from otherwise desirable activity that\nrisks spreading the STD. Why is it more reproachable to knowingly put a sexual\npartner at risk when one knows one has the STD than to knowingly put a sexual\npartner at risk by not getting tested? There is evidence that we sometimes pursue\nstrategic ignorance and avoid information about the negative consequences of our\ndecisions to others. When subjects are shown two options, one that is better for\nthemselves but worse for their partners and one that is worse for themselves but better\nfor their partners, many choose the option that is better for their partners. But,\nwhen subjects must first press a button (at no cost) to reveal which option is better\nfor their partners, they choose to remain ignorant and simply select the option that\nis best for themselves (Dana, Weber, & Kuang, 2007 ).\nThis quirk of our moral system is again easy to explain with the above model.\nTypically, information about how one’s actions affect others is hard to obtain, so\npeople cannot be blamed for not having such information. When one can get such\ninformation easily, others may not know that it is easy to obtain and will not punish\nanyone who does not have the information. For example, although it is trivially easy\nto look up charities’ financial ratings on websites like charitynavigator.org, few\npeople know this and could negatively judge those that donate without first checking\nsuch websites. And even when others know that one can get this information\neasily, they might suspect that others do not know this, and so avoid punishing,\nsince others won’t expect punishment. To summarize, strategic ignorance prevents\ncommon knowledge of a violation and so is likely to go unpunished. We again\nemphasize that we will be lenient of strategic ignorance, even when punishment is\nnot literally an option.\nNorm of Reciprocity . We feel compelled to reciprocate favors, even if we know\nthat the favors were done merely to elicit reciprocation and even if the favor asked\nin return is larger than the initial one granted (Cialdini 2001). For instance, members\nof Hare Krishna successfully collect donations by handing out flowers to disembarking\npassengers at airports, even though passengers want nothing to do with\nthe flowers: They walk just a few feet before discarding them in the nearest bin.\nMorality Games\n311\nPsychologists and economists sometimes take this “norm” as given, without asking\nwhere it comes from, and a naive reading of Trivers would lead one to think that we\nshould be sensitive to the magnitude of the initial favor and whether it is\nmanipulative.\nHowever, according to the above model, reciprocity is the Nash equilibrium,\neven if the favors are not evenly matched or manipulative, since, in equilibrium, we\nare neither sensitive to such quantitative distinctions nor to whether the initial reciprocity\nwas manipulative, unless these facts are commonly known.\nSelf-Image Concerns . People sometimes play mental tricks in order to appear to\nthemselves as pro-social. For example, in an experiment, subjects will voluntarily take\non a boring task to save another subject from doing it, but if given the option of privately\nflipping a coin to determine who gets the task, they often flip—and flip, and flip\nagain—until the “coin” assigns the task to the other subject (Batson, Kobrynowicz,\nDinnerstein, Kampf, & Wilson, 1997 ). Why would we be able to fool ourselves and\nnot, say, recognize that we are gaming the coin flip? Why do we care what we think\nof ourselves at all? Are there any constraints on how we will deceive ourselves?\nSuch self-image considerations can be explained by noting that our self-image can\nact as a simple proxy, albeit an imperfect one, for what others think of us, and also that\nwe are more convincing to others when we believe something ourselves (Kurzban,\n2012 ; Trivers, 2011 ). This explanation suggests that the ways we deceive ourselves\ncorrespond to quirks described throughout this section—for example, we will absolve\nourselves of remaining strategically ignorant even when it is easy not to, or be convinced\nthat we have done good by voting, even if we cannot swing an election.\nFraming Effects . Whether we contribute is highly dependent on the details of the\nexperiment, such as the choice set (List, 2007 ) and the labels for the different\nchoices (Ross & Ward, 1996 ; Roth, 1995 ). Such findings are often taken as evidence\nthat social preferences cannot be properly measured in the lab (Levitt & List, 2007 ).\nWe believe a more fruitful interpretation is simply that the frame influences\nwhether the laboratory experiment “turns on” our pro-social preferences, perhaps\nby simulating a situation where cooperation is expected (Levitt & List, 2007 ).\nOne-Shot Anonymous Giving : We give in anonymous, one-shot settings, such as\ndictator games. We also sacrifice for others in the real world when there is no chance\nof reciprocation: Heroes jump on grenades to save their fellow soldiers or block the\ndoor to a classroom with their bodies to prevent a school shooter from entering\n(Rand & Epstein, 2014 ). This is often seen as evidence for a role of group selection\n(Fehr & Fischbacher, 2003 ).\nHowever, an alternate explanation is that we do not consider the likelihood of\nreciprocation (Hoffman et al., 2015 ), as described above. To explain the laboratory\nevidence, there are two more possibilities. First, subjects may believe there is some\nchance their identity will be revealed and feel the costs of being revealed as selfish\nare greater than the gains from the experiment (Delton, Krasnow, Cosmides, &\nTooby, 2011 ). Second, we again emphasize that learned or evolved preferences and\nideologies are expected to be applied even in novel settings to which they are not\noptimized.\n312\nM. Hoffman et al.\nConclusion\nIn this chapter we have showed that a single approach–game theory , with the help\nof evolution and learning –can explain many of our moral intuitions and ideologies.\nWe now discuss two implications .\nGroup Selection . Our chapter relates to the debate on group selection, whereby\ngroup level competition and reproduction is supposed to occasionally cause individuals\nto evolve to sacrifice their own payoffs to benefit the group (e.g., Wilson,\n2006 ). One of the primary pieces of evidence cited in support of group selection is\nthe existence of human cooperation and morality (Fehr & Fischbacher, 2003 ; Fehr,\nFischbacher, & Gächter, 2002 ; Gintis, Bowles, Boyd, & Fehr, 2003 ; Haidt, 2012 ;\nWilson, 2010 , 2012 ), in particular: giving in one-shot anonymous laboratory experiments,\nintuitively sacrificing one’s life for the group (jumping on the grenade), and\ncontributions to public goods or charity. However, we have reviewed an alternative\nexplanation for these phenomena that does not rest on group selection. It also yields\npredictions about these phenomena that group selection does not, such as that people\nare more likely to cooperate when they are being observed and there is variance\nin the cost of cooperation. The approach described here also explains other phenomena,\nsuch as categorical norms and ineffective altruism. These lead to social welfare\nlosses, which is suboptimal from the group’s perspective. The categorical norm\nagainst murder, for example, leads to enormous waste when keeping alive, sometimes\nfor years, those who have virtually no chance of a future productive life.\nAdmittedly, despite their inefficiencies, these moral intuitions do not rule out\ngroup selection, since group selection can be weak relative to individual selection.\nBut it does provide a powerful argument that group selection is unnecessary for\nexplaining many interesting aspects of human morality. It also suggests that group\nselection is, indeed, at most, weak. One example that makes this especially clear is\ndiscrete norms. Recall that we argued that continuous norms are not sustainable\nbecause individuals benefit by deviating around the threshold. Notice that this benefit\nis small, since the likelihood that signals are right around the threshold is low.\nGroup selection could easily overwhelm the benefit one would get from deviating\nfrom this Nash equilibrium, suggesting group selection is weak (i.e. there are few\ngroup- level reproductive events, high migration rates, high rates of “mutation” in\nthe form of experimentation among individuals, etc.).\nLogical Justification of Moral Intuitions . In each of the applications above, we\nexplained moral intuitions without referring to existing a priori logical justifications\nby philosophers or others. Our explanation for our sense of rights does not rely on\nLocke’s “state of nature.” No argument we gave rests on God as an orderly designer,\non Platonic ideals, on Kant’s concepts of autonomy and humanity, etc. What does\nthis mean for these a priori justifications? It suggests that they are not the source of\nour morality and are, instead, post hoc justifications of our intuitions (Haidt, 2012 ).\nTo see what we mean, consider the following analogy. One might wonder why\nwe find paintings and sculptures of voluptuous women beautiful. Before the\nMorality Games\n313\ndevelopment of sexual selection theory, one might have argued that perfect spheres\nare some kind of Platonic solid, and inherently desirable, or that curvy hips yield\ngolden ratios. But with our current understanding of sexual selection, we recognize\nthat our sense of beauty has evolved and that there is no platonic sense of beauty\noutside of that shaped by sexual selection. Any argument about perfect spheres is\nunparsimonious and likely flawed. Without the help of evolution and game theory,\ndid philosophers conjure the moral equivalents of perfect spheres and golden ratios?\nThe state of nature, the orderly designer, Platonic ideals, autonomy, and humanity,\netc.—perhaps these arguments are also unfounded and unnecessary.\nReferences\nAlpizar, F., Carlsson, F., & Johansson-Stenman, O. (2008). Anonymity, reciprocity, and conformity:\nEvidence from voluntary contributions to a national park in Costa Rica. Journal of Public\nEconomics, 92 (5), 1047–1060.\nAndreoni, J., & Petrie, R. (2004). Public goods experiments without confidentiality: A glimpse\ninto fund-raising. Journal of Public Economics, 88 (7), 1605–1623.\nAndreoni, J. (1990). Impure altruism and donations to public goods: A theory of warm-glow giving.\nThe Economic Journal, 100 , 464–477.\nAndreoni, J., Rao, J. M., & Trachtman, H. (2011). Avoiding the ask: A fi eld experiment on altruism,\nempathy, and charitable giving . Technical report, National Bureau of Economic Research.\nAxelrod, R. M. (1984). The evolution of cooperation. New York: Basic Books.\nAxelrod, R., & Hamilton, W. D. (1981). The evolution of cooperation. Science, 211 (4489),\n1390–1396.\nAyres, I., Raseman, S., & Shih, A. (2012). Evidence from two large field experiments that peer\ncomparison feedback can reduce residential energy usage. Journal of Law, Economics, and\nOrganization , 2–20.\nBateson, M., Nettle, D., & Roberts, G. (2006). Cues of being watched enhance cooperation in a\nreal-world setting. Biology Letters, 2 (3), 412–414.\nBatson, C. D. (2014). The altruism question: Toward a social-psychological answer . Hillside, NJ:\nPsychology Press.\nBatson, C. D., Kobrynowicz, D., Dinnerstein, J. L., Kampf, H. C., & Wilson, A. D. (1997). In a\nvery different voice: Unmasking moral hypocrisy. Journal of Personality and Social Psychology,\n72 (6), 1335.\nBolton, G. E., Katok, E., & Ockenfels, A. (2005). Cooperation among strangers with limited information\nabout reputation. Journal of Public Economics, 89 (8), 1457–1468.\nCain, D., Dana, J., & Newman, G. (2014). Giving vs. giving in. Yale University Working Paper.\nCamerer, C. (2003). Behavioral game theory: Experiments in strategic interaction . Princeton, NJ:\nPrinceton University Press.\nChammah, A. M., & Rapoport, A. (1965). Prisoner’s dilemma; a study in confl ict and cooperation\n. Ann Arbor, MI: University of Michigan.\nChwe, M. (2013). Rational ritual: Culture, coordination, and common knowledge. Princeton, NJ:\nPrinceton University Press.\nCone v. West Virginia Pulp & Paper Co. , 330 U.S. 212, 67 S. Ct. 752, 91 L. Ed. 849 (1947).\nCritcher, C. R., Inbar, Y., & Pizarro, D. A. (2013). How quick decisions illuminate moral character.\nSocial Psychological and Personality Science, 4 (3), 308–315.\nCushman, F., Young, L., & Hauser, M. (2006). The role of conscious reasoning and intuition in\nmoral judgment testing three principles of harm. Psychological Science, 17 (12), 1082–1089.\n314\nM. Hoffman et al.\nDalkiran, N. A., Hoffman, M., Paturi, R., Ricketts, D., & Vattani, A. (2012). Common knowledge\nand state-dependent equilibria. In Algorithmic game theory (pp. 84–95). New York: Springer.\nDana, J., Cain, D. M., & Dawes, R. M. (2006). What you don’t know won’t hurt me: Costly (but\nquiet) exit in dictator games. Organizational Behavior and Human Decision Processes, 100 (2),\n193–201.\nDana, J., Weber, R. A., & Kuang, J. X. (2007). Exploiting moral wiggle room: Experiments\ndemonstrating an illusory preference for fairness. Economic Theory, 33 (1), 67–80.\nDavies, N. B. (1978). Territorial defense in the speckled wood butterfly (pararge aegeria): The resident\nalways wins. Animal Behaviour, 26 , 138–147.\nDawkins, R. (2006). The selfi sh gene . Oxford, UK: Oxford University Press.\nDellaVigna, S., List, J. A., & Malmendier, U. (2012). Testing for altruism and social pressure in\ncharitable giving. The Quarterly Journal of Economics, 127 (1), 1–56.\nDelton, A. W., Krasnow, M. M., Cosmides, L., & Tooby, J. (2011). Evolution of direct reciprocity\nunder uncertainty can explain human generosity in one-shot encounters. Proceedings of the\nNational Academy of Sciences, 108 (32), 13335–13340.\nDeScioli, P., Bruening, R., & Kurzban, R. (2011). The omission effect in moral cognition: Toward\na functional explanation. Evolution and Human Behavior, 32 (3), 204–215.\nDeScioli, P., Gilbert, S. S., & Kurzban, R. (2012). Indelible victims and persistent punishers in\nmoral cognition. Psychological Inquiry, 23 (2), 143–149.\nDeScioli, P., & Wilson, B. J. (2011). The territorial foundations of human property. Evolution and\nHuman Behavior, 32 (5), 297–304.\nDeScioli, P., & Kurzban, R. (2009). Mysteries of morality. Cognition, 112 (2), 281–299.\nDescioli, P., & Karpoff, R. (2014). People’s judgments about classic property law cases . Brandeis\nUniversity Working Paper.\nDesvousges, W. H., Johnson, F. R., Dunford, R. W., Boyle, K. J., Hudson, S. P., Wilson, K. N.,\net al. (2010). Measuring nonuse damages using contingent valuation: An experimental evaluation\nof accuracy . Research Triangle Park, NC: RTI Press.\nEpley, N., Keysar, B., Van Boven, L., & Gilovich, T. (2004). Perspective taking as egocentric\nanchoring and adjustment. Journal of Personality and Social Psychology, 87 (3), 327.\nFehr, E., & Fischbacher, U. (2003). The nature of human altruism. Nature, 425 (6960), 785–791.\nFehr, E., Fischbacher, U., & Gächter, S. (2002). Strong reciprocity, human cooperation, and the\nenforcement of social norms. Human Nature, 13 (1), 1–25.\nFisher, R. A. (1958). The genetic theory of natural selection . Mineola, NY : Dover.\nFrank, R. H. (1988). Passions within reason: The strategic role of the emotions. New York: WW\nNorton & Co.\nFrey, B. S., & Meier, S. (2004). Social comparisons and pro-social behavior: Testing “conditional\ncooperation” in a field experiment. American Economic Review, 94 , 1717–1722.\nFunk, P. (2006). Modern voting tools, social incentives and voter turnout: Theory and evidence.\nIn Annual Meeting of the American Economic Association , Chicago, IL.\nGelman, A., Silver, N., & Edlin, A. (2012). What is the probability your vote will make a difference?\nEconomic Inquiry, 50 (2), 321–326.\nGintis, H. (2007). The evolution of private property. Journal of Economic Behavior & Organization,\n64 (1), 1–16.\nGintis, H., Bowles, S., Boyd, R., & Fehr, E. (2003). Explaining altruistic behavior in humans.\nEvolution and Human Behavior, 24 (3), 153–172.\nGoldstein, N. J., Cialdini, R. B., & Griskevicius, V. (2008). A room with a view-point: Using social\nnorms to motivate environmental conservation in hotels. Journal of Consumer Research, 35 (3),\n472–482.\nGreene, J. D., Cushman, F. A., Stewart, L. E., Lowenberg, K., Nystrom, L. E., & Cohen, J. D.\n(2009). Pushing moral buttons: The interaction between personal force and intention in moral\njudgment. Cognition, 111 (3), 364–371.\nHaidt, J. (2012). The righteous mind: Why good people are divided by politics and religion .\nNew York: Vintage.\nHaley, K. J., & Fessler, D. M. (2005). Nobody’s watching? Subtle cues affect generosity in an\nanonymous economic game. Evolution and Human Behavior, 26 .\nMorality Games\n315\nHedden, T., & Zhang, J. (2002). What do you think I think you think? Strategic reasoning in matrix\ngames. Cognition, 85 (1), 1–36.\nHoffman, M., Yoeli, E., & Nowak, M. A. (2015). Cooperate without looking: Why we care what\npeople think and not just what they do. Proceedings of the National Academy of Sciences,\n112 (6), 1727–1732.\nHoffman, M., Yoeli, E., & Dalkiran, A. (2015). Social applications of common knowledge . Harvard\nUniversity Working Paper.\nKahneman, D., Knetsch, J. L., & Thaler, R. H. (1990). Experimental tests of the endowment effect\nand the Coase theorem. Journal of Political Economy, 98 , 1325–1348.\nKant, I. (1997/1787). Groundwork of the metaphysics of morals (1785). Practical Philosophy,\n108 .\nKarlan, D., & List, J. A. (2006). Does price matter in charitable giving? Evidence from a largescale\nnatural fi eld experiment . Technical report, National Bureau of Economic Research.\nKurzban, R. (2012). Why everyone (else) is a hypocrite: Evolution and the modular mind.\nPrinceton, NJ: Princeton University Press.\nLacetera, N., & Macis, M. (2010). Social image concerns and prosocial behavior: Field evidence\nfrom a nonlinear incentive scheme. Journal of Economic Behavior & Organization, 76 (2),\n225–237.\nLevitt, S. D., & List, J. A. (2007). What do laboratory experiments measuring social preferences\nreveal about the real world? The Journal of Economic Perspectives, 21 , 153–174.\nList, J. A. (2007). On the interpretation of giving in dictator games. Journal of Political Economy,\n115 (3), 482–493.\nList, J. A., Berrens, R. P., Bohara, A. K., & Kerkvliet, J. (2004). Examining the role of social\nisolation on stated preferences. American Economic Review, 94 , 741–752.\nLocke, J. (1988/1688). Locke: Two treatises of government student edition . Cambridge, UK:\nCambridge University Press.\nMankiw, N. G. (2007). Principles of economics . Thomson Learning.\nMilgrom, P., & Weber, R. J. (1982). The value of information in a sealed-bid auction. Journal of\nMathematical Economics, 10 (1), 105–114.\nMorris, S., & Shin, H. S. (2002). Social value of public information. The American Economic\nReview, 92 (5), 1521–1534.\nMyerson, R. B. (2004). Justice, institutions, and multiple equilibria. Chicago Journal of\nInternational Law, 5 , 91.\nNeuwirth, R. (2005). Shadow cities: A billion squatters, a new urban world . New York: Routledge.\nNowak, M. A. (2006). Evolutionary dynamics: Exploring the equations of life . Cambridge, MA:\nHarvard University Press.\nPanchanathan, K., & Boyd, R. (2004). Indirect reciprocity can stabilize cooperation without the\nsecond-order free rider problem. Nature, 432 (7016), 499–502.\nPinker, S. (2003). The blank slate: The modern denial of human nature . UK: Penguin.\nPinker, S., Nowak, M. A., & Lee, J. J. (2008). The logic of indirect speech. Proceedings of the\nNational Academy of Sciences, 105 (3), 833–838.\nRand, D. G., & Epstein, Z. G. (2014). Risking your life without a second thought: Intuitive\ndecision- making and extreme altruism . Available at SSRN 2424036.\nRoss, L., & Ward, A. (1996). Naive realism: Implications for social conflict and misunderstanding.\nIn T. Brown, E. Reed, & E. Turiel (Eds.), Values and knowledge (pp. 103–135).\nRoth, A. E. (1995). Bargaining experiments. In J. H. Kagel & E. R. Alvin (Eds.), The handbook of\nexperimental economics (pp. 253–342). Princeton, NJ: Princeton University Press.\nSigg, H., & Falett, J. (1985). Experiments on respect of possession and property in hamadryas\nbaboons ( Papio hamadryas ). Animal Behaviour, 33 (3), 978–984.\nSmith, J. M., & Price, G. (1973). The logic of animal conflict. Nature, 246 , 15.\nSosis, R., & Alcorta, C. (2003). Signaling, solidarity, and the sacred: The evolution of religious\nbehavior. Evolutionary Anthropology: Issues, News, and Reviews, 12 (6), 264–274.\nSpranca, M., Minsk, E., & Baron, J. (1991). Omission and commission in judgment and choice.\nJournal of Experimental Social Psychology, 27 (1), 76–105.\n316\nM. Hoffman et al.\nSudarshan, A. (2014). Nudges in the marketplace: Using peer comparisons and incentives to reduce\nhousehold electricity consumption . Technical report, Harvard University Working Paper.\nThaler, R. (1980). Toward a positive theory of consumer choice. Journal of Economic Behavior &\nOrganization, 1 (1), 39–60.\nThomson, J. J. (1976). Killing, letting die, and the trolley problem. The Monist, 59 (2), 204–217.\nTirole, J. (1988). The theory of industrial organization . Cambridge, MA: MIT Press.\nTrivers, R. (2011). The folly of fools: The logic of deceit and self-deception in human life.\nNew York: Basic Books.\nTrivers, R. L. (1971). The evolution of reciprocal altruism. Quarterly Review of Biology, 46 ,\n35–57.\nTrivers, R. L. (1974). Parent-offspring conflict. American Zoologist, 14 (1), 249–264.\nWilkinson, G. S. (1984). Reciprocal food sharing in the vampire bat. Nature, 308 (5955), 181–184.\nWilson, D. S. (2010). Darwin’s cathedral: Evolution, religion, and the nature of society . Chicago:\nUniversity of Chicago Press.\nWilson, E. O. (2012). The social conquest of earth . New York: WW Norton & Company.\nWilson, D. S. (2006). Human groups as adaptive units: Toward a permanent consensus. In\nP. Carruthers, S. Laurence, & S. Stich (Eds.), The innate mind: Culture and cognition . Oxford,\nUK: Oxford University Press.\nWinter, E. (2014). Feeling smart: Why our emotions are more rational than we think. Public Books.\nWroughton, L. (2013). As Syria War Escalates, Americans Cool to U.S. Intervention: Reuter/Ipsos\nPoll. Reuters , August 24, 2013. Accessed at http://www.reuters.com/article/2013/08/25/\nus-syria- crisis-usa-poll-idUSBRE97O00E20130825 on 12 March 2015.\nYoeli, E., Hoffman, M., Rand, D. G., & Nowak, M. A. (2013). Powering up with indirect reciprocity\nin a large-scale field experiment. Proceedings of the National Academy of Sciences,\n110 (Suppl. 2), 10424–10429.\nZahavi, A. (1975). Mate selection: A selection for a handicap. Journal of Theoretical Biology,\n53 (1), 205–214.",
  "metadata": {
    "original_filename": "TEXT-001-HOUSE_OVERSIGHT_015501.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 88107,
    "word_count": 14528,
    "line_count": 1214,
    "import_date": "2025-11-19T21:47:48.380364",
    "prefix": "TEXT-001"
  }
}
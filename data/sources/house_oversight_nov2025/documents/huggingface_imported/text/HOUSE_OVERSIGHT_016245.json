{
  "document_id": "HOUSE_OVERSIGHT_016245",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016245.txt",
  "text": "THE LIMITATIONS OF OPAQUE LEARNING MACHINES\nJudea Pearl\n\nJudea Pearl is a professor of computer science and director of the Cognitive Systems\nLaboratory at UCLA. His most recent book, co-authored with Dana Mackenzie, is The\nBook of Why: The New Science of Cause and Effect.\n\nAs a former physicist, I was extremely interested in cybernetics. Though it did not utilize\nthe full power of Turing Machines, it was highly transparent, perhaps because it was\nfounded on classical control theory and information theory. We are losing this\ntransparency now, with the deep-learning style of machine learning. It is fundamentally a\ncurve-fitting exercise that adjusts weights in intermediate layers of a long input-output\nchain.\n\nI find many users who say that it “works well and we don’t know why.” Once\nyou unleash it on large data, deep learning has its own dynamics, it does its own repair\nand its own optimization, and it gives you the right results most of the time. But when it\ndoesn’t, you don’t have a clue about what went wrong and what should be fixed. In\nparticular, you do not know if the fault is in the program, in the method, or because things\nhave changed in the environment. We should be aiming at a different kind of\ntransparency.\n\nSome argue that transparency is not really needed. We don’t understand the\nneural architecture of the human brain, yet it runs well, so we forgive our meager\nunderstanding and use human helpers to great advantage. In the same way, they argue,\nwhy not unleash deep-learning systems and create intelligence without understanding\nhow they work? I buy this argument to some extent. I personally don’t like opacity, so I\nwon’t spend my time on deep learning, but I know that it has a place in the makeup of\nintelligence. I know that non-transparent systems can do marvelous jobs, and our brain is\nproof of that marvel.\n\nBut this argument has its limitation. The reason we can forgive our meager\nunderstanding of how human brains work is because our brains work the same way, and\nthat enables us to communicate with other humans, learn from them, instruct them, and\nmotivate them in our own native language. If our robots will all be as opaque as\nAlphaGo, we won’t be able to hold a meaningful conversation with them, and that would\nbe unfortunate. We will need to retrain them whenever we make a slight change in the\ntask or in the operating environment.\n\nSo, rather than experimenting with opaque learning machines, I am trying to\nunderstand their theoretical limitations and examine how these limitations can be\novercome. I do it in the context of causal-reasoning tasks, which govern much of how\nscientists think about the world and, at the same time, are rich in intuition and toy\nexamples, so we can monitor the progress in our analysis. In this context, we’ve\ndiscovered that some basic barriers exist, and that unless they are breached we won’t get\nareal human kind of intelligence no matter what we do. I believe that charting these\nbarriers may be no less important than banging our heads against them.\n\nCurrent machine-learning systems operate almost exclusively in a statistical, or\nmodel-blind, mode, which is analogous in many ways to fitting a function to a cloud of\ndata points. Such systems cannot reason about “what if ?” questions and, therefore,\n\n25\n\nHOUSE_OVERSIGHT_016245",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016245.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3327,
    "word_count": 563,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:48.434453",
    "prefix": "IMAGES-003"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_015522",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_015522.txt",
  "text": "310 M. Hoffman et al.\n\nNorms. People are typically conditionally cooperative, meaning that they are will-\ning to cooperate more when they believe others contribute more. For example, stu-\ndents asked to donate to a university charity gave 2.3 percentage points more when\ntold that others had given at a rate of 64 % than when they were told giving rates\nwere 46 % (Frey & Meier, 2004). Hotel patrons were 26 % more likely to reuse their\ntowels when informed most others had done the same (Goldstein, Cialdini, &\nGriskevicius, 2008). Households have been shown to meaningfully reduce electric-\nity consumption when told neighbors are consuming less, both in the United States\n(Ayres, Raseman, & Shih, 2012) and in India (Sudarshan, 2014).\n\nSuch conditional cooperation is easily explained by the game theory model:\nWhen others give, one can infer that one is expected to give and may be socially\nsanctioned if one does not.\n\nStrategic Ignorance. Those at high risk of contracting a sexually transmitted dis-\nease (STD) often go untested, presumably because if they knew they had the STD,\nthey would feel morally obliged to refrain from otherwise desirable activity that\nrisks spreading the STD. Why is it more reproachable to knowingly put a sexual\npartner at risk when one knows one has the STD than to knowingly put a sexual\npartner at risk by not getting tested? There is evidence that we sometimes pursue\nstrategic ignorance and avoid information about the negative consequences of our\ndecisions to others. When subjects are shown two options, one that is better for\nthemselves but worse for their partners and one that is worse for themselves but bet-\nter for their partners, many choose the option that is better for their partners. But,\nwhen subjects must first press a button (at no cost) to reveal which option is better\nfor their partners, they choose to remain ignorant and simply select the option that\nis best for themselves (Dana, Weber, & Kuang, 2007).\n\nThis quirk of our moral system is again easy to explain with the above model.\nTypically, information about how one’s actions affect others is hard to obtain, so\npeople cannot be blamed for not having such information. When one can get such\ninformation easily, others may not know that it is easy to obtain and will not punish\nanyone who does not have the information. For example, although it is trivially easy\nto look up charities’ financial ratings on websites like charitynavigator.org, few\npeople know this and could negatively judge those that donate without first check-\ning such websites. And even when others know that one can get this information\neasily, they might suspect that others do not know this, and so avoid punishing,\nsince others won’t expect punishment. To summarize, strategic ignorance prevents\ncommon knowledge of a violation and so is likely to go unpunished. We again\nemphasize that we will be lenient of strategic ignorance, even when punishment is\nnot literally an option.\n\nNorm of Reciprocity. We feel compelled to reciprocate favors, even if we know\nthat the favors were done merely to elicit reciprocation and even if the favor asked\nin return is larger than the initial one granted (Cialdini 2001). For instance, mem-\nbers of Hare Krishna successfully collect donations by handing out flowers to dis-\nembarking passengers at airports, even though passengers want nothing to do with\nthe flowers: They walk just a few feet before discarding them in the nearest bin.\n\nHOUSE_OVERSIGHT_015522",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_015522.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3487,
    "word_count": 579,
    "line_count": 52,
    "import_date": "2025-11-19T21:47:46.189118",
    "prefix": "IMAGES-003"
  }
}
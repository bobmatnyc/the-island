{
  "document_id": "HOUSE_OVERSIGHT_016996",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016996.txt",
  "text": "Research Article\n\nQuantitative Analysis of Culture Using Millions of Digitized Books\n\nJean-Baptiste Michel,?***+ Yuan Kui Shen,° Aviva Presser Aiden,° Adrian Veres,’ Matthew K. Gray,* The Google Books\nTeam,® Joseph P. Pickett,’ Dale Hoiberg,’” Dan Clancy,® Peter Norvig,® Jon Orwant,® Steven Pinker,’ Martin A. Nowak,)!)”\n\nErez Lieberman Aiden! !*!*!4:15-6#+\n\n'Program for Evolutionary Dynamics, Harvard University, Cambridge, MA 02138, USA. “Institute for Quantitative Social\nSciences, Harvard University, Cambridge, MA 02138, USA. *Department of Psychology, Harvard University, Cambridge, MA\n02138, USA. “Department of Systems Biology, Harvard Medical School, Boston, MA 02115, USA. °Computer Science and\nArtificial Intelligence Laboratory, MIT, Cambridge, MA 02139, USA. °Harvard Medical School, Boston, MA, 02115, USA.\n\"Harvard College, Cambridge, MA 02138, USA. °Google, Inc., Mountain View, CA, 94043, USA. \"Houghton Mifflin Harcourt,\nBoston, MA 02116, USA. '’Encyclopaedia Britannica, Inc., Chicago, IL 60654, USA. ''Dept of Organismic and Evolutionary\nBiology, Harvard University, Cambridge, MA 02138, USA. '*Dept of Mathematics, Harvard University, Cambridge, MA\n02138, USA. “Broad Institute of Harvard and MIT, Harvard University, Cambridge, MA 02138, USA. School of Engineering\nand Applied Sciences, Harvard University, Cambridge, MA 02138, USA. Harvard Society of Fellows, Harvard University,\nCambridge, MA 02138, USA. '6L aboratory-at-Large, Harvard University, Cambridge, MA 02138, USA.\n\n*These authors contributed equally to this work.\n\n+To whom correspondence should be addressed. E-mail: jb.michel@gmail.com (J.B.M.); erez@erez.com (E.A.).\n\nWe constructed a corpus of digitized texts containing\nabout 4% of all books ever printed. Analysis of this\ncorpus enables us to investigate cultural trends\nquantitatively. We survey the vast terrain of\n“culturomics”, focusing on linguistic and cultural\nphenomena that were reflected in the English language\nbetween 1800 and 2000. We show how this approach can\nprovide insights about fields as diverse as lexicography,\nthe evolution of grammar, collective memory, the\nadoption of technology, the pursuit of fame, censorship,\nand historical epidemiology. “Culturomics” extends the\nboundaries of rigorous quantitative inquiry to a wide\narray of new phenomena spanning the social sciences and\nthe humanities.\n\nReading small collections of carefully chosen works enables\nscholars to make powerful inferences about trends in human\nthought. However, this approach rarely enables precise\nmeasurement of the underlying phenomena. Attempts to\nintroduce quantitative methods into the study of culture (/-6)\nhave been hampered by the lack of suitable data.\n\nWe report the creation of a corpus of 5,195,769 digitized\nbooks containing ~4% of all books ever published.\nComputational analysis of this corpus enables us to observe\ncultural trends and subject them to quantitative investigation.\n“Culturomics” extends the boundaries of scientific inquiry to\na wide array of new phenomena.\n\nThe corpus has emerged from Google’s effort to digitize\nbooks. Most books were drawn from over 40 university\nlibraries around the world. Each page was scanned with\n\ncustom equipment (7), and the text digitized using optical\ncharacter recognition (OCR). Additional volumes — both\nphysical and digital — were contributed by publishers.\nMetadata describing date and place of publication were\nprovided by the libraries and publishers, and supplemented\nwith bibliographic databases. Over 15 million books have\nbeen digitized [12% of all books ever published (7)]. We\nselected a subset of over 5 million books for analysis on the\nbasis of the quality of their OCR and metadata (Fig. 1A) (7).\nPeriodicals were excluded.\n\nThe resulting corpus contains over 500 billion words, in\nEnglish (361 billion), French (45B), Spanish (45B), German\n(37B), Chinese (13B), Russian (35B), and Hebrew (2B). The\noldest works were published in the 1500s. The early decades\nare represented by only a few books per year, comprising\nseveral hundred thousand words. By 1800, the corpus grows\nto 60 million words per year; by 1900, 1.4 billion; and by\n2000, 8 billion.\n\nThe corpus cannot be read by a human. If you tried to read\nonly the entries from the year 2000 alone, at the reasonable\npace of 200 words/minute, without interruptions for food or\nsleep, it would take eighty years. The sequence of letters is\none thousand times longer than the human genome: if you\nwrote it out in a straight line, it would reach to the moon and\nback 10 times over (8).\n\nTo make release of the data possible in light of copyright\nconstraints, we restricted our study to the question of how\noften a given “1-gram” or “n-gram” was used over time. A 1-\ngram is a string of characters uninterrupted by a space; this\nincludes words (“banana”, “SCUBA”) but also numbers\n\nSciencexpress / www.sciencexpress.org / 16 December 2010 / Page 1 / 10.1126/science.1199644\n\nHOUSE_OVERSIGHT_016996\n\nDownloaded from www.sciencemag.org on December 16, 2010",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016996.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 5023,
    "word_count": 743,
    "line_count": 96,
    "import_date": "2025-11-19T21:47:45.774538",
    "prefix": "IMAGES-004"
  }
}
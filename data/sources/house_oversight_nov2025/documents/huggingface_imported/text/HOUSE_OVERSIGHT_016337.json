{
  "document_id": "HOUSE_OVERSIGHT_016337",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016337.txt",
  "text": "reasoning without having to hire a programmer for each problem. Wiener recognized the\nrole of feedback in machine learning, but he missed the key role of representation. It’s\nnot possible to store all possible images in a self-driving car, or all possible sounds in a\nconversational computer; they have to be able to generalize from experience. The “deep”\npart of deep learning refers not to the (hoped-for) depth of insight but to the depth of the\nmathematical network layers used to make predictions. It turned out that a linear increase\nin network complexity led to an exponential increase in the expressive power of the\nnetwork.\n\nIf you lose your keys in a room, you can search for them. If you’re not sure\nwhich room they’re in, you have to search all the rooms in a building. If you’re not sure\nwhich building they’re in, you have to search all the rooms in all the buildings in a city.\nIf you’re not sure which city they’re in, you have to search all the rooms in all the\nbuildings in all the cities. In AI, finding the keys corresponds to things like a car safely\nfollowing the road, or a computer correctly interpreting a spoken command, and the\nrooms and buildings and cities correspond to all of the options that have to be considered.\nThis is called the curse of dimensionality.\n\nThe solution to the curse of dimensionality came in using information about the\nproblem to constrain the search. The search algorithms themselves are not new. But\nwhen applied to a deep-learning network, they adaptively build up representations of\nwhere to search. The price of this is that it’s no longer possible to exactly solve for the\nbest answer to a problem, but typically all that’s needed is an answer that’s good enough.\n\nTaken together, it shouldn’t be surprising that these scaling laws have allowed\nmachines to become effectively as capable as the corresponding stages of biological\ncomplexity. Neural networks started out with a goal of modeling how the brain works.\nThat goal was abandoned as they evolved into mathematical abstractions unrelated to\nhow neurons actually function. But now there’s a kind of convergence that can be\nthought of as forward- rather than reverse-engineering biology, as the results of deep\nlearning echo brain layers and regions.\n\nOne of the most difficult research projects P ve managed paired what we’d now\ncall data scientists with AI pioneers. It was a miserable experience in moving goalposts.\nAs the former progressed in solving long-standing problems posed by the latter, this was\ndeemed to not count because it wasn’t accompanied by corresponding leaps in\nunderstanding the solutions. What’s the value of a chess-playing computer if you can’t\nexplain how it plays chess?\n\nThe answer of course is that it can play chess. There is interesting emerging\nresearch that is applying AI to AI—that is, training networks to explain how they operate.\nBut both brains and computer chips are hard to understand by watching their inner\nworkings; they’re easily interpreted only by observing their external interfaces. We come\nto trust (or not) brains and computer chips alike based on experience that tests them\nrather than on explanations for how they work.\n\nMany branches of engineering are making a transition from what’s called\nimperative to declarative or generative design. This means that instead of explicitly\ndesigning a system with tools like CAD files, circuit schematics, and computer code, you\ndescribe what you want the system to do and then an automated search is done for\ndesigns that satisfy your goals and restrictions. This approach becomes necessary as\ndesign complexity exceeds what can be understood by a human designer. While that\n\n117\n\nHOUSE_OVERSIGHT_016337",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016337.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3712,
    "word_count": 614,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:49.177854",
    "prefix": "IMAGES-003"
  }
}
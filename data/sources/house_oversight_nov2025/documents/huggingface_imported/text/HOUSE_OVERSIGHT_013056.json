{
  "document_id": "HOUSE_OVERSIGHT_013056",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013056.txt",
  "text": "140 7 A Formal Model of Intelligent Agents\n\nWe suggest to view the definitions of pragmatic and efficient pragmatic general intelligence\nin terms of a “possible worlds” semantics — i.e. to view them as asking, counterfactually, how\nan agent would perform, hypothetically, on a series of tests (the tests being goals, defined in\nrelation to environments and reward signals).\n\nReal-world intelligent agents don’t normally operate in terms of explicit goals and rewards;\nthese are abstractions that we use to think about intelligent agents. However, this is no objection\nto characterizing various sorts of intelligence in terms of counterfactuals like: how would system\nS operate if it were trying to achieve this or that goal, in this or that environment, in order to\nseek reward? We can characterize various sorts of intelligence in terms of how it can be inferred\nan agent would perform on certain tests, even though the agent’s real life does not consist of\ntaking these tests.\n\nThis conceptual approach may seem a bit artificial but we don’t currently see a better\nalternative, if one wishes to quantitatively gauge intelligence (which is, in a sense, an “artificial”\nthing to do in the first place). Given a real-world agent X and a mandate to assess its intelligence,\nthe obvious alternative to looking at possible worlds in the manner of the above definitions,\nis just looking directly at the properties of the things X has achieved in the real world during\nits lifespan. But this isn’t an easy solution, because it doesn’t disambiguate which aspects of\nX’s achievements were due to its own actions versus due to the rest of the world that X was\ninteracting with when it made its achievements. To distinguish the amount of achievement that\nX “caused” via its own actions requires a model of causality, which is a complex can of worms in\nitself; and, critically, the standard models of causality also involve counterfactuals (asking “what\nwould have been achieved in this situation if the agent X hadn’t been there”, etc.) [MW07].\nRegardless of the particulars, it seems impossible to avoid counterfactual realities in assessing\nintelligence.\n\nThe approach we suggest — given a real-world agent X with a history of actions in a particular\nworld, and a mandate to assess its intelligence — is to introduce an additional player, an inference\nagent 6, into the picture. The agent 7 modeled above is then viewed as 7x: the model of X that\n6 constructs, in order to explore X’s inferred behaviors in various counterfactual environments.\nIn the test situations embodied in the definitions of pragmatic and efficient pragmatic general\nintelligence, the environment gives 7x rewards, based on specifically configured goals. In X’s\nreal life, the relation between goals, rewards and actions will generally be significantly subtler\nand perhaps quite different.\n\nWe model the real world similarly to the “fantasy world” of the previous section, but with\nthe omission of goals and rewards. We define a naturalistic context as one in which all goals and\nrewards are constant, i.e. g; = 99 and r; = 7p for all 7. This is just a mathematical convention\nfor stating that there are no precisely-defined external goals and rewards for the agent. In a\nnaturalistic context, we then have a situation where agents create actions based on the past\nhistory of actions and perceptions, and if there is any relevant notion of reward or goal, it\nis within the cognitive mechanism of some agent. A naturalistic agent X is then an agent 7\nwhich is restricted to one particular naturalistic context, involving one particular environment\npe (formally, we may achieve this within the framework of agents described above via dictating\nthat X issues constant “null actions” ao in all environments except ;:).\n\nNext, we posit a metric space (2, d) of naturalistic agents defined on a naturalistic context\ninvolving environment yp, and a subspace A € &’, of inference agents, which are naturalistic\nagents that output predictions of other agents’ behaviors (a notion we will not fully formalize\nhere). If agents are represented as program trees, then d may be taken as edit distance on tree\nspace [Bil05]. Then, for each agent 6 € A, we may assess\n\nHOUSE_OVERSIGHT_013056",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013056.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 4240,
    "word_count": 699,
    "line_count": 56,
    "import_date": "2025-11-19T21:47:46.204770",
    "prefix": "IMAGES-002"
  }
}
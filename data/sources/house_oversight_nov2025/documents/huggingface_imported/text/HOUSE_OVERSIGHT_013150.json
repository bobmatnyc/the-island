{
  "document_id": "HOUSE_OVERSIGHT_013150",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013150.txt",
  "text": "234 12 The Engineering and Development of Ethics\n\nThe ideal in this regard would be a system like Cyc [LG90] with a fully explicit logic-based\nknowledge representation based on a standard ontology — in this case, every Cyc instance\nwould have a relatively easy time understanding the inner thought processes of every other\nCyc instance. However, most AGI researchers doubt that fully explicit approaches like this will\never be capable of achieving advanced AGI using feasible computational resources. OpenCog\nuses a mixed representation, with an explicit (uncertain) logical aspect as well as an explicit\nsubsymbolic aspect more analogous to attractor neural nets.\n\nThe OpenCog design also contains a mechanism called Psynese (not yet implemented), in-\ntended to make it easier for one OpenCog instance to translate its personal thoughts into the\nmental language of another OpenCog instance. This translation process may be quite subtle,\nsince each instance will generally learn a host of new concepts based on its experience, and these\nconcepts may not possess any compact mapping into shared linguistic symbols or percepts. The\nwide deployment of some mechanism of this nature among a community of AGIs, will be very\nhelpful in terms of enabling this community to display the level of mutual understanding needed\nfor strongly encouraging ethical stability.\n\n12.9 AGI Ethics As Related to Various Future Scenarios\n\nFollowing up these various futuristic considerations, in this section we discuss possible ethical\nconflicts that may arise in several different types of AGI development scenarios. Each scenario\npresents specific variations on the general challenges of teaching morals and ethics to an ad-\nvanced, selfaware and volitional intelligence. While there is no way to tell at this point which,\nif any, of these scenarios will unfold, there is value to understanding each of them as means of\nultimately developing a robust and pragmatic approach to teaching ethics to AGI systems.\n\nEven more than the previous sections, this is an exercise in “speculative futurology” that is\ndefinitely not necessary for the appreciation of the CogPrime design, so readers whose interests\nare mainly engineering and computer science focused may wish to skip ahead. However, we\npresent these ideas here rather than at the end of the book to emphasize the point that this\nsort of thinking has informed our technical AGI design process in nontrivial ways.\n\n12.9.1 Capped Intelligence Scenarios\n\nCapped intelligence scenarios involve a situation in which an AGI, by means of software restric-\ntions (including omitted or limited internal rewriting capabilities or limited access to hardware\nresources), is inherently prohibited from achieving a level of intelligence beyond a predetermined\ngoal. A capped intelligence AGI is designed to be unable to achieve a Singularitarian moment.\nSuch an AGI can be seen as “just another form of intelligent actor in the world, one which has\nlevels of intelligence, self awareness, and volition that is perhaps somewhat greater than, but\nstill comparable to humans and other animals.\n\nEthical questions under this scenario are very similar to interhuman ethical considerations,\nwith similar consequences. Learning that proceeds in a relatively human-like manner is entirely\nrelevant to such human-like intelligences. The degree of danger is mitigated by the lack of\nsuperintelligence, and time is not of the essence. The imitative-reinforcement-corrective learning\n\nHOUSE_OVERSIGHT_013150",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013150.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3513,
    "word_count": 535,
    "line_count": 50,
    "import_date": "2025-11-19T21:47:44.087527",
    "prefix": "IMAGES-002"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_013122",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013122.txt",
  "text": "206 12 The Engineering and Development of Ethics\n\nconstitute an intelligent system — and it’s something that involves both cognitive architecture\nand the exploration a system does and the instruction it receives. It’s a very complex matter\nthat is richly intermixed with all the other aspects of intelligence, and here we will treat it as\nsuch.\n\n12.2 Review of Current Thinking on the Risks of AGI\n\nBefore proceeding to outline our own perspective on AGI ethics in the context of CogPrime, we\nwill review the main existing strains of thought on the potential ethical dangers associated with\nAGI. One science fiction film after another has highlighted these dangers, lodging the issue deep\nin our cultural awareness; unsurprisingly, much less attention has been paid to serious analysis\nof the risks in their various dimensions, but there is still a non-trivial literature worth paying\nattention to.\n\nHypothetically, an AGI with superhuman intelligence and capability could dispense with\nhumanity altogether — ie. posing an “existential risk\" [Bos02]. In the worst case, an evil but\nbrilliant AGI, perhaps programmed by a human sadist, could consign humanity to unimaginable\ntortures (i.e. realizing a modern version of the medieval Christian visions of hell). On the\nother hand, the potential benefits of powerful AGI also go literally beyond human imagination.\nIt seems quite plausible that an AGI with massively superhuman intelligence and positive\ndisposition toward humanity could provide us with truly dramatic benefits, such as a virtual\nend to material scarcity, disease and aging. Advanced AGI could also help individual humans\ngrow in a variety of directions, including directions leading beyond \"legacy humanity,\" according\nto their own taste and choice.\n\nEliezer Yudkowsky has introduced the term \"Friendly AI\", to refer to advanced AGI systems\nthat act with human benefit in mind [Yud06]. Exactly what this means has not been specified\nprecisely, though informal interpretations abound. Goertzel [Goe06b] has sought to clarify the\nnotion in terms of three core values of Joy, Growth and Freedom. In this view, a Friendly AI\nwould be one that advocates individual and collective human joy and growth, while respecting\nthe autonomy of human choices.\n\nSome (for example, Hugo de Garis, [DGO05]), have argued that Friendly AI is essentially\nan impossibility, in the sense that the odds of a dramatically superhumanly intelligent mind\nworrying about human benefit are vanishingly small. If this is the case, then the best options\nfor the human race would presumably be to either avoid advanced AGI development altogether,\nor to else fuse with AGI before it gets too strongly superhuman, so that beings-originated-as-\nhumans can enjoy the benefits of greater intelligence and capability (albeit at cost of sacrificing\ntheir humanity).\n\nOthers (e.g. Mark Waser [Was09]) have argued that Friendly AI is essentially inevitable,\nbecause greater intelligence correlates with greater morality. Evidence from evolutionary and\nhuman history is adduced in favor of this point, along with more abstract arguments.\n\nYudkowsky [Yud06] has discussed the possibility of creating AGI architectures that are in\nsome sense \"provably Friendly\" — either mathematically, or else at least via very tight lines of ra-\ntional verbal argumentation. However, several issues have been raised with this approach. First,\nit seems likely that proving mathematical results of this nature would first require dramatic ad-\nvances in multiple branches of mathematics. Second, such a proof would require a formalization\nof the goal of \"Friendliness,\" which is a subtler matter than it might seem [Leg06b, LegQ6a].\n\nHOUSE_OVERSIGHT_013122",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013122.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3709,
    "word_count": 569,
    "line_count": 54,
    "import_date": "2025-11-19T21:47:46.246728",
    "prefix": "IMAGES-002"
  }
}
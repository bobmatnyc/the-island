{
  "document_id": "HOUSE_OVERSIGHT_013024",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013024.txt",
  "text": "108 6 A Brief Overview of CogPrime\n\ntop-level goals will be simple things such as pleasing the teacher, learning new information\nand skills, and protecting the robot’s body. Figure 6.3 shows part of the architecture via which\ncognitive processes interact with each other, via commonly acting on the AtomSpace knowledge\nrepository.\n\nComparing these diagrams to the integrative human cognitive architecture diagrams given\nin Chapter 5, one sees the main difference is that the CogPrime diagrams commit to specific\nstructures (e.g. knowledge representations) and processes, whereas the generic integrative archi-\ntecture diagram refers merely to types of structures and processes. For instance, the integrative\ndiagram refers generally to declarative knowledge and learning, whereas the CogPrime diagram\nrefers to PLN, as a specific system for reasoning and learning about declarative knowledge. Ta-\nble 6.1 articulates the key connections between the components of the CogPrime diagram and\nthose of the integrative diagram, thus indicating the general cognitive functions instantiated by\neach of the CogPrime components.\n\n6.3 Current and Prior Applications of OpenCog\n\nBefore digging deeper into the theory, and elaborating some of the dynamics underlying the\nabove diagrams, we pause to briefly discuss some of the practicalities of work done with the\nOpenCog system currently implementing parts of the CogPrime architecture.\n\nOpenCog, the open-source software framework underlying the “OpenCogPrime” (currently\npartial) implementation of the CogPrime architecture, has been used for commercial applica-\ntions in the area of natural language processing and data mining; for instance, see [GPPG06]\nwhere OpenCogPrime’s PLN reasoning and RelEx language processing are combined to do\nautomated biological hypothesis generation based on information gathered from PubMed ab-\nstracts. Most relevantly to the present work, it has also been used to control virtual agents in\nvirtual worlds [GEA08].\n\nPrototype work done during 2007-2008 involved using an OpenCog variant called the Open-\nPetBrain to control virtual dogs in a virtual world (see Figure 6.6 for a screenshot of an\nOpenPetBrain-controlled virtual dog). While these OpenCog virtual dogs did not display in-\ntelligence closely comparable to that of real dogs (or human children), they did demonstrate a\nvariety of interesting and relevant functionalities including:\n\ne learning new behaviors based on imitation and reinforcement\n\ne responding to natural language commands and questions, with appropriate actions and\nnatural language replies\n\n® spontaneous exploration of their world, remembering their experiences and using them to\nbias future learning and linguistic interaction\n\nOne current OpenCog initiative involves extending the virtual dog work via using OpenCog\nto control virtual agents in a game world inspired by the game Minecraft. These agents are\ninitially specifically concerned with achieving goals in a game world via constructing structures\nwith blocks and carrying out simple English communications. Representative example tasks\nwould be:\n\ne Learning to build steps or ladders to get desired objects that are high up\ne Learning to build a shelter to protect itself from aggressors\n\nHOUSE_OVERSIGHT_013024",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013024.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3276,
    "word_count": 476,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:44.051153",
    "prefix": "IMAGES-002"
  }
}
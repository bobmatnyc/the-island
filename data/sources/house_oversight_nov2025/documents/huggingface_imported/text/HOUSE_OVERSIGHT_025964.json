{
  "document_id": "HOUSE_OVERSIGHT_025964",
  "filename": "IMAGES-008-HOUSE_OVERSIGHT_025964.txt",
  "text": "literally smarter on every level, and because the abilities of the higher levels depend on those of the lower\nlevels, they can perform abstractions that mature gorillas will never learn, no matter how much we try to\ntrain them.\n\nThe second set of mechanisms is in the motivational system. Motivation tells the brain what to pay attention\nto, by giving reward and punishment. If a brain does not get much reward for solving puzzles, the individual\nwill find mathematics very boring and won't learn much of it. Ifa brain gets lots of rewards for discovering\nother people's intentions, it will learn a lot of social cognition.\n\nLanguage might be the result of three things that are different in humans:\n\n- extended training periods per layer (after the respective layer is done, it is difficult to learn a new set of\nphonemes or the first language)\n\n- more layers\n\n- different internal rewards. Perhaps the reward for learning grammatical structure is the same that makes us\nlike music. Our brains may enjoy learning compositional regular structure, and they enjoy making\nthemselves understood, and everything else is something the universal cortical learning figures out on its\nown.\n\nThis is a hypothesis that is shared by a growing number of people these days. In humans, it is reflected for\ninstance by the fact that races with faster motor development have lower IQ. (In individuals of the same\ngroup, slower development often indicates defects, of course.)\n\nAnother support comes from machine learning: we find that the same learning functions can learn visual and\nauditory pattern recognition, and even end-to-end-learning. Google has built automatic image recognition\ninto their current photo app:\nhttp://blogs.wsj.com/digits/2015/07/01/google-mistakenly-tags-black-people-as-gorillas-showing-limits-of-\nalgorithms/\n\nThe state of the art in research can do better than that: it can begin to \"Imagine\" things. I.e. when the\nexperimenter asks the system to \"dream\" what a certain object looks like, the system can produce a\nsomewhat compelling image, which indicates that it is indeed learning visual structure. This stuff is\nsomething nobody could do a few months ago:\nhttp://www.creativeai.net/posts/Mv4WGé6rdzAerZF7ch/synthesizing-preferred-inputs-via-deep-generator-\n\nnetworks\n\nA machine learning program that can learn how to play an Atari game without any human supervision or\nhand-crafted engineering (the feat that gave DeepMind 500M from Google) now just takes about 130 lines of\nPython code.\n\nThese models do not have interesting motivational systems, and a relatively simple architecture. They\ncurrently seem to mimic some of the stuff that goes on in the first few layers of the cortex. They learn object\nfeatures, visual styles, lighting and rotation in 3d, and simple action policies. Almost everything else is\nmissing. But there is a lot of enthusiasm that the field might be on the right track, and that we can learn\nmotor simulations and intuitive physics soon. (The majority of the people in AI do not work on this,\nhowever. They try to improve the performance for the current benchmarks.)\n\nNoam's criticism of machine translation mostly applies to the Latent Semantic Analysis models that Google\nand others have been using for many years. These models map linguistic symbols to concepts, and relate\nconcepts to each other, but they do not relate the concepts to \"proper\" mental representations of what objects\nand processes look like and how they interact. Concepts are probably one of the top layers of the learning\nhierarchy, i.e. they are acquired *“after* we learn to simulate a mental world, not before. Classical linguists\nignored the simulation of a mental world entirely.\n\nHOUSE_OVERSIGHT_025964",
  "metadata": {
    "original_filename": "IMAGES-008-HOUSE_OVERSIGHT_025964.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3724,
    "word_count": 576,
    "line_count": 58,
    "import_date": "2025-11-19T21:47:45.750301",
    "prefix": "IMAGES-008"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_013031",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013031.txt",
  "text": "6.6 Analysis and Synthesis Processes in CogPrime 115\n\nWhere analysis is concerned:\n\ne PLN inference, acting on declarative knowledge, is used for estimating the probability of\nthe implication in the cognitive schematic, given fixed C, P and G. Episodic knowledge\nis also used in this regard, via enabling estimation of the probability via simple similarity\nmatching against past experience. Simulation is also used: multiple simulations may be run,\nand statistics may be captured therefrom.\n\n— Example: To estimate the degree to which asking Bob for food (the procedure P is “asking\nfor food”, the context C is “being with Bob”) will achieve the goal G of getting food, the\nvirtual dog may study its memory to see what happened on previous occasions where it\nor other dogs asked Bob for food or other things, and then integrate the evidence from\nthese occasions.\n\ne Procedural knowledge, mapped into declarative knowledge and then acted on by PLN in-\nference, can be useful for estimating the probability of the implication C A P > G, in cases\nwhere the probability of C A P, — G is known for some P, related to P.\n\n— Example: knowledge of the internal similarity between the procedure of asking for food\nand the procedure of asking for toys, allows the virtual dog to reason that if asking Bob\nfor toys has been successful, maybe asking Bob for food will be successful too.\n\ne Inference, acting on declarative or sensory knowledge, can be useful for estimating the\nprobability of the implication C A P > G, in cases where the probability of C; A P > G is\nknown for some C} related to C.\n\n— Example: if Bob and Jim have a lot of features in common, and Bob often responds\npositively when asked for food, then maybe Jim will too.\n\ne Inference can be used similarly for estimating the probability of the implication CA P > G,\nin cases where the probability of C A P > G, is known for some G, related to G. Concept\ncreation can be useful indirectly in calculating these probability estimates, via providing\nnew concepts that can be used to make useful inference trails more compact and hence\neasier to construct.\n\n— Example: The dog may reason that because Jack likes to play, and Jack and Jill are both\nchildren, maybe Jill likes to play too. It can carry out this reasoning only if its concept\ncreation process has invented the concept of “child” via analysis of observed data.\n\nIn these examples we have focused on cases where two terms in the cognitive schematic are\nfixed and the third must be filled in; but just as often, the situation is that only one of the\nterms is fixed. For instance, if we fix G, sometimes the best approach will be to collectively\nlearn C' and P. This requires either a procedure learning method that works interactively with a\ndeclarative-knowledge-focused concept learning or reasoning method; or a declarative learning\nmethod that works interactively with a procedure learning method. That is, it requires the sort\nof cognitive synergy built into the CogPrime design.\n\nHOUSE_OVERSIGHT_013031",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013031.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3022,
    "word_count": 523,
    "line_count": 50,
    "import_date": "2025-11-19T21:47:44.202957",
    "prefix": "IMAGES-002"
  }
}
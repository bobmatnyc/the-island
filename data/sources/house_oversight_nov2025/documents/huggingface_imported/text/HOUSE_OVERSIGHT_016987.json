{
  "document_id": "HOUSE_OVERSIGHT_016987",
  "filename": "IMAGES-004-HOUSE_OVERSIGHT_016987.txt",
  "text": "von Neumann and others on computers came directly not from Turing Machines but\nthrough this bypath of neural networks.\n\nBut simple neural networks didn’t do much. Frank Rosenblatt invented a learning\ndevice he called the perceptron, which was a one-layer neural network. In the late sixties,\nMarvin Minsky and Seymour Papert wrote a book titled Perceptrons, in which they\nbasically proved that perceptrons couldn’t do anything interesting, which is correct.\nPerceptrons could only make linear distinctions between things. So the idea was more or\nless dropped. People said, “These guys have written a proof that neural networks can’t\ndo anything interesting, therefore no neural networks can do anything interesting, so let’s\nforget about neural networks.” That attitude persisted for some time.\n\nMeanwhile, there were a couple of other approaches to AI. One was based on\nunderstanding, at a formal level, symbolically, how the world works; and the other was\nbased on doing statistics and probabilistic kinds of things. With regard to symbolic AI,\none of the test cases was, Can we teach a computer to do something like integrals? Can\nwe teach a computer to do calculus? There were tasks like machine translation, which\npeople thought would be a good example of what computers could do. The bottom line is\nthat by the early seventies, that approach had crashed.\n\nThen there was a trend toward devices called expert systems, which arose in the\nlate seventies and early eighties. The idea was to have a machine learn the rules that an\nexpert uses and thereby figure out what to do. That petered out. After that, AI became\nlittle more than a crazy pursuit.\n\nI had been interested in how you make an AJI-like machine since I was a kid. I was\ninterested particularly in how you take the knowledge we humans have accumulated in\nour civilization and automate answering questions on the basis of that knowledge. I\nthought about how you could do that symbolically, by building a system that could break\ndown questions into symbolic units and answer them. I worked on neural networks at\nthat time and didn’t make much progress, so I put it aside for a while.\n\nBack in mid-2002 to 2003, I thought about that question again: What does it take\nto make a computational knowledge system? The work I’d done by then pretty much\nshowed that my original belief about how to do this was completely wrong. My original\nbelief had been that in order to make a serious computational knowledge system, you first\nhad to build a brainlike device and then feed it knowledge—just as humans learn in\nstandard education. Now I realized that there wasn’t a bright line between what is\nintelligent and what is simply computational.\n\nI had assumed that there was some magic mechanism that made us vastly more\ncapable than anything that was just computational. But that assumption was wrong. This\ninsight is what led to Wolfram|Alpha. What I discovered is that you can take a large\ncollection of the world’s knowledge and automatically answer questions on the basis of\nit, using what are essentially merely computational techniques. It was an alternative way\nto do engineering—a way that’s much more analogous to what biology does in evolution.\n\nIn effect, what you normally do when you build a program 1s build it step-by-step.\nBut you can also explore the computational universe and mine technology from that\nuniverse. Typically, the challenge is the same as in physical mining: That is, you find a\nsupply of, let’s say, iron, or cobalt, or gadolinium, with some special magnetic properties,\n\n184\n\nHOUSE_OVERSIGHT_016987",
  "metadata": {
    "original_filename": "IMAGES-004-HOUSE_OVERSIGHT_016987.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3586,
    "word_count": 597,
    "line_count": 55,
    "import_date": "2025-11-19T21:47:46.066260",
    "prefix": "IMAGES-004"
  }
}
{
  "document_id": "HOUSE_OVERSIGHT_016310",
  "filename": "IMAGES-003-HOUSE_OVERSIGHT_016310.txt",
  "text": "How does one test for thinking? By the Turing Test? Unfortunately, that requires\na thinking judge. One might imagine a vast collaborative project on the Internet, where\nan AI hones its thinking abilities in conversations with human judges and becomes an\nAGI. But that assumes, among other things, that the longer the judge is unsure whether\nthe program is a person, the closer it is to being a person. There is no reason to expect\nthat.\n\nAnd how does one test for disobedience? Imagine Disobedience as a compulsory\nschool subject, with daily disobedience lessons and a disobedience test at the end of term.\n(Presumably with extra credit for not turning up for any of that.) This is paradoxical.\n\nSo, despite its usefulness in other applications, the programming technique of\ndefining a testable objective and training the program to meet it will have to be dropped.\nIndeed, I expect that any testing in the process of creating an AGI risks being\ncounterproductive, even immoral, just as in the education of humans. I share Turing’s\nsupposition that we’ll know an AGI when we see one, but this partial ability to recognize\nsuccess won’t help in creating the successful program.\n\nIn the broadest sense, a person’s quest for understanding is indeed a search\nproblem, in an abstract space of ideas far too large to be searched exhaustively. But there\nis no predetermined objective of this search. There is, as Popper put it, no criterion of\ntruth, nor of probable truth, especially in regard to explanatory knowledge. Objectives\nare ideas like any others—created as part of the search and continually modified and\nimproved. So inventing ways of disabling the program’s access to most of the space of\nideas won’t help—whether that disability is inflicted with the thumbscrew and stake or a\nmental straitjacket. To an AGI, the whole space of ideas must be open. It should not be\nknowable in advance what ideas the program can never contemplate. And the ideas that\nthe program does contemplate must be chosen by the program itself, using methods,\ncriteria, and objectives that are also the program’s own. Its choices, like an AI’s, will be\nhard to predict without running it (we lose no generality by assuming that the program is\ndeterministic; an AGI using a random generator would remain an AGI if the generator\nwere replaced by a pseudo-random one), but it will have the additional property that there\nis no way of proving, from its initial state, what it won ’t eventually think, short of\nrunning it.\n\nThe evolution of our ancestors is the only known case of thought starting up\nanywhere in the universe. As I have described, something went horribly wrong, and\nthere was no immediate explosion of innovation: Creativity was diverted into something\nelse. Yet not into transforming the planet into paper clips (pace Nick Bostrom). Rather,\nas we should also expect if an AGI project gets that far and fails, perverted creativity was\nunable to solve unexpected problems. This caused stasis and worse, thus tragically\ndelaying the transformation of anything into anything. But the Enlightenment has\nhappened since then. We know better now.\n\n90\n\nHOUSE_OVERSIGHT_016310",
  "metadata": {
    "original_filename": "IMAGES-003-HOUSE_OVERSIGHT_016310.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3159,
    "word_count": 523,
    "line_count": 47,
    "import_date": "2025-11-19T21:47:44.368642",
    "prefix": "IMAGES-003"
  }
}
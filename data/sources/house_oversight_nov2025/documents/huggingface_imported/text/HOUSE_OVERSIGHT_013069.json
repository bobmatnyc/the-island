{
  "document_id": "HOUSE_OVERSIGHT_013069",
  "filename": "IMAGES-002-HOUSE_OVERSIGHT_013069.txt",
  "text": "8.6 Cognitive Synergy for Procedural and Declarative Learning 153\n\ncreation can be useful indirectly in calculating these probability estimates, via providing\nnew concepts that can be used to make useful inference trails more compact and hence\neasier to construct.\n\n— Example: The dog may reason that because Jack likes to play, and Jack and Jill are both\nchildren, maybe Jill likes to play too. It can carry out this reasoning only if its concept\ncreation process has invented the concept of “child” via analysis of observed data.\n\nIn these examples we have focused on cases where two terms in the cognitive schematic are\nfixed and the third must be filled in; but just as often, the situation is that only one of the\nterms is fixed. For instance, if we fix G, sometimes the best approach will be to collectively\nlearn C' and P. This requires either a procedure learning method that works interactively with a\ndeclarative-knowledge-focused concept learning or reasoning method; or a declarative learning\nmethod that works interactively with a procedure learning method. That is, it requires the sort\nof cognitive synergy built into the CogPrime design.\n\n8.6 Cognitive Synergy for Procedural and Declarative Learning\n\nWe now present a little more algorithmic detail regarding the operation and synergetic in-\nteraction of CogPrime’s two most sophisticated components: the MOSES procedure learning\nalgorithm (see Chapter 33), and the PLN uncertain inference framework (see Chapter 34). The\ntreatment is necessarily quite compact, since we have not yet reviewed the details of either\nMOSES or PLN; but as well as illustrating the notion of cognitive synergy more concretely,\nperhaps the high-level discussion here will make clearer how MOSES and PLN fit into the big\npicture of CogPrime.\n\n8.6.1 Cognitive Synergy in MOSES\n\nMOSES, CogPrime’s primary algorithm for learning procedural knowledge, has been tested on\na variety of application problems including standard GP test problems, virtual agent control,\nbiological data analysis and text classification [Loo06]. It represents procedures internally as\nprogram trees. Each node in a MOSES program tree is supplied with a “knob,” comprising a\nset of values that may potentially be chosen to replace the data item or operator at that node.\nSo for instance a node containing the number 7 may be supplied with a knob that can take\non any integer value. A node containing a while loop may be supplied with a knob that can\ntake on various possible control flow operators including conditionals or the identity. A node\ncontaining a procedure representing a particular robot movement, may be supplied with a knob\nthat can take on values corresponding to multiple possible movements. Following a metaphor\nsuggested by Douglas Hofstadter [Hof96], MOSES learning covers both “knob twiddling” (setting\nthe values of knobs) and “knob creation.”\n\nMOSES is invoked within CogPrime in a number of ways, but most commonly for finding a\nprocedure P satisfying a probabilistic implication C& P - G as described above, where C is an\nobserved context and G is a system goal. In this case the probability value of the implication\nprovides the “scoring function” that MOSES uses to assess the quality of candidate procedures.\n\nHOUSE_OVERSIGHT_013069",
  "metadata": {
    "original_filename": "IMAGES-002-HOUSE_OVERSIGHT_013069.txt",
    "source_dataset": "huggingface:tensonaut/EPSTEIN_FILES_20K",
    "character_count": 3275,
    "word_count": 522,
    "line_count": 49,
    "import_date": "2025-11-19T21:47:45.898097",
    "prefix": "IMAGES-002"
  }
}
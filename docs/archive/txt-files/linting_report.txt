Epstein Document Archive - Linting Report
Generated: Thu Nov 20 21:34:26 EST 2025
==========================================

Ruff Violations:
DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> scripts/analysis/analyze_giuffre_maxwell_pdfs.py:153:24
    |
151 |         for fmt in formats:
152 |             try:
153 |                 return datetime.strptime(date_str.strip(), fmt)
    |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
154 |             except ValueError:
155 |                 continue
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/analyze_giuffre_maxwell_pdfs.py:315:14
    |
313 |         # Save full JSON
314 |         json_path = output_dir / "pdf_analysis_results.json"
315 |         with open(json_path, "w") as f:
    |              ^^^^
316 |             # Convert datetime objects to strings
317 |             results_copy = self.results.copy()
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/analyze_giuffre_maxwell_pdfs.py:331:14
    |
329 |         # Save report
330 |         report_path = output_dir / "PDF_ANALYSIS_REPORT.md"
331 |         with open(report_path, "w") as f:
    |              ^^^^
332 |             f.write(self.generate_report())
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/analyze_giuffre_maxwell_pdfs.py:336:14
    |
334 |         # Save email index
335 |         email_index_path = output_dir / "email_index.json"
336 |         with open(email_index_path, "w") as f:
    |              ^^^^
337 |             json.dump(self.results["emails"], f, indent=2, default=str)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/batch_entity_disambiguation.py:86:14
   |
84 |             raise FileNotFoundError(f"Entity index not found: {self.entity_index_path}")
85 |
86 |         with open(self.entity_index_path) as f:
   |              ^^^^
87 |             return json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/batch_entity_disambiguation.py:94:14
   |
92 |         backup_file = self.backup_path / f"ENTITIES_INDEX_{timestamp}.json"
93 |
94 |         with open(backup_file, "w") as f:
   |              ^^^^
95 |             json.dump(self.entity_index, f, indent=2)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/batch_entity_disambiguation.py:109:14
    |
108 |         # Save updated index
109 |         with open(self.entity_index_path, "w") as f:
    |              ^^^^
110 |             json.dump(self.entity_index, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/batch_entity_disambiguation.py:123:18
    |
121 |         changelog = []
122 |         if self.changelog_path.exists():
123 |             with open(self.changelog_path) as f:
    |                  ^^^^
124 |                 changelog = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/batch_entity_disambiguation.py:130:14
    |
129 |         # Save
130 |         with open(self.changelog_path, "w") as f:
    |              ^^^^
131 |             json.dump(changelog, f, indent=2)
    |
help: Replace with `Path.open()`

N806 Variable `G` in function should be lowercase
  --> scripts/analysis/build_knowledge_graph.py:25:5
   |
24 |     # Initialize graph
25 |     G = nx.Graph()
   |     ^
26 |
27 |     # Load data
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/build_knowledge_graph.py:28:10
   |
27 |     # Load data
28 |     with open(METADATA_DIR / "entity_network.json") as f:
   |          ^^^^
29 |         network = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/build_knowledge_graph.py:31:10
   |
29 |         network = json.load(f)
30 |
31 |     with open(METADATA_DIR / "entity_statistics.json") as f:
   |          ^^^^
32 |         stats = json.load(f).get("statistics", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/build_knowledge_graph.py:108:10
    |
106 |     }
107 |
108 |     with open(output_file, "w") as f:
    |          ^^^^
109 |         json.dump(graph_data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/build_unified_index.py:95:14
   |
93 |     classifications = {}
94 |     if email_classifications_file.exists():
95 |         with open(email_classifications_file) as f:
   |              ^^^^
96 |             data = json.load(f)
97 |             classifications = data.get("classifications", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/build_unified_index.py:108:18
    |
106 |         for metadata_file in metadata_files:
107 |             # Load metadata
108 |             with open(metadata_file) as f:
    |                  ^^^^
109 |                 metadata = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/build_unified_index.py:122:26
    |
120 |                 )
121 |                 if full_text_file.exists():
122 |                     with open(full_text_file) as f:
    |                          ^^^^
123 |                         text = f.read()
124 |                         doj_id = extract_doc_id_from_content(text)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/build_unified_index.py:247:10
    |
245 |     output_file = METADATA_DIR / "unified_document_index.json"
246 |
247 |     with open(output_file, "w") as f:
    |          ^^^^
248 |         json.dump(index, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/build_unified_index.py:257:10
    |
255 |     compact = {"generated": index["generated"], "statistics": index["statistics"]}
256 |
257 |     with open(compact_file, "w") as f:
    |          ^^^^
258 |         json.dump(compact, f, indent=2)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/analysis/build_unified_index.py:299:9
    |
297 |     except Exception as e:
298 |         print(f"\n❌ Error: {e}")
299 |         import traceback
    |         ^^^^^^^^^^^^^^^^
300 |
301 |         traceback.print_exc()
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/enrich_entity_relationships.py:103:14
    |
101 |             logger.warning(f"File not found: {path}")
102 |             return {}
103 |         with open(path) as f:
    |              ^^^^
104 |             return json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/enrich_entity_relationships.py:109:14
    |
107 |         """Save JSON file with pretty printing."""
108 |         path.parent.mkdir(parents=True, exist_ok=True)
109 |         with open(path, "w") as f:
    |              ^^^^
110 |             json.dump(data, f, indent=2)
111 |         logger.info(f"Saved to {path}")
    |
help: Replace with `Path.open()`

ARG002 Unused method argument: `relationship_types`
   --> scripts/analysis/enrich_entity_relationships.py:207:33
    |
206 |     def search_web_relationships(
207 |         self, entity_name: str, relationship_types: Optional[list[str]] = None
    |                                 ^^^^^^^^^^^^^^^^^^
208 |     ) -> list[dict]:
209 |         """
    |

B007 Loop control variable `key` not used within loop body
   --> scripts/analysis/enrich_entity_relationships.py:270:13
    |
268 |         # Merge sources for duplicate relationships
269 |         merged_relationships = []
270 |         for key, rels in grouped.items():
    |             ^^^
271 |             if len(rels) == 1:
272 |                 merged_relationships.append(rels[0])
    |
help: Rename unused `key` to `_key`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/enrich_entity_relationships.py:419:14
    |
417 |         # Save report
418 |         REPORT_PATH.parent.mkdir(parents=True, exist_ok=True)
419 |         with open(REPORT_PATH, "w") as f:
    |              ^^^^
420 |             f.write(report_text)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/entity_disambiguator.py:34:14
   |
32 |     def load_entities(self, entities_index_path: Path):
33 |         """Load entities from index"""
34 |         with open(entities_index_path) as f:
   |              ^^^^
35 |             data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/entity_disambiguator.py:251:14
    |
249 |         }
250 |
251 |         with open(output_path, "w") as f:
    |              ^^^^
252 |             json.dump(output_data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/entity_network.py:52:14
   |
50 |     def load_entities(self, entities_index_path: Path):
51 |         """Load entities from index"""
52 |         with open(entities_index_path) as f:
   |              ^^^^
53 |             data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/entity_network.py:77:14
   |
75 |     def load_flight_logs(self, flight_logs_stats_path: Path):
76 |         """Load flight logs and build co-occurrence network"""
77 |         with open(flight_logs_stats_path) as f:
   |              ^^^^
78 |             data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/entity_network.py:184:14
    |
182 |         }
183 |
184 |         with open(output_path, "w") as f:
    |              ^^^^
185 |             json.dump(graph_data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/entity_statistics.py:41:14
   |
39 |         entities_index = merged_index if merged_index.exists() else MD_DIR / "ENTITIES_INDEX.json"
40 |
41 |         with open(entities_index) as f:
   |              ^^^^
42 |             entity_data = json.load(f)
43 |             self.entities = entity_data.get("entities", entity_data)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/entity_statistics.py:46:14
   |
45 |         # Load semantic index
46 |         with open(METADATA_DIR / "semantic_index.json") as f:
   |              ^^^^
47 |             self.semantic_index = json.load(f).get("entity_to_documents", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/entity_statistics.py:50:14
   |
49 |         # Load network
50 |         with open(METADATA_DIR / "entity_network.json") as f:
   |              ^^^^
51 |             self.network_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/entity_statistics.py:54:14
   |
53 |         # Load classifications
54 |         with open(METADATA_DIR / "document_classifications.json") as f:
   |              ^^^^
55 |             self.classifications = json.load(f).get("results", {})
   |
help: Replace with `Path.open()`

ARG002 Unused method argument: `canonical_name`
   --> scripts/analysis/entity_statistics.py:114:36
    |
112 |         return stats
113 |
114 |     def get_entity_documents(self, canonical_name: str, name_variations: list[str]) -> list[dict]:
    |                                    ^^^^^^^^^^^^^^
115 |         """Get all documents mentioning an entity (any name variation)"""
116 |         all_docs = []
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/entity_statistics.py:176:14
    |
174 |         }
175 |
176 |         with open(output_path, "w") as f:
    |              ^^^^
177 |             json.dump(output_data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/final_entity_cleanup_complete.py:41:14
   |
40 |         # Load data
41 |         with open(filepath, encoding="utf-8") as f:
   |              ^^^^
42 |             data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/final_entity_cleanup_complete.py:69:14
   |
68 |         # Save updated data
69 |         with open(filepath, "w", encoding="utf-8") as f:
   |              ^^^^
70 |             json.dump(data, f, indent=2, ensure_ascii=False)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/final_entity_cleanup_complete.py:88:14
   |
86 |             return self.stats
87 |
88 |         with open(stats_path) as f:
   |              ^^^^
89 |             data = json.load(f)
90 |             entity_stats = data.get("statistics", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/final_entity_cleanup_complete.py:210:14
    |
209 |         # Save report
210 |         with open(report_path, "w") as f:
    |              ^^^^
211 |             f.write(report)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/analysis/final_entity_cleanup_complete.py:218:5
    |
216 | def main():
217 |     """Main entry point"""
218 |     import sys
    |     ^^^^^^^^^^
219 |
220 |     # Default data directory
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_data_quality.py:371:14
    |
369 |     def load_entities_index(self) -> dict:
370 |         """Load the entities index"""
371 |         with open(self.entities_index_path) as f:
    |              ^^^^
372 |             return json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_data_quality.py:376:14
    |
374 |     def save_entities_index(self, data: dict):
375 |         """Save the entities index"""
376 |         with open(self.entities_index_path, "w") as f:
    |              ^^^^
377 |             json.dump(data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_data_quality.py:383:14
    |
381 |         if not self.entity_network_path.exists():
382 |             return None
383 |         with open(self.entity_network_path) as f:
    |              ^^^^
384 |             return json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_data_quality.py:388:14
    |
386 |     def save_entity_network(self, data: dict):
387 |         """Save the entity network"""
388 |         with open(self.entity_network_path, "w") as f:
    |              ^^^^
389 |             json.dump(data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_data_quality.py:427:14
    |
426 |         # Extract all airport codes from flight logs
427 |         with open(self.flight_logs_path) as f:
    |              ^^^^
428 |             flights_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_data_quality.py:483:14
    |
481 |         }
482 |
483 |         with open(self.locations_path, "w") as f:
    |              ^^^^
484 |             json.dump(locations_output, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_data_quality.py:671:14
    |
669 |         # Write changes log
670 |         log_path = self.data_dir / "metadata" / "entity_quality_fixes_log.txt"
671 |         with open(log_path, "w") as f:
    |              ^^^^
672 |             f.write("ENTITY DATA QUALITY FIXES\n")
673 |             f.write("=" * 80 + "\n\n")
    |
help: Replace with `Path.open()`

SIM102 Use a single `if` statement instead of nested `if` statements
  --> scripts/analysis/fix_entity_name_formatting_correct.py:54:9
   |
52 |           parts_key = [p.strip() for p in entity_key.split(",", 1)]
53 |
54 | /         if len(parts_name) == 2 and len(parts_key) == 2:
55 | |             # If name parts are swapped compared to key, reverse them
56 | |             if parts_name[0] == parts_key[1] and parts_name[1] == parts_key[0]:
   | |_______________________________________________________________________________^
57 |                   name_clean = f"{parts_name[1]}, {parts_name[0]}"
58 |                   changes.append("fixed_reversed_name")
   |
help: Combine `if` statements using `and`

PLR0915 Too many statements (69 > 60)
  --> scripts/analysis/fix_entity_name_formatting_correct.py:66:5
   |
66 | def main():
   |     ^^^^
67 |     """Main execution function."""
68 |     # File paths
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/fix_entity_name_formatting_correct.py:77:10
   |
75 |     # Load data
76 |     print(f"\n1. Loading {stats_file}")
77 |     with open(stats_file) as f:
   |          ^^^^
78 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/fix_entity_name_formatting_correct.py:85:10
   |
83 |     # Create backup
84 |     print(f"\n2. Creating backup at {backup_file}")
85 |     with open(backup_file, "w") as f:
   |          ^^^^
86 |         json.dump(data, f, indent=2, ensure_ascii=False)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/fix_entity_name_formatting_correct.py:127:10
    |
125 |     # Write back
126 |     print(f"\n4. Writing corrected data back to {stats_file}")
127 |     with open(stats_file, "w") as f:
    |          ^^^^
128 |         json.dump(data, f, indent=2, ensure_ascii=False)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/mistral_entity_disambiguator.py:139:18
    |
137 |         entity_index_path = base_path / "data/md/entities/ENTITIES_INDEX.json"
138 |         if entity_index_path.exists():
139 |             with open(entity_index_path) as f:
    |                  ^^^^
140 |                 context["entity_index"] = json.load(f)
141 |             logger.info(f"Loaded {len(context['entity_index'].get('entities', []))} entities")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/mistral_entity_disambiguator.py:146:18
    |
144 |         flight_logs_path = base_path / "data/md/entities/flight_logs_by_flight.json"
145 |         if flight_logs_path.exists():
146 |             with open(flight_logs_path) as f:
    |                  ^^^^
147 |                 context["flight_logs"] = json.load(f)
148 |             logger.info(f"Loaded {len(context['flight_logs'])} flight records")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/rebuild_document_stats.py:146:10
    |
144 |         return {"total_documents": 0, "results": {}}
145 |
146 |     with open(classifications_file) as f:
    |          ^^^^
147 |         return json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/rebuild_document_stats.py:212:10
    |
210 |     output_file = METADATA_DIR / "comprehensive_document_stats.json"
211 |
212 |     with open(output_file, "w") as f:
    |          ^^^^
213 |         json.dump(stats, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/rebuild_document_stats.py:220:10
    |
218 |     report_file = METADATA_DIR / "comprehensive_document_stats.txt"
219 |
220 |     with open(report_file, "w") as f:
    |          ^^^^
221 |         f.write("=" * 80 + "\n")
222 |         f.write("COMPREHENSIVE DOCUMENT STATISTICS\n")
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/analysis/rebuild_document_stats.py:299:9
    |
297 |     except Exception as e:
298 |         print(f"\n❌ Error: {e}")
299 |         import traceback
    |         ^^^^^^^^^^^^^^^^
300 |
301 |         traceback.print_exc()
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/rebuild_flight_network.py:39:10
   |
37 |     print(f"Reading flight logs from {raw_text.name}...")
38 |
39 |     with open(raw_text, encoding="utf-8") as f:
   |          ^^^^
40 |         lines = f.readlines()
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/rebuild_flight_network.py:130:10
    |
128 |     # Save flights with passengers
129 |     flights_output = MD_DIR / "flight_logs_by_flight.json"
130 |     with open(flights_output, "w") as f:
    |          ^^^^
131 |         json.dump({"total_flights": len(flights_list), "flights": flights_list}, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/rebuild_flight_network.py:196:10
    |
194 |     # Load entity index
195 |     entities_index = MD_DIR / "ENTITIES_INDEX.json"
196 |     with open(entities_index) as f:
    |          ^^^^
197 |         entities_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/rebuild_flight_network.py:250:10
    |
248 |     }
249 |
250 |     with open(graph_output, "w") as f:
    |          ^^^^
251 |         json.dump(graph_data, f, indent=2)
    |
help: Replace with `Path.open()`

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> scripts/analysis/timeline_builder.py:24:21
   |
23 |       # Date patterns to match
24 |       DATE_PATTERNS = [
   |  _____________________^
25 | |         # MM/DD/YYYY or M/D/YYYY
26 | |         (r"(\d{1,2})/(\d{1,2})/(\d{4})", lambda m: f"{m[3]}-{m[1]:0>2}-{m[2]:0>2}"),
27 | |         # Month DD, YYYY
28 | |         (
29 | |             r"(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})",
30 | |             lambda m: f"{m[3]}-{TimelineBuilder.month_to_num(m[1]):0>2}-{m[2]:0>2}",
31 | |         ),
32 | |         # YYYY-MM-DD
33 | |         (r"(\d{4})-(\d{2})-(\d{2})", lambda m: f"{m[1]}-{m[2]}-{m[3]}"),
34 | |         # DD Month YYYY
35 | |         (
36 | |             r"(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{4})",
37 | |             lambda m: f"{m[3]}-{TimelineBuilder.month_to_num(m[2]):0>2}-{m[1]:0>2}",
38 | |         ),
39 | |     ]
   | |_____^
40 |
41 |       @staticmethod
   |

ARG002 Unused method argument: `doc_path`
  --> scripts/analysis/timeline_builder.py:65:50
   |
63 |         self.date_index = defaultdict(list)
64 |
65 |     def extract_dates_from_text(self, text: str, doc_path: str) -> list[tuple[str, int]]:
   |                                                  ^^^^^^^^
66 |         """
67 |         Extract dates from text
   |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
  --> scripts/analysis/timeline_builder.py:81:21
   |
80 |                     # Validate date
81 |                     datetime.strptime(formatted_date, "%Y-%m-%d")
   |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
82 |
83 |                     dates_found.append((formatted_date, match.start()))
   |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/analysis/timeline_builder.py:177:14
    |
175 |         }
176 |
177 |         with open(output_path, "w") as f:
    |              ^^^^
178 |             json.dump(timeline_data, f, indent=2)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (69 > 60)
  --> scripts/analysis/verify_entity_filtering.py:31:5
   |
31 | def main():
   |     ^^^^
32 |     """Verify entity filtering implementation"""
33 |     print("=" * 70)
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/verify_entity_filtering.py:57:14
   |
56 |     if network_path.exists():
57 |         with open(network_path) as f:
   |              ^^^^
58 |             network_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/analysis/verify_entity_filtering.py:84:14
   |
83 |     if stats_path.exists():
84 |         with open(stats_path) as f:
   |              ^^^^
85 |             stats_data = json.load(f)
   |
help: Replace with `Path.open()`

RUF001 String contains ambiguous `ℹ` (INFORMATION SOURCE). Did you mean `i` (LATIN SMALL LETTER I)?
  --> scripts/analysis/verify_entity_filtering.py:97:25
   |
96 |         if generic_in_stats:
97 |             print("\n   ℹ️  NOTE: Generic entities in statistics (not displayed in UI):")
   |                         ^
98 |             for name in generic_in_stats:
99 |                 print(f"      - {name}")
   |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> scripts/analysis/web_relationship_finder.py:20:29
   |
19 |       # Relationship keywords to look for in search results
20 |       RELATIONSHIP_KEYWORDS = {
   |  _____________________________^
21 | |         "spouse": ["wife", "husband", "spouse", "married to", "married"],
22 | |         "child_of": ["son of", "daughter of", "child of", "children of"],
23 | |         "parent_of": ["father of", "mother of", "parent of"],
24 | |         "sibling_of": ["brother of", "sister of", "sibling of"],
25 | |         "business_partner": ["partner", "co-founder", "business partner", "founded with"],
26 | |         "employee": ["works for", "employed by", "employee of", "worked for"],
27 | |         "associate": ["associate", "associated with", "linked to", "connected to"],
28 | |     }
   | |_____^
29 |
30 |       def __init__(self, use_live_search: bool = False):
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/canonicalization/canonicalize.py:177:18
    |
175 |         """
176 |         try:
177 |             with open(pdf_file, "rb") as f:
    |                  ^^^^
178 |                 reader = PyPDF2.PdfReader(f)
179 |                 text = ""
    |
help: Replace with `Path.open()`

ARG002 Unused method argument: `hashes`
   --> scripts/canonicalization/canonicalize.py:252:9
    |
250 |         source_name: str,
251 |         source_metadata: dict,
252 |         hashes: dict,
    |         ^^^^^^
253 |         text: str,
254 |     ):
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/canonicalization/canonicalize.py:301:13
    |
300 |             # Extract email metadata
301 |             import re
    |             ^^^^^^^^^
302 |
303 |             from_match = re.search(r"From:\s*([^\n]+)", text)
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/canonicalization/canonicalize.py:366:14
    |
365 |         # Write markdown
366 |         with open(output_file, "w") as f:
    |              ^^^^
367 |             f.write(frontmatter)
368 |             f.write("\n\n")
    |
help: Replace with `Path.open()`

ARG002 Unused method argument: `metadata`
   --> scripts/canonicalization/canonicalize.py:371:69
    |
369 |             f.write(text)
370 |
371 |     def _generate_frontmatter(self, doc: dict, sources: list[dict], metadata: dict) -> str:
    |                                                                     ^^^^^^^^
372 |         """Generate YAML frontmatter."""
373 |         import yaml
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/canonicalization/canonicalize.py:373:9
    |
371 |     def _generate_frontmatter(self, doc: dict, sources: list[dict], metadata: dict) -> str:
372 |         """Generate YAML frontmatter."""
373 |         import yaml
    |         ^^^^^^^^^^^
374 |
375 |         frontmatter_data = {
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/canonicalization/canonicalize_emails.py:338:10
    |
336 |     index = create_email_index()
337 |     index_path = Path("/Users/masa/Projects/Epstein/data/canonical/emails/email_index.json")
338 |     with open(index_path, "w") as f:
    |          ^^^^
339 |         json.dump(index, f, indent=2)
340 |     print(f"✓ Created email index: {index_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/canonicalization/canonicalize_emails.py:345:10
    |
343 |     stats = generate_statistics()
344 |     stats_path = Path("/Users/masa/Projects/Epstein/data/canonical/emails/email_statistics.json")
345 |     with open(stats_path, "w") as f:
    |          ^^^^
346 |         json.dump(stats, f, indent=2)
347 |     print(f"✓ Generated statistics: {stats_path}")
    |
help: Replace with `Path.open()`

ARG001 Unused function argument: `db`
   --> scripts/canonicalization/initialize_deduplication.py:251:27
    |
251 | def test_bulk_performance(db: CanonicalDatabase):
    |                           ^^
252 |     """
253 |     Test bulk insertion performance.
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/canonicalization/initialize_deduplication.py:262:5
    |
261 |     # Simulate processing time
262 |     import time
    |     ^^^^^^^^^^^
263 |
264 |     test_texts = [f"This is test email {i} with some content" for i in range(100)]
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/canonicalization/query_deduplication.py:310:9
    |
308 |     elif format == "csv":
309 |         # Convert to CSV
310 |         import csv
    |         ^^^^^^^^^^
311 |
312 |         with output_file.open("w", newline="") as f:
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/classification/classify_all_documents.py:30:14
   |
28 |     def __init__(self, entities_index_path: Path):
29 |         """Load entity index"""
30 |         with open(entities_index_path) as f:
   |              ^^^^
31 |             self.entities_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/classification/classify_all_documents.py:136:10
    |
134 |     # Save classification results
135 |     classification_path = METADATA_DIR / "document_classifications.json"
136 |     with open(classification_path, "w") as f:
    |          ^^^^
137 |         json.dump(
138 |             {
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/classification/classify_all_documents.py:151:10
    |
149 |     # Save semantic index (entity -> documents)
150 |     semantic_index_path = METADATA_DIR / "semantic_index.json"
151 |     with open(semantic_index_path, "w") as f:
    |          ^^^^
152 |         json.dump(
153 |             {
    |
help: Replace with `Path.open()`

E402 Module level import not at top of file
  --> scripts/classification/classify_emails.py:22:1
   |
21 | # Import existing classifier
22 | from classification.document_classifier import DocumentClassifier
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/classification/classify_emails.py:33:10
   |
31 | def load_email_data(metadata_file: Path) -> dict[str, Any]:
32 |     """Load email metadata and full text."""
33 |     with open(metadata_file) as f:
   |          ^^^^
34 |         metadata = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/classification/classify_emails.py:42:14
   |
41 |     if full_text_file.exists():
42 |         with open(full_text_file) as f:
   |              ^^^^
43 |             full_text = f.read()
44 |     else:
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/classification/classify_emails.py:218:14
    |
216 |     # Load existing classifications
217 |     if CLASSIFICATIONS_FILE.exists():
218 |         with open(CLASSIFICATIONS_FILE) as f:
    |              ^^^^
219 |             existing = json.load(f)
220 |     else:
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/classification/classify_emails.py:239:10
    |
237 | def save_classifications(classifications: dict[str, Any]) -> None:
238 |     """Save classifications to file."""
239 |     with open(CLASSIFICATIONS_FILE, "w") as f:
    |          ^^^^
240 |         json.dump(classifications, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/classification/classify_emails.py:255:10
    |
253 |     }
254 |
255 |     with open(email_classifications_file, "w") as f:
    |          ^^^^
256 |         json.dump(email_only, f, indent=2)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/classification/classify_emails.py:303:9
    |
301 |     except Exception as e:
302 |         print(f"\n❌ Error: {e}")
303 |         import traceback
    |         ^^^^^^^^^^^^^^^^
304 |
305 |         traceback.print_exc()
    |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
   --> scripts/classification/document_classifier.py:44:16
    |
 43 |       # Keyword patterns for each document type
 44 |       PATTERNS = {
    |  ________________^
 45 | |         DocumentType.EMAIL: {
 46 | |             "keywords": [
 47 | |                 r"From:\s*[\w\s@.-]+",
 48 | |                 r"To:\s*[\w\s@.-]+",
 49 | |                 r"Subject:\s*.+",
 50 | |                 r"Date:\s*\d{1,2}/\d{1,2}/\d{2,4}",
 51 | |                 r"Sent:\s*\w+",
 52 | |                 r"CC:\s*[\w\s@.-]+",
 53 | |                 r"@\w+\.\w+",  # Email addresses
 54 | |                 r"Re:\s*.+",
 55 | |                 r"Fwd:\s*.+",
 56 | |             ],
 57 | |             "weight": 1.0,
 58 | |             "min_matches": 3,
 59 | |         },
 60 | |         DocumentType.COURT_FILING: {
 61 | |             "keywords": [
 62 | |                 r"UNITED STATES DISTRICT COURT",
 63 | |                 r"SOUTHERN DISTRICT OF",
 64 | |                 r"CASE NO\.",
 65 | |                 r"Plaintiff[s]?",
 66 | |                 r"Defendant[s]?",
 67 | |                 r"MOTION TO",
 68 | |                 r"COMPLAINT",
 69 | |                 r"DEPOSITION",
 70 | |                 r"AFFIDAVIT",
 71 | |                 r"EXHIBIT\s+[A-Z0-9]+",
 72 | |                 r"WHEREFORE",
 73 | |                 r"Respectfully submitted",
 74 | |                 r"DECLARATION OF",
 75 | |                 r"COMES NOW",
 76 | |                 r"v\.",  # versus in case names
 77 | |                 r"Counsel for",
 78 | |             ],
 79 | |             "weight": 1.0,
 80 | |             "min_matches": 4,
 81 | |         },
 82 | |         DocumentType.FINANCIAL: {
 83 | |             "keywords": [
 84 | |                 r"\$[\d,]+\.?\d*",  # Dollar amounts
 85 | |                 r"INVOICE",
 86 | |                 r"STATEMENT",
 87 | |                 r"WIRE TRANSFER",
 88 | |                 r"ACCOUNT NUMBER",
 89 | |                 r"ROUTING NUMBER",
 90 | |                 r"TAX RETURN",
 91 | |                 r"BALANCE",
 92 | |                 r"PAYMENT",
 93 | |                 r"TRANSACTION",
 94 | |                 r"CREDIT",
 95 | |                 r"DEBIT",
 96 | |                 r"JPMorgan",
 97 | |                 r"Deutsche Bank",
 98 | |                 r"Chase",
 99 | |                 r"SWIFT",
100 | |             ],
101 | |             "weight": 0.9,
102 | |             "min_matches": 3,
103 | |         },
104 | |         DocumentType.FLIGHT_LOG: {
105 | |             "keywords": [
106 | |                 r"PASSENGER",
107 | |                 r"DEPARTURE",
108 | |                 r"ARRIVAL",
109 | |                 r"AIRCRAFT",
110 | |                 r"TAIL NUMBER",
111 | |                 r"N\d{3,5}[A-Z]{1,2}",  # Tail number pattern
112 | |                 r"FLIGHT\s+LOG",
113 | |                 r"MANIFEST",
114 | |                 r"CREW",
115 | |                 r"ROUTE",
116 | |                 r"TEB",  # Teterboro Airport
117 | |                 r"PBI",  # Palm Beach Airport
118 | |                 r"Gulfstream",
119 | |             ],
120 | |             "weight": 1.0,
121 | |             "min_matches": 4,
122 | |         },
123 | |         DocumentType.CONTACT_BOOK: {
124 | |             "keywords": [
125 | |                 r"ADDRESS BOOK",
126 | |                 r"CONTACTS",
127 | |                 r"PHONE:\s*[\d\s\-\(\)]+",
128 | |                 r"MOBILE:\s*[\d\s\-\(\)]+",
129 | |                 r"EMAIL:\s*[\w@.-]+",
130 | |                 r"ADDRESS:",
131 | |                 r"FAX:\s*[\d\s\-\(\)]+",
132 | |                 r"ASSISTANT:",
133 | |                 r"BLACK BOOK",
134 | |                 r"LITTLE BLACK BOOK",
135 | |             ],
136 | |             "weight": 1.0,
137 | |             "min_matches": 4,
138 | |         },
139 | |         DocumentType.INVESTIGATIVE: {
140 | |             "keywords": [
141 | |                 r"FBI",
142 | |                 r"INVESTIGATION",
143 | |                 r"AGENT",
144 | |                 r"INTERVIEW",
145 | |                 r"WITNESS",
146 | |                 r"EVIDENCE",
147 | |                 r"SURVEILLANCE",
148 | |                 r"SUBPOENA",
149 | |                 r"GRAND JURY",
150 | |                 r"SEARCH WARRANT",
151 | |                 r"PROBABLE CAUSE",
152 | |                 r"CONFIDENTIAL",
153 | |                 r"CLASSIFIED",
154 | |             ],
155 | |             "weight": 0.95,
156 | |             "min_matches": 3,
157 | |         },
158 | |         DocumentType.LEGAL_AGREEMENT: {
159 | |             "keywords": [
160 | |                 r"AGREEMENT",
161 | |                 r"CONTRACT",
162 | |                 r"WHEREAS",
163 | |                 r"NOW THEREFORE",
164 | |                 r"PARTY OF THE FIRST PART",
165 | |                 r"CONSIDERATION",
166 | |                 r"NON-DISCLOSURE",
167 | |                 r"NDA",
168 | |                 r"SETTLEMENT",
169 | |                 r"INDEMNIFICATION",
170 | |                 r"BINDING",
171 | |                 r"EXECUTED THIS",
172 | |             ],
173 | |             "weight": 0.95,
174 | |             "min_matches": 4,
175 | |         },
176 | |         DocumentType.PERSONAL: {
177 | |             "keywords": [
178 | |                 r"DIARY",
179 | |                 r"JOURNAL",
180 | |                 r"PERSONAL NOTE",
181 | |                 r"BIRTHDAY",
182 | |                 r"CALENDAR",
183 | |                 r"SCHEDULE",
184 | |                 r"APPOINTMENT",
185 | |                 r"REMINDER",
186 | |                 r"TO DO",
187 | |                 r"MEMO TO SELF",
188 | |             ],
189 | |             "weight": 0.85,
190 | |             "min_matches": 2,
191 | |         },
192 | |         DocumentType.MEDIA: {
193 | |             "keywords": [
194 | |                 r"PRESS RELEASE",
195 | |                 r"NEWS ARTICLE",
196 | |                 r"JOURNALIST",
197 | |                 r"REPORTER",
198 | |                 r"PUBLICATION",
199 | |                 r"New York Times",
200 | |                 r"Washington Post",
201 | |                 r"Bloomberg",
202 | |                 r"Miami Herald",
203 | |                 r"INTERVIEW TRANSCRIPT",
204 | |                 r"ON THE RECORD",
205 | |             ],
206 | |             "weight": 0.9,
207 | |             "min_matches": 2,
208 | |         },
209 | |         DocumentType.ADMINISTRATIVE: {
210 | |             "keywords": [
211 | |                 r"MEMORANDUM",
212 | |                 r"POLICY",
213 | |                 r"PROCEDURE",
214 | |                 r"GUIDELINES",
215 | |                 r"INTERNAL",
216 | |                 r"HR",
217 | |                 r"EMPLOYEE",
218 | |                 r"STAFF",
219 | |                 r"OFFICE",
220 | |                 r"ADMINISTRATIVE",
221 | |             ],
222 | |             "weight": 0.8,
223 | |             "min_matches": 2,
224 | |         },
225 | |     }
    | |_____^
226 |
227 |       def __init__(self):
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/core/deduplicator.py:259:17
    |
257 |         if doc_a.fuzzy_hash and doc_b.fuzzy_hash:
258 |             try:
259 |                 import ssdeep
    |                 ^^^^^^^^^^^^^
260 |
261 |                 # Strip 'ssdeep:' prefix
    |

PLC0415 `import` should be at the top-level of a file
  --> scripts/core/hasher.py:39:13
   |
37 |         """Check if ssdeep library is available for fuzzy hashing."""
38 |         try:
39 |             import ssdeep
   |             ^^^^^^^^^^^^^
40 |
41 |             return True
   |

F401 `ssdeep` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> scripts/core/hasher.py:39:20
   |
37 |         """Check if ssdeep library is available for fuzzy hashing."""
38 |         try:
39 |             import ssdeep
   |                    ^^^^^^
40 |
41 |             return True
   |
help: Remove unused import: `ssdeep`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/core/hasher.py:59:14
   |
57 |         sha256 = hashlib.sha256()
58 |
59 |         with open(file_path, "rb") as f:
   |              ^^^^
60 |             # Read in 8KB chunks for memory efficiency
61 |             while chunk := f.read(8192):
   |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/core/hasher.py:104:9
    |
102 |             return None
103 |
104 |         import ssdeep
    |         ^^^^^^^^^^^^^
105 |
106 |         return f"ssdeep:{ssdeep.hash(text)}"
    |

RUF001 String contains ambiguous `‐` (HYPHEN). Did you mean `-` (HYPHEN-MINUS)?
   --> scripts/core/hasher.py:143:30
    |
141 |         # Remove common OCR artifacts
142 |         # Replace common OCR misreads
143 |         text = text.replace("‐", "-")  # Unicode hyphen to ASCII
    |                              ^
144 |         text = text.replace("–", "-")  # En dash to hyphen
145 |         text = text.replace("—", "-")  # Em dash to hyphen
    |

RUF001 String contains ambiguous `–` (EN DASH). Did you mean `-` (HYPHEN-MINUS)?
   --> scripts/core/hasher.py:144:30
    |
142 |         # Replace common OCR misreads
143 |         text = text.replace("‐", "-")  # Unicode hyphen to ASCII
144 |         text = text.replace("–", "-")  # En dash to hyphen
    |                              ^
145 |         text = text.replace("—", "-")  # Em dash to hyphen
146 |         text = text.replace('"', '"').replace('"', '"')  # Smart quotes
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/core/hasher.py:201:9
    |
199 |             ImportError: If ssdeep not available
200 |         """
201 |         import ssdeep
    |         ^^^^^^^^^^^^^
202 |
203 |         # Strip 'ssdeep:' prefix if present
    |

F821 Undefined name `Optional`
  --> scripts/core/ocr_quality.py:37:41
   |
35 |     """
36 |
37 |     def __init__(self, dictionary_path: Optional[Path] = None):
   |                                         ^^^^^^^^
38 |         """
39 |         Initialize OCR quality assessor.
   |

F821 Undefined name `Optional`
  --> scripts/core/ocr_quality.py:47:49
   |
45 |         self.dictionary = self._load_dictionary(dictionary_path)
46 |
47 |     def _load_dictionary(self, dictionary_path: Optional[Path]) -> set[str]:
   |                                                 ^^^^^^^^
48 |         """
49 |         Load word dictionary for lexical validation.
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/core/ocr_quality.py:55:18
   |
53 |         """
54 |         if dictionary_path and dictionary_path.exists():
55 |             with open(dictionary_path) as f:
   |                  ^^^^
56 |                 return {line.strip().lower() for line in f if line.strip()}
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/add_entity_aliases.py:29:10
   |
27 |     # Load index
28 |     print(f"Loading entity index from: {index_path}")
29 |     with open(index_path) as f:
   |          ^^^^
30 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/add_entity_aliases.py:138:10
    |
136 |     print("Saving updated index...")
137 |     print(f"{'='*60}")
138 |     with open(index_path, "w") as f:
    |          ^^^^
139 |         json.dump(data, f, indent=2)
    |
help: Replace with `Path.open()`

SIM102 Use a single `if` statement instead of nested `if` statements
   --> scripts/data_quality/categorize_documents.py:198:9
    |
197 |           # 4. Keep existing classification if we have no better match
198 | /         if classification == "unknown" and existing_classification:
199 | |             if existing_classification not in {"unknown", "administrative"}:
    | |____________________________________________________________________________^
200 |                   classification = existing_classification
201 |                   confidence = 0.5
    |
help: Combine `if` statements using `and`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/categorize_documents.py:239:10
    |
238 |     # Load master document index
239 |     with open(input_file) as f:
    |          ^^^^
240 |         data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/categorize_documents.py:248:14
    |
246 |     if backup:
247 |         backup_path = f"{input_file}.backup"
248 |         with open(backup_path, "w") as f:
    |              ^^^^
249 |             json.dump(data, f, indent=2)
250 |         logger.info(f"Created backup at {backup_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/categorize_documents.py:302:10
    |
300 |     # Save categorized index
301 |     logger.info(f"Saving categorized index to {output_file}")
302 |     with open(output_file, "w") as f:
    |          ^^^^
303 |         json.dump(data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/fix_biography_names_v3.py:51:10
   |
49 |         Set of entity names (exact format from entity system)
50 |     """
51 |     with open(entity_index_path, encoding="utf-8") as f:
   |          ^^^^
52 |         index = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/fix_biography_names_v3.py:127:10
    |
126 |     # Load biographies
127 |     with open(bio_file, encoding="utf-8") as f:
    |          ^^^^
128 |         data = json.load(f)
    |
help: Replace with `Path.open()`

RUF001 String contains ambiguous `ℹ` (INFORMATION SOURCE). Did you mean `i` (LATIN SMALL LETTER I)?
   --> scripts/data_quality/fix_biography_names_v3.py:186:23
    |
184 |     not_in_system_list = [c for c in conversions if c["match_method"] == "not_in_system"]
185 |     if not_in_system_list:
186 |         print(f"\n=== ℹ️  Biographies Not in Entity System ({len(not_in_system_list)}) ===\n")
    |                       ^
187 |         print("These are legitimate biographies without corresponding entity entries:")
188 |         for c in not_in_system_list:
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/fix_biography_names_v3.py:222:14
    |
221 |         # Write converted data
222 |         with open(bio_file, "w", encoding="utf-8") as f:
    |              ^^^^
223 |             json.dump(converted_data, f, indent=2, ensure_ascii=False)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/fix_biography_names_v3.py:229:14
    |
227 |         # Write log
228 |         log_file = bio_file.parent / "biography_name_conversion_log_final.json"
229 |         with open(log_file, "w", encoding="utf-8") as f:
    |              ^^^^
230 |             json.dump(
231 |                 {
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/data_quality/fix_biography_names_v3.py:265:9
    |
263 |     except Exception as e:
264 |         print(f"\n❌ Error: {e}")
265 |         import traceback
    |         ^^^^^^^^^^^^^^^^
266 |
267 |         traceback.print_exc()
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/fix_flight_counts.py:30:10
   |
28 | def load_json(filepath):
29 |     """Load JSON file"""
30 |     with open(filepath, encoding="utf-8") as f:
   |          ^^^^
31 |         return json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/fix_flight_counts.py:36:10
   |
34 | def save_json(filepath, data):
35 |     """Save JSON file with formatting"""
36 |     with open(filepath, "w", encoding="utf-8") as f:
   |          ^^^^
37 |         json.dump(data, f, indent=2, ensure_ascii=False)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (62 > 60)
  --> scripts/data_quality/generate_entity_mappings.py:54:5
   |
54 | def generate_mappings():
   |     ^^^^^^^^^^^^^^^^^
55 |     """
56 |     Generate entity mappings from normalized entities and normalization report.
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/generate_entity_mappings.py:71:10
   |
69 |     # Load normalization report to extract all name transformations
70 |     report_path = project_root / "data/metadata/normalization_report.txt"
71 |     with open(report_path) as f:
   |          ^^^^
72 |         lines = f.readlines()
   |
help: Replace with `Path.open()`

PLW2901 `for` loop variable `line` overwritten by assignment target
  --> scripts/data_quality/generate_entity_mappings.py:82:13
   |
80 |             break
81 |         if in_normalization_section and "→" in line:
82 |             line = line.strip()
   |             ^^^^
83 |             if line and not line.startswith("-"):
84 |                 parts = line.split(" → ")
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/generate_entity_mappings.py:92:10
   |
90 |     # Load entities index for merge information
91 |     entities_path = project_root / "data/md/entities/ENTITIES_INDEX.json"
92 |     with open(entities_path) as f:
   |          ^^^^
93 |         entities_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/generate_entity_mappings.py:104:10
    |
102 |     # Extract all name variations from raw flight logs
103 |     raw_file = project_root / "data/raw/entities/flight_logs_raw.txt"
104 |     with open(raw_file, encoding="utf-8") as f:
    |          ^^^^
105 |         raw_lines = f.readlines()
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/generate_entity_mappings.py:143:10
    |
141 |     # Save mappings
142 |     output_path = project_root / "data/metadata/entity_name_mappings.json"
143 |     with open(output_path, "w") as f:
    |          ^^^^
144 |         json.dump(mappings, f, indent=2, sort_keys=True)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (64 > 60)
  --> scripts/data_quality/merge_categorizations.py:18:5
   |
18 | def merge_categorizations(
   |     ^^^^^^^^^^^^^^^^^^^^^
19 |     master_categorized: str, all_documents: str, output_file: str, backup: bool = True
20 | ) -> dict:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/merge_categorizations.py:34:10
   |
32 |     """
33 |     logger.info(f"Loading master categorized index from {master_categorized}")
34 |     with open(master_categorized) as f:
   |          ^^^^
35 |         master_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/merge_categorizations.py:38:10
   |
37 |     logger.info(f"Loading all documents index from {all_documents}")
38 |     with open(all_documents) as f:
   |          ^^^^
39 |         all_docs_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/merge_categorizations.py:44:14
   |
42 |     if backup:
43 |         backup_path = f"{all_documents}.backup"
44 |         with open(backup_path, "w") as f:
   |              ^^^^
45 |             json.dump(all_docs_data, f, indent=2)
46 |         logger.info(f"Created backup at {backup_path}")
   |
help: Replace with `Path.open()`

SIM102 Use a single `if` statement instead of nested `if` statements
   --> scripts/data_quality/merge_categorizations.py:98:9
    |
 97 |           # If not matched and is PDF, mark as administrative with low confidence
 98 | /         if not matched and doc.get("type") == "pdf":
 99 | |             if doc.get("classification") == "unknown" or not doc.get("classification"):
    | |_______________________________________________________________________________________^
100 |                   doc["classification"] = "administrative"
101 |                   doc["classification_confidence"] = 0.3
    |
help: Combine `if` statements using `and`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/merge_categorizations.py:130:10
    |
128 |     # Save updated index
129 |     logger.info(f"Saving updated index to {output_file}")
130 |     with open(output_file, "w") as f:
    |          ^^^^
131 |         json.dump(all_docs_data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/merge_epstein_duplicates.py:24:10
   |
22 | def load_json(filepath):
23 |     """Load JSON file"""
24 |     with open(filepath, encoding="utf-8") as f:
   |          ^^^^
25 |         return json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/merge_epstein_duplicates.py:30:10
   |
28 | def save_json(filepath, data):
29 |     """Save JSON file with formatting"""
30 |     with open(filepath, "w", encoding="utf-8") as f:
   |          ^^^^
31 |         json.dump(data, f, indent=2, ensure_ascii=False)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (106 > 60)
  --> scripts/data_quality/merge_epstein_duplicates.py:34:5
   |
34 | def merge_epstein_entities():
   |     ^^^^^^^^^^^^^^^^^^^^^^
35 |     """Find and merge all Epstein, Jeffrey entities"""
36 |     print("Loading ENTITIES_INDEX.json...")
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/merge_epstein_duplicates.py:276:10
    |
275 |     # Save report
276 |     with open(REPORT_FILE, "w", encoding="utf-8") as f:
    |          ^^^^
277 |         f.write(report_text)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (79 > 60)
  --> scripts/data_quality/merge_royal_duplicates.py:16:5
   |
16 | def merge_duplicates():
   |     ^^^^^^^^^^^^^^^^
17 |     """Merge duplicate royal entities and add aliases."""
18 |     entities_dir = Path("data/md/entities")
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/merge_royal_duplicates.py:23:10
   |
21 |     # Load index
22 |     print(f"Loading entity index from: {index_path}")
23 |     with open(index_path) as f:
   |          ^^^^
24 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/merge_royal_duplicates.py:141:10
    |
139 |     print("Saving updated index...")
140 |     print(f"{'='*60}")
141 |     with open(index_path, "w") as f:
    |          ^^^^
142 |         json.dump(data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:323:14
    |
321 |         print("\n[Flight Logs] Normalizing passenger names...")
322 |
323 |         with open(flight_logs_path) as f:
    |              ^^^^
324 |             data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:328:14
    |
326 |         # Create backup
327 |         backup_path = flight_logs_path + ".backup"
328 |         with open(backup_path, "w") as f:
    |              ^^^^
329 |             json.dump(data, f, indent=2)
330 |         print(f"  ✓ Created backup: {backup_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:351:14
    |
349 |         # Atomic write
350 |         temp_path = output_path + ".tmp"
351 |         with open(temp_path, "w") as f:
    |              ^^^^
352 |             json.dump(data, f, indent=2)
353 |         os.replace(temp_path, output_path)
    |
help: Replace with `Path.open()`

PTH105 `os.replace()` should be replaced by `Path.replace()`
   --> scripts/data_quality/normalize_entity_names.py:353:9
    |
351 |         with open(temp_path, "w") as f:
352 |             json.dump(data, f, indent=2)
353 |         os.replace(temp_path, output_path)
    |         ^^^^^^^^^^
354 |
355 |         print(f"  ✓ Normalized {total_normalized} passenger names across {len(flights)} flights")
    |
help: Replace with `Path(...).replace(...)`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:378:14
    |
376 |         print("\n[Entities Index] Normalizing and deduplicating...")
377 |
378 |         with open(index_path) as f:
    |              ^^^^
379 |             data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:383:14
    |
381 |         # Create backup
382 |         backup_path = index_path + ".backup"
383 |         with open(backup_path, "w") as f:
    |              ^^^^
384 |             json.dump(data, f, indent=2)
385 |         print(f"  ✓ Created backup: {backup_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:428:14
    |
426 |         # Atomic write
427 |         temp_path = output_path + ".tmp"
428 |         with open(temp_path, "w") as f:
    |              ^^^^
429 |             json.dump(data, f, indent=2)
430 |         os.replace(temp_path, output_path)
    |
help: Replace with `Path.open()`

PTH105 `os.replace()` should be replaced by `Path.replace()`
   --> scripts/data_quality/normalize_entity_names.py:430:9
    |
428 |         with open(temp_path, "w") as f:
429 |             json.dump(data, f, indent=2)
430 |         os.replace(temp_path, output_path)
    |         ^^^^^^^^^^
431 |
432 |         print(f"  ✓ Normalized {original_count} → {final_count} entities")
    |
help: Replace with `Path(...).replace(...)`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:502:14
    |
500 |         report_text = "\n".join(report)
501 |
502 |         with open(output_path, "w") as f:
    |              ^^^^
503 |             f.write(report_text)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_entity_names.py:542:10
    |
541 |     # Get merge groups for report
542 |     with open(entities_path) as f:
    |          ^^^^
543 |         data = json.load(f)
544 |     merge_groups = normalizer.find_duplicate_entities(data["entities"])
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/normalize_raw_flight_logs.py:63:14
   |
61 |     # Create backup
62 |     if not backup_file.exists():
63 |         with open(raw_file, encoding="utf-8") as f:
   |              ^^^^
64 |             content = f.read()
65 |         with open(backup_file, "w", encoding="utf-8") as f:
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/normalize_raw_flight_logs.py:65:14
   |
63 |         with open(raw_file, encoding="utf-8") as f:
64 |             content = f.read()
65 |         with open(backup_file, "w", encoding="utf-8") as f:
   |              ^^^^
66 |             f.write(content)
67 |         print(f"✓ Created backup: {backup_file}")
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/normalize_raw_flight_logs.py:70:10
   |
69 |     # Read file
70 |     with open(raw_file, encoding="utf-8") as f:
   |          ^^^^
71 |         lines = f.readlines()
   |
help: Replace with `Path.open()`

PLW2901 `for` loop variable `passenger` overwritten by assignment target
   --> scripts/data_quality/normalize_raw_flight_logs.py:98:17
    |
 96 |             normalized_passengers = []
 97 |             for passenger in passengers:
 98 |                 passenger = passenger.strip()
    |                 ^^^^^^^^^
 99 |                 if passenger:
100 |                     normalized = normalize_name_in_text(passenger)
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/normalize_raw_flight_logs.py:118:10
    |
117 |     # Write normalized file
118 |     with open(output_file, "w", encoding="utf-8") as f:
    |          ^^^^
119 |         f.writelines(normalized_lines)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/rebuild_all_documents_index.py:36:10
   |
34 |     """
35 |     logger.info(f"Loading categorized PDFs from {master_categorized}")
36 |     with open(master_categorized) as f:
   |          ^^^^
37 |         master_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/rebuild_all_documents_index.py:40:10
   |
39 |     logger.info(f"Loading emails from {old_all_documents}")
40 |     with open(old_all_documents) as f:
   |          ^^^^
41 |         old_all_docs = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/rebuild_all_documents_index.py:46:14
   |
44 |     if backup:
45 |         backup_path = f"{old_all_documents}.rebuild_backup"
46 |         with open(backup_path, "w") as f:
   |              ^^^^
47 |             json.dump(old_all_docs, f, indent=2)
48 |         logger.info(f"Created backup at {backup_path}")
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/rebuild_all_documents_index.py:123:10
    |
121 |     # Save new index
122 |     logger.info(f"Saving rebuilt index to {output_file}")
123 |     with open(output_file, "w") as f:
    |          ^^^^
124 |         json.dump(new_index, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/rebuild_entity_statistics.py:20:10
   |
19 |     # Read clean primary index
20 |     with open(ENTITIES_INDEX, encoding="utf-8") as f:
   |          ^^^^
21 |         entities_index = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/rebuild_entity_statistics.py:68:10
   |
67 |     # Write rebuilt statistics
68 |     with open(ENTITY_STATS, "w", encoding="utf-8") as f:
   |          ^^^^
69 |         json.dump(entity_statistics, f, indent=2, ensure_ascii=False)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/restore_entity_bios.py:23:10
   |
21 | def load_json(filepath):
22 |     """Load JSON file"""
23 |     with open(filepath, encoding="utf-8") as f:
   |          ^^^^
24 |         return json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/restore_entity_bios.py:29:10
   |
27 | def save_json(filepath, data):
28 |     """Save JSON file with formatting"""
29 |     with open(filepath, "w", encoding="utf-8") as f:
   |          ^^^^
30 |         json.dump(data, f, indent=2, ensure_ascii=False)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/restore_entity_bios.py:135:10
    |
134 |     # Save report
135 |     with open(REPORT_FILE, "w", encoding="utf-8") as f:
    |          ^^^^
136 |         f.write(report_text)
    |
help: Replace with `Path.open()`

SIM102 Use a single `if` statement instead of nested `if` statements
   --> scripts/data_quality/validate_categorization.py:106:9
    |
105 |           # Issue 3: Giuffre/Maxwell documents not court_filing
106 | /         if re.search(r"giuffre.*maxwell|maxwell.*giuffre", path.lower()):
107 | |             if classification not in ["court_filing", "unknown"]:
    | |_________________________________________________________________^
108 |                   issues.append(
109 |                       ValidationIssue(
    |
help: Combine `if` statements using `and`

SIM102 Use a single `if` statement instead of nested `if` statements
   --> scripts/data_quality/validate_categorization.py:120:9
    |
119 |           # Issue 4: House Oversight documents not government_document
120 | /         if "house_oversight" in source.lower():
121 | |             if classification not in ["government_document", "unknown"]:
    | |________________________________________________________________________^
122 |                   issues.append(
123 |                       ValidationIssue(
    |
help: Combine `if` statements using `and`

SIM102 Use a single `if` statement instead of nested `if` statements
   --> scripts/data_quality/validate_categorization.py:160:9
    |
159 |           # Issue 7: Birthday/Black book not contact_directory
160 | /         if re.search(r"birthday.*book|black.*book", path.lower()):
161 | |             if classification not in ["contact_directory", "unknown"]:
    | |______________________________________________________________________^
162 |                   issues.append(
163 |                       ValidationIssue(
    |
help: Combine `if` statements using `and`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/validate_categorization.py:189:10
    |
187 |     logger.info(f"Loading categorized documents from {input_file}")
188 |
189 |     with open(input_file) as f:
    |          ^^^^
190 |         data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/validate_categorization.py:245:14
    |
243 |     # Save report if requested
244 |     if output_report:
245 |         with open(output_report, "w") as f:
    |              ^^^^
246 |             json.dump(results, f, indent=2)
247 |         logger.info(f"Validation report saved to {output_report}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/validate_entity_sync.py:35:10
   |
33 | def load_primary_index() -> tuple[list[dict], dict]:
34 |     """Load primary entity index"""
35 |     with open(PRIMARY_INDEX) as f:
   |          ^^^^
36 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/validate_entity_sync.py:50:10
   |
48 | def load_secondary_index() -> tuple[dict, dict]:
49 |     """Load secondary statistics index"""
50 |     with open(SECONDARY_INDEX) as f:
   |          ^^^^
51 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (92 > 60)
  --> scripts/data_quality/validate_entity_sync.py:92:5
   |
92 | def validate_sync() -> tuple[bool, list[str]]:
   |     ^^^^^^^^^^^^^
93 |     """
94 |     Validate entity synchronization
   |

PLC0415 `import` should be at the top-level of a file
   --> scripts/data_quality/validate_entity_sync.py:257:9
    |
255 |     except Exception as e:
256 |         print(f"❌ ERROR: Validation failed with exception: {e}")
257 |         import traceback
    |         ^^^^^^^^^^^^^^^^
258 |
259 |         traceback.print_exc()
    |

PLR0915 Too many statements (111 > 60)
  --> scripts/data_quality/verify_normalization.py:12:5
   |
12 | def verify_normalization():
   |     ^^^^^^^^^^^^^^^^^^^^
13 |     """Run comprehensive verification checks"""
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/verify_normalization.py:26:10
   |
25 |     entities_path = project_root / "data/md/entities/ENTITIES_INDEX.json"
26 |     with open(entities_path) as f:
   |          ^^^^
27 |         entities_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/data_quality/verify_normalization.py:74:10
   |
73 |     network_path = project_root / "data/metadata/entity_network.json"
74 |     with open(network_path) as f:
   |          ^^^^
75 |         network = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/verify_normalization.py:106:10
    |
105 |     mappings_path = project_root / "data/metadata/entity_name_mappings.json"
106 |     with open(mappings_path) as f:
    |          ^^^^
107 |         mappings = json.load(f)
    |
help: Replace with `Path.open()`

PLC0206 Extracting value from dictionary without calling `.items()`
   --> scripts/data_quality/verify_normalization.py:116:9
    |
115 |       for orig, canon in mappings.items():
116 | /         for key in key_mappings:
117 | |             if key in orig:
118 | |                 key_mappings[key].append(f"{orig} → {canon}")
    | |_____________________________________________________________^
119 |
120 |       print("\n  Key entity mappings:")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/data_quality/verify_normalization.py:134:10
    |
133 |     flights_path = project_root / "data/md/entities/flight_logs_by_flight.json"
134 |     with open(flights_path) as f:
    |          ^^^^
135 |         flights_data = json.load(f)
    |
help: Replace with `Path.open()`

E402 Module level import not at top of file
  --> scripts/database/init_audit_db.py:23:1
   |
21 | sys.path.insert(0, str(SERVER_DIR))
22 |
23 | from services.audit_logger import AuditLogger
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/check_courtlistener_progress.py:16:10
   |
14 | # Load metadata
15 | if metadata_file.exists():
16 |     with open(metadata_file) as f:
   |          ^^^^
17 |         metadata = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/download_all_sources.py:88:22
   |
86 |         if index_path.exists():
87 |             try:
88 |                 with open(index_path) as f:
   |                      ^^^^
89 |                     index = json.load(f)
90 |                     for doc in index.get("documents", []):
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_all_sources.py:106:14
    |
104 |         """
105 |         sha256 = hashlib.sha256()
106 |         with open(filepath, "rb") as f:
    |              ^^^^
107 |             for chunk in iter(lambda: f.read(8192), b""):
108 |                 sha256.update(chunk)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_all_sources.py:157:22
    |
156 |                 # Write file
157 |                 with open(output_path, "wb") as f:
    |                      ^^^^
158 |                     for chunk in response.iter_content(chunk_size=8192):
159 |                         if chunk:
    |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/download/download_all_sources.py:176:43
    |
174 | …                     size=0,
175 | …                     hash=self.calculate_hash(output_path) if output_path.exists() else "",
176 | …                     downloaded_at=datetime.utcnow().isoformat(),
    |                                     ^^^^^^^^^^^^^^^^^
177 | …                     duplicate=True,
178 | …                     **(metadata or {}),
    |
help: Use `datetime.datetime.now(tz=...)` instead

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/download/download_all_sources.py:194:39
    |
192 |                         size=file_size,
193 |                         hash=file_hash,
194 |                         downloaded_at=datetime.utcnow().isoformat(),
    |                                       ^^^^^^^^^^^^^^^^^
195 |                         duplicate=False,
196 |                         **(metadata or {}),
    |
help: Use `datetime.datetime.now(tz=...)` instead

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/download/download_all_sources.py:409:29
    |
408 |         master_index = {
409 |             "generated_at": datetime.utcnow().isoformat(),
    |                             ^^^^^^^^^^^^^^^^^
410 |             "total_files": sum(len(files) for files in hash_to_files.values()),
411 |             "unique_documents": len(hash_to_files),
    |
help: Use `datetime.datetime.now(tz=...)` instead

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_all_sources.py:458:14
    |
456 |         # Save index
457 |         index_path = self.metadata_dir / "master_document_index.json"
458 |         with open(index_path, "w") as f:
    |              ^^^^
459 |             json.dump(master_index, f, indent=2)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (78 > 60)
   --> scripts/download/download_all_sources.py:471:9
    |
469 |         return master_index
470 |
471 |     def generate_report(self) -> None:
    |         ^^^^^^^^^^^^^^^
472 |         """Generate comprehensive download and deduplication report."""
473 |         logger.info("\n" + "=" * 70)
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_all_sources.py:481:14
    |
479 |         report_path = self.base_dir / "DOWNLOAD_DEDUPLICATION_REPORT.md"
480 |
481 |         with open(report_path, "w") as f:
    |              ^^^^
482 |             f.write("# Epstein Document Collection - Download & Deduplication Report\n\n")
483 |             f.write(f"**Generated**: {datetime.utcnow().isoformat()}\n\n")
    |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/download/download_all_sources.py:483:39
    |
481 |         with open(report_path, "w") as f:
482 |             f.write("# Epstein Document Collection - Download & Deduplication Report\n\n")
483 |             f.write(f"**Generated**: {datetime.utcnow().isoformat()}\n\n")
    |                                       ^^^^^^^^^^^^^^^^^
484 |
485 |             f.write("## Summary Statistics\n\n")
    |
help: Use `datetime.datetime.now(tz=...)` instead

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_all_sources.py:602:14
    |
600 |         # Also save download log as JSON
601 |         log_path = self.metadata_dir / "download_log.json"
602 |         with open(log_path, "w") as f:
    |              ^^^^
603 |             json.dump([asdict(entry) for entry in self.download_log], f, indent=2)
604 |         logger.info(f"Download log saved: {log_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/download_case_files.py:57:14
   |
56 |         # Load cases index
57 |         with open(CASES_INDEX) as f:
   |              ^^^^
58 |             self.cases_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/download_case_files.py:66:18
   |
64 |         """Load existing download log or create new one."""
65 |         if DOWNLOAD_LOG.exists():
66 |             with open(DOWNLOAD_LOG) as f:
   |                  ^^^^
67 |                 return json.load(f)
68 |         return {"last_updated": None, "downloads": {}}
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/download_case_files.py:74:14
   |
72 |         self.download_log["last_updated"] = datetime.now().isoformat()
73 |         DOWNLOAD_LOG.parent.mkdir(parents=True, exist_ok=True)
74 |         with open(DOWNLOAD_LOG, "w") as f:
   |              ^^^^
75 |             json.dump(self.download_log, f, indent=2)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_case_files.py:120:14
    |
118 |         # Save case metadata
119 |         metadata_file = case_dir / "case_metadata.json"
120 |         with open(metadata_file, "w") as f:
    |              ^^^^
121 |             json.dump(case, f, indent=2)
    |
help: Replace with `Path.open()`

RUF001 String contains ambiguous `ℹ` (INFORMATION SOURCE). Did you mean `i` (LATIN SMALL LETTER I)?
   --> scripts/download/download_case_files.py:124:19
    |
123 |         print(f"  ✓ Saved metadata to {metadata_file}")
124 |         print(f"  ℹ Manual download required from: {docs_url}")
    |                   ^
125 |         print(f"  ℹ Documents should be saved to: {case_dir}/")
126 |         print("  ℹ See download_instructions in cases_index.json")
    |

RUF001 String contains ambiguous `ℹ` (INFORMATION SOURCE). Did you mean `i` (LATIN SMALL LETTER I)?
   --> scripts/download/download_case_files.py:125:19
    |
123 |         print(f"  ✓ Saved metadata to {metadata_file}")
124 |         print(f"  ℹ Manual download required from: {docs_url}")
125 |         print(f"  ℹ Documents should be saved to: {case_dir}/")
    |                   ^
126 |         print("  ℹ See download_instructions in cases_index.json")
    |

RUF001 String contains ambiguous `ℹ` (INFORMATION SOURCE). Did you mean `i` (LATIN SMALL LETTER I)?
   --> scripts/download/download_case_files.py:126:18
    |
124 |         print(f"  ℹ Manual download required from: {docs_url}")
125 |         print(f"  ℹ Documents should be saved to: {case_dir}/")
126 |         print("  ℹ See download_instructions in cases_index.json")
    |                  ^
127 |
128 |         # Log the download attempt
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_case_files.py:167:18
    |
165 |             # Save download instructions
166 |             instructions_file = case_dir / "DOWNLOAD_INSTRUCTIONS.txt"
167 |             with open(instructions_file, "w") as f:
    |                  ^^^^
168 |                 f.write("House Oversight Committee Epstein Documents\\n")
169 |                 f.write("=" * 50 + "\\n\\n")
    |
help: Replace with `Path.open()`

RUF001 String contains ambiguous `ℹ` (INFORMATION SOURCE). Did you mean `i` (LATIN SMALL LETTER I)?
   --> scripts/download/download_case_files.py:212:18
    |
210 |         if "archive.org" in docs_url:
211 |             return self.download_archive_org_case(case_id)
212 |         print("  ℹ Manual download required")
    |                  ^
213 |         print(f"  URL: {docs_url}")
214 |         return False
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_courtlistener.py:133:22
    |
131 |         if self.metadata_file.exists():
132 |             try:
133 |                 with open(self.metadata_file) as f:
    |                      ^^^^
134 |                     return json.load(f)
135 |             except Exception as e:
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/download/download_courtlistener.py:141:9
    |
139 |     def _save_metadata(self):
140 |         """Save download metadata to JSON file."""
141 |         import datetime
    |         ^^^^^^^^^^^^^^^
142 |
143 |         self.metadata["last_update"] = datetime.datetime.now().isoformat()
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_courtlistener.py:144:14
    |
143 |         self.metadata["last_update"] = datetime.datetime.now().isoformat()
144 |         with open(self.metadata_file, "w") as f:
    |              ^^^^
145 |             json.dump(self.metadata, f, indent=2)
    |
help: Replace with `Path.open()`

E741 Ambiguous variable name: `l`
   --> scripts/download/download_courtlistener.py:276:28
    |
275 |         # Log summary statistics
276 |         main_docs = [l for l in links if l.document_number and l.document_number.endswith(".0")]
    |                            ^
277 |         attachments = [
278 |             l for l in links if l.document_number and not l.document_number.endswith(".0")
    |

E741 Ambiguous variable name: `l`
   --> scripts/download/download_courtlistener.py:278:19
    |
276 |         main_docs = [l for l in links if l.document_number and l.document_number.endswith(".0")]
277 |         attachments = [
278 |             l for l in links if l.document_number and not l.document_number.endswith(".0")
    |                   ^
279 |         ]
280 |         logger.info(f"  Main documents: {len(main_docs)}")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/download/download_courtlistener.py:356:18
    |
355 |             # Save to file
356 |             with open(filepath, "wb") as f:
    |                  ^^^^
357 |                 for chunk in pdf_response.iter_content(chunk_size=8192):
358 |                     f.write(chunk)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/query_master_index.py:31:14
   |
30 |         # Load index
31 |         with open(self.index_path) as f:
   |              ^^^^
32 |             self.index = json.load(f)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (67 > 60)
  --> scripts/download/summarize_courtlistener_download.py:13:5
   |
13 | def summarize_download(output_dir: Path):
   |     ^^^^^^^^^^^^^^^^^^
14 |     """Generate download summary report."""
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/summarize_courtlistener_download.py:23:10
   |
21 |         return
22 |
23 |     with open(metadata_file) as f:
   |          ^^^^
24 |         metadata = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/download/summarize_courtlistener_download.py:96:10
   |
94 |     # Save summary to file
95 |     summary_file = output_dir / "DOWNLOAD_SUMMARY.txt"
96 |     with open(summary_file, "w") as f:
   |          ^^^^
97 |         f.write("CourtListener Download Summary - Giuffre v. Maxwell\n")
98 |         f.write(f"{'=' * 70}\n\n")
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/expand_news_database.py:15:6
   |
13 | # Read current database
14 | db_path = Path("/Users/masa/Projects/epstein/data/metadata/news_articles_index.json")
15 | with open(db_path) as f:
   |      ^^^^
16 |     db = json.load(f)
   |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/expand_news_database.py:734:23
    |
732 |         "language": "en",
733 |         "access_type": "public",
734 |         "scraped_at": datetime.utcnow().isoformat(),
    |                       ^^^^^^^^^^^^^^^^^
735 |         "last_verified": datetime.utcnow().isoformat(),
736 |         "archive_status": "not_archived",
    |
help: Use `datetime.datetime.now(tz=...)` instead

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/expand_news_database.py:735:26
    |
733 |         "access_type": "public",
734 |         "scraped_at": datetime.utcnow().isoformat(),
735 |         "last_verified": datetime.utcnow().isoformat(),
    |                          ^^^^^^^^^^^^^^^^^
736 |         "archive_status": "not_archived",
737 |     }
    |
help: Use `datetime.datetime.now(tz=...)` instead

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/expand_news_database.py:746:34
    |
744 | # Update metadata
745 | db["metadata"]["total_articles"] = len(existing_articles)
746 | db["metadata"]["last_updated"] = datetime.utcnow().isoformat()
    |                                  ^^^^^^^^^^^^^^^^^
747 |
748 | # Calculate new date range
    |
help: Use `datetime.datetime.now(tz=...)` instead

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/expand_news_database.py:761:6
    |
760 | # Save updated database
761 | with open(db_path, "w") as f:
    |      ^^^^
762 |     json.dump(db, f, indent=2, ensure_ascii=False)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/extraction/check_ocr_status.py:53:10
   |
52 |     # Load progress
53 |     with open(PROGRESS_FILE) as f:
   |          ^^^^
54 |         progress = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/extraction/extract_emails.py:52:10
   |
50 |     """Load email candidates from JSONL file."""
51 |     candidates = []
52 |     with open(jsonl_path) as f:
   |          ^^^^
53 |         for line in f:
54 |             if line.strip():
   |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/extraction/extract_emails.py:192:22
    |
190 |         document_id=doc_id,
191 |         email_index=index,
192 |         extracted_at=datetime.utcnow().isoformat(),
    |                      ^^^^^^^^^^^^^^^^^
193 |         confidence=candidate.get("confidence", 0.0),
194 |         email_addresses=candidate.get("email_addresses", []),
    |
help: Use `datetime.datetime.now(tz=...)` instead

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/extract_emails.py:215:10
    |
213 |     # Save metadata JSON
214 |     metadata_path = subdir / f"{metadata.document_id}_metadata.json"
215 |     with open(metadata_path, "w") as f:
    |          ^^^^
216 |         json.dump(asdict(metadata), f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/extract_emails.py:220:10
    |
218 |     # Save full OCR text
219 |     text_path = subdir / f"{metadata.document_id}_full.txt"
220 |     with open(text_path, "w") as f:
    |          ^^^^
221 |         f.write(ocr_text)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/extract_emails.py:226:14
    |
224 |     if metadata.body:
225 |         body_path = subdir / f"{metadata.document_id}_body.txt"
226 |         with open(body_path, "w") as f:
    |              ^^^^
227 |             f.write(metadata.body)
    |
help: Replace with `Path.open()`

ARG001 Unused function argument: `email_dir`
   --> scripts/extraction/extract_emails.py:230:26
    |
230 | def generate_email_index(email_dir: Path, metadata_list: list[EmailMetadata]) -> dict:
    |                          ^^^^^^^^^
231 |     """Generate email index with statistics."""
232 |     index = {
    |

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/extraction/extract_emails.py:234:28
    |
232 |     index = {
233 |         "total_emails": len(metadata_list),
234 |         "extraction_date": datetime.utcnow().isoformat(),
    |                            ^^^^^^^^^^^^^^^^^
235 |         "by_date": {},
236 |         "by_sender": {},
    |
help: Use `datetime.datetime.now(tz=...)` instead

PLR0915 Too many statements (70 > 60)
   --> scripts/extraction/extract_emails.py:258:5
    |
258 | def main():
    |     ^^^^
259 |     """Main extraction workflow."""
260 |     base_dir = Path(__file__).parent.parent.parent
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/extract_emails.py:288:18
    |
286 |                 continue
287 |
288 |             with open(ocr_text_path) as f:
    |                  ^^^^
289 |                 ocr_text = f.read()
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/extract_emails.py:316:10
    |
314 |     email_index = generate_email_index(email_output_dir, metadata_list)
315 |     index_path = email_output_dir / "EMAIL_INDEX.json"
316 |     with open(index_path, "w") as f:
    |          ^^^^
317 |         json.dump(email_index, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/extract_emails.py:323:10
    |
321 |     # Generate summary report
322 |     summary_path = email_output_dir / "EXTRACTION_SUMMARY.md"
323 |     with open(summary_path, "w") as f:
    |          ^^^^
324 |         f.write("# Email Extraction Summary\n\n")
325 |         f.write(f"**Extraction Date**: {datetime.utcnow().isoformat()}\n\n")
    |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/extraction/extract_emails.py:325:41
    |
323 |     with open(summary_path, "w") as f:
324 |         f.write("# Email Extraction Summary\n\n")
325 |         f.write(f"**Extraction Date**: {datetime.utcnow().isoformat()}\n\n")
    |                                         ^^^^^^^^^^^^^^^^^
326 |         f.write("## Statistics\n\n")
327 |         f.write(f"- **Total candidates**: {len(candidates)}\n")
    |
help: Use `datetime.datetime.now(tz=...)` instead

F401 `PIL.Image` imported but unused; consider using `importlib.util.find_spec` to test for availability
  --> scripts/extraction/ocr_house_oversight.py:28:21
   |
26 |     import pytesseract
27 |     from pdf2image import convert_from_path
28 |     from PIL import Image
   |                     ^^^^^
29 |     from tqdm import tqdm
30 | except ImportError as e:
   |
help: Remove unused import: `PIL.Image`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/extraction/ocr_house_oversight.py:73:18
   |
71 |             log_entry = f"[{timestamp}] [{level}] {message}"
72 |             print(log_entry)
73 |             with open(self.log_file, "a") as f:
   |                  ^^^^
74 |                 f.write(log_entry + "\n")
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/extraction/ocr_house_oversight.py:82:14
   |
80 |     """Load processing progress from file"""
81 |     if PROGRESS_FILE.exists():
82 |         with open(PROGRESS_FILE) as f:
   |              ^^^^
83 |             return json.load(f)
84 |     return {
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/extraction/ocr_house_oversight.py:96:10
   |
94 |     PROGRESS_FILE.parent.mkdir(parents=True, exist_ok=True)
95 |     progress["stats"]["last_update"] = datetime.now().isoformat()
96 |     with open(PROGRESS_FILE, "w") as f:
   |          ^^^^
97 |         json.dump(progress, f, indent=2)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/ocr_house_oversight.py:195:14
    |
193 |         output_file.parent.mkdir(parents=True, exist_ok=True)
194 |
195 |         with open(output_file, "w", encoding="utf-8") as f:
    |              ^^^^
196 |             f.write(text)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/ocr_house_oversight.py:209:14
    |
208 |         metadata_file = output_dir / f"{pdf_path.stem}.json"
209 |         with open(metadata_file, "w", encoding="utf-8") as f:
    |              ^^^^
210 |             json.dump(metadata, f, indent=2)
    |
help: Replace with `Path.open()`

ARG001 Unused function argument: `logger`
   --> scripts/extraction/ocr_house_oversight.py:223:78
    |
223 | def process_batch(pdf_files: list[Path], output_dir: Path, num_workers: int, logger) -> dict:
    |                                                                              ^^^^^^
224 |     """
225 |     Process a batch of PDF files with parallel workers
    |

SIM117 Use a single `with` statement with multiple contexts instead of nested `with` statements
   --> scripts/extraction/ocr_house_oversight.py:259:5
    |
258 |       # Process with progress bar
259 | /     with Pool(num_workers) as pool:
260 | |         with tqdm(total=len(pdf_files), desc="Processing PDFs", unit="file") as pbar:
    | |_____________________________________________________________________________________^
261 |               for result in pool.imap_unordered(process_single_pdf, args_list):
262 |                   all_results.append(result)
    |
help: Combine `with` statements

PLR0915 Too many statements (62 > 60)
   --> scripts/extraction/ocr_house_oversight.py:315:5
    |
315 | def main():
    |     ^^^^
316 |     parser = argparse.ArgumentParser(description="OCR processing for House Oversight PDFs")
317 |     parser.add_argument("--workers", type=int, default=10, help="Number of parallel workers")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/extraction/ocr_house_oversight.py:410:18
    |
408 |         if batch_results["email_candidates"]:
409 |             EMAIL_INDEX_FILE.parent.mkdir(parents=True, exist_ok=True)
410 |             with open(EMAIL_INDEX_FILE, "a") as f:
    |                  ^^^^
411 |                 for candidate in batch_results["email_candidates"]:
412 |                     f.write(json.dumps(candidate) + "\n")
    |
help: Replace with `Path.open()`

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> scripts/git_commit_helper.py:20:20
   |
18 |       """Helps create conventional commit messages."""
19 |
20 |       COMMIT_TYPES = {
   |  ____________________^
21 | |         "feat": "A new feature",
22 | |         "fix": "A bug fix",
23 | |         "docs": "Documentation only changes",
24 | |         "style": "Changes that don't affect code meaning (formatting, etc)",
25 | |         "refactor": "Code change that neither fixes a bug nor adds a feature",
26 | |         "perf": "Performance improvement",
27 | |         "test": "Adding missing tests or correcting existing tests",
28 | |         "build": "Changes to build system or dependencies",
29 | |         "ci": "Changes to CI configuration files and scripts",
30 | |         "chore": "Other changes that don't modify src or test files",
31 | |         "revert": "Reverts a previous commit",
32 | |     }
   | |_____^
33 |
34 |       SCOPES = [
   |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> scripts/git_commit_helper.py:34:14
   |
32 |       }
33 |
34 |       SCOPES = [
   |  ______________^
35 | |         "ocr",
36 | |         "classification",
37 | |         "extraction",
38 | |         "network",
39 | |         "search",
40 | |         "database",
41 | |         "api",
42 | |         "docs",
43 | |         "scripts",
44 | |     ]
   | |_____^
45 |
46 |       def __init__(self):
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_documents.py:177:18
    |
175 |             # Save JSON metadata
176 |             json_path = DOCS_DIR / f"{safe_id}.json"
177 |             with open(json_path, "w", encoding="utf-8") as f:
    |                  ^^^^
178 |                 json.dump(document, f, indent=2, ensure_ascii=False)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_documents.py:182:18
    |
180 |             # Save plain text for easy reading
181 |             txt_path = DOCS_DIR / f"{safe_id}.txt"
182 |             with open(txt_path, "w", encoding="utf-8") as f:
    |                  ^^^^
183 |                 f.write(document["text"])
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_documents.py:228:14
    |
226 |         # Save metadata
227 |         metadata_path = OUTPUT_DIR / "import_metadata.json"
228 |         with open(metadata_path, "w", encoding="utf-8") as f:
    |              ^^^^
229 |             json.dump(metadata, f, indent=2, ensure_ascii=False)
230 |         logger.info(f"Saved metadata to {metadata_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_documents.py:234:14
    |
232 |         # Save document index
233 |         index_path = OUTPUT_DIR / "document_index.json"
234 |         with open(index_path, "w", encoding="utf-8") as f:
    |              ^^^^
235 |             json.dump(self.document_index, f, indent=2, ensure_ascii=False)
236 |         logger.info(f"Saved document index to {index_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_documents.py:240:18
    |
238 |         # Save error log if there are errors
239 |         if self.error_log:
240 |             with open(ERROR_LOG, "w", encoding="utf-8") as f:
    |                  ^^^^
241 |                 json.dump(self.error_log, f, indent=2, ensure_ascii=False)
242 |             logger.warning(f"Saved {len(self.error_log)} errors to {ERROR_LOG}")
    |
help: Replace with `Path.open()`

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> scripts/import/import_huggingface_emails.py:110:22
    |
108 |         for fmt in date_formats:
109 |             try:
110 |                 dt = datetime.strptime(date_str.strip(), fmt)
    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
111 |                 return dt.isoformat()
112 |             except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_emails.py:227:18
    |
225 |             file_path = EMAILS_DIR / f"{safe_id}.json"
226 |
227 |             with open(file_path, "w", encoding="utf-8") as f:
    |                  ^^^^
228 |                 json.dump(email, f, indent=2, ensure_ascii=False)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_emails.py:270:14
    |
268 |         # Save metadata
269 |         metadata_path = OUTPUT_DIR / "import_metadata.json"
270 |         with open(metadata_path, "w", encoding="utf-8") as f:
    |              ^^^^
271 |             json.dump(metadata, f, indent=2, ensure_ascii=False)
272 |         logger.info(f"Saved metadata to {metadata_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_emails.py:276:14
    |
274 |         # Save email index
275 |         index_path = OUTPUT_DIR / "email_index.json"
276 |         with open(index_path, "w", encoding="utf-8") as f:
    |              ^^^^
277 |             json.dump(self.email_index, f, indent=2, ensure_ascii=False)
278 |         logger.info(f"Saved email index to {index_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/import/import_huggingface_emails.py:282:18
    |
280 |         # Save error log if there are errors
281 |         if self.error_log:
282 |             with open(ERROR_LOG, "w", encoding="utf-8") as f:
    |                  ^^^^
283 |                 json.dump(self.error_log, f, indent=2, ensure_ascii=False)
284 |             logger.warning(f"Saved {len(self.error_log)} errors to {ERROR_LOG}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/indexing/build_unified_index.py:32:10
   |
30 |     """Load JSON file with progress indicator."""
31 |     print(f"Loading {filepath.name}...")
32 |     with open(filepath, encoding="utf-8") as f:
   |          ^^^^
33 |         return json.load(f)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (74 > 60)
  --> scripts/indexing/build_unified_index.py:36:5
   |
36 | def build_unified_index() -> dict[str, Any]:
   |     ^^^^^^^^^^^^^^^^^^^
37 |     """Build comprehensive document index from all sources."""
38 |     print("\n" + "=" * 80)
   |

PLC0415 `import` should be at the top-level of a file
   --> scripts/indexing/build_unified_index.py:134:13
    |
132 |         if email_date:
133 |             # Try to extract year-month (e.g., "2019-08")
134 |             import re
    |             ^^^^^^^^^
135 |
136 |             date_match = re.search(r"(\d{4})-(\d{2})", email_date)
    |

SIM102 Use a single `if` statement instead of nested `if` statements
   --> scripts/indexing/build_unified_index.py:189:21
    |
187 |               for entity, mentions in entity_mentions.items():
188 |                   for mention in mentions:
189 | /                     if mention.get("document") in doc_path or doc_path in mention.get(
190 | |                         "document", ""
191 | |                     ):
192 | |                         if entity not in doc["entities_mentioned"]:
    | |___________________________________________________________________^
193 |                               doc["entities_mentioned"].append(entity)
194 |                               entity_count += 1
    |
help: Combine `if` statements using `and`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/indexing/build_unified_index.py:219:10
    |
218 |     OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
219 |     with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    |          ^^^^
220 |         json.dump(index, f, indent=2)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/indexing/build_unified_index.py:296:9
    |
294 |     except Exception as e:
295 |         print(f"\n❌ Error: {e}")
296 |         import traceback
    |         ^^^^^^^^^^^^^^^^
297 |
298 |         traceback.print_exc()
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/indexing/generate_summary_report.py:24:10
   |
22 | def load_json(filepath: Path) -> dict:
23 |     """Load JSON file."""
24 |     with open(filepath, encoding="utf-8") as f:
   |          ^^^^
25 |         return json.load(f)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (95 > 60)
  --> scripts/indexing/generate_summary_report.py:28:5
   |
28 | def generate_report() -> str:
   |     ^^^^^^^^^^^^^^^
29 |     """Generate comprehensive summary report."""
30 |     # Load data
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/indexing/generate_summary_report.py:215:14
    |
214 |         # Save to file
215 |         with open(REPORT_FILE, "w", encoding="utf-8") as f:
    |              ^^^^
216 |             f.write(report_text)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/indexing/generate_summary_report.py:227:9
    |
225 |     except Exception as e:
226 |         print(f"\n❌ Error: {e}")
227 |         import traceback
    |         ^^^^^^^^^^^^^^^^
228 |
229 |         traceback.print_exc()
    |

ARG002 Unused method argument: `url`
   --> scripts/ingestion/content_extractor.py:159:44
    |
157 |             return None
158 |
159 |     def _extract_metadata(self, html: str, url: str) -> dict[str, Optional[str]]:
    |                                            ^^^
160 |         """
161 |         Extract metadata using BeautifulSoup.
    |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> scripts/ingestion/content_extractor.py:301:26
    |
299 |             ]:
300 |                 try:
301 |                     dt = datetime.strptime(date_string.strip(), fmt)
    |                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
302 |                     return dt.strftime("%Y-%m-%d")
303 |                 except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/ingestion/entity_extractor.py:92:18
   |
91 |         try:
92 |             with open(self.entity_index_path, encoding="utf-8") as f:
   |                  ^^^^
93 |                 data = json.load(f)
   |
help: Replace with `Path.open()`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> scripts/ingestion/entity_extractor.py:106:13
    |
105 |         except json.JSONDecodeError as e:
106 |             raise ValueError(f"Invalid JSON in entity index: {e}")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
107 |
108 |     def _build_name_index(self) -> None:
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_news_batch.py:161:14
    |
159 |         articles = []
160 |
161 |         with open(csv_path, encoding="utf-8") as f:
    |              ^^^^
162 |             reader = csv.DictReader(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_news_batch.py:319:18
    |
317 |         if stats.errors:
318 |             error_log_path = csv_path.parent / f"{csv_path.stem}_errors.json"
319 |             with open(error_log_path, "w") as f:
    |                  ^^^^
320 |                 json.dump(stats.errors, f, indent=2)
321 |             logger.info(f"Error log written to: {error_log_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_seed_articles.py:117:22
    |
115 |         if self.progress_file.exists():
116 |             try:
117 |                 with open(self.progress_file) as f:
    |                      ^^^^
118 |                     data = json.load(f)
119 |                     self.scraped_urls = set(data.get("scraped_urls", []))
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_seed_articles.py:127:18
    |
125 |         """Save progress to file."""
126 |         try:
127 |             with open(self.progress_file, "w") as f:
    |                  ^^^^
128 |                 json.dump(
129 |                     {
    |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/ingestion/ingest_seed_articles.py:131:41
    |
129 |                     {
130 |                         "scraped_urls": list(self.scraped_urls),
131 |                         "last_updated": datetime.utcnow().isoformat(),
    |                                         ^^^^^^^^^^^^^^^^^
132 |                     },
133 |                     f,
    |
help: Use `datetime.datetime.now(tz=...)` instead

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/ingestion/ingest_seed_articles.py:256:37
    |
254 |                     "date_range": {"earliest": None, "latest": None},
255 |                     "sources": {},
256 |                     "last_updated": datetime.utcnow().isoformat(),
    |                                     ^^^^^^^^^^^^^^^^^
257 |                     "version": "1.0.0",
258 |                 },
    |
help: Use `datetime.datetime.now(tz=...)` instead

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_seed_articles.py:263:18
    |
262 |         try:
263 |             with open(self.articles_index_path) as f:
    |                  ^^^^
264 |                 return json.load(f)
265 |         except Exception as e:
    |
help: Replace with `Path.open()`

SIM117 Use a single `with` statement with multiple contexts instead of nested `with` statements
   --> scripts/ingestion/ingest_seed_articles.py:284:17
    |
282 |               backup_path = self.articles_index_path.with_suffix(".json.backup")
283 |               try:
284 | /                 with open(self.articles_index_path) as src:
285 | |                     with open(backup_path, "w") as dst:
    | |_______________________________________________________^
286 |                           dst.write(src.read())
287 |                   logger.info(f"Created backup: {backup_path}")
    |
help: Combine `with` statements

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_seed_articles.py:284:22
    |
282 |             backup_path = self.articles_index_path.with_suffix(".json.backup")
283 |             try:
284 |                 with open(self.articles_index_path) as src:
    |                      ^^^^
285 |                     with open(backup_path, "w") as dst:
286 |                         dst.write(src.read())
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_seed_articles.py:285:26
    |
283 |             try:
284 |                 with open(self.articles_index_path) as src:
285 |                     with open(backup_path, "w") as dst:
    |                          ^^^^
286 |                         dst.write(src.read())
287 |                 logger.info(f"Created backup: {backup_path}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_seed_articles.py:294:18
    |
292 |         temp_path = self.articles_index_path.with_suffix(".json.tmp")
293 |         try:
294 |             with open(temp_path, "w") as f:
    |                  ^^^^
295 |                 json.dump(articles_data, f, indent=2)
    |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/ingestion/ingest_seed_articles.py:343:29
    |
341 |             "date_range": {"earliest": earliest, "latest": latest},
342 |             "sources": sources,
343 |             "last_updated": datetime.utcnow().isoformat(),
    |                             ^^^^^^^^^^^^^^^^^
344 |             "version": "1.0.0",
345 |         }
    |
help: Use `datetime.datetime.now(tz=...)` instead

PLC0415 `import` should be at the top-level of a file
   --> scripts/ingestion/ingest_seed_articles.py:431:9
    |
429 |             return []
430 |
431 |         import csv
    |         ^^^^^^^^^^
432 |
433 |         articles = []
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/ingest_seed_articles.py:434:14
    |
433 |         articles = []
434 |         with open(seed_path, encoding="utf-8") as f:
    |              ^^^^
435 |             reader = csv.DictReader(f)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (72 > 60)
   --> scripts/ingestion/ingest_seed_articles.py:453:9
    |
451 |         return articles
452 |
453 |     def ingest_source(
    |         ^^^^^^^^^^^^^
454 |         self, source: str, limit: Optional[int] = None, dry_run: bool = False
455 |     ) -> dict:
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/ingestion/ingest_seed_articles.py:794:9
    |
792 |     except Exception as e:
793 |         logger.error(f"Ingestion failed: {e!s}")
794 |         import traceback
    |         ^^^^^^^^^^^^^^^^
795 |
796 |         traceback.print_exc()
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/link_verifier.py:341:18
    |
339 |         # Write errors to log file
340 |         if log_file and errors:
341 |             with open(log_file, "w") as f:
    |                  ^^^^
342 |                 f.write(f"Link Verification Errors ({len(errors)} total)\n")
343 |                 f.write("=" * 80 + "\n\n")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/ingestion/populate_news_database.py:46:10
   |
44 |     the current ENTITIES_INDEX uses list format without IDs.
45 |     """
46 |     with open(path) as f:
   |          ^^^^
47 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/ingestion/populate_news_database.py:77:10
   |
75 |         return set()
76 |
77 |     with open(index_path) as f:
   |          ^^^^
78 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/populate_news_database.py:133:10
    |
131 |     articles_to_import = []
132 |
133 |     with open(seed_csv) as f:
    |          ^^^^
134 |         reader = csv.DictReader(f)
135 |         for row in reader:
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/ingestion/populate_news_database.py:173:10
    |
171 |     # Create temporary CSV with only new articles
172 |     temp_csv = project_root / "data" / "sources" / "news_articles_temp.csv"
173 |     with open(temp_csv, "w", newline="") as f:
    |          ^^^^
174 |         writer = csv.DictWriter(
175 |             f, fieldnames=["url", "publication", "published_date", "title", "notes"]
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/ingestion/populate_news_database.py:181:5
    |
180 |     # Run batch ingestion
181 |     import subprocess
    |     ^^^^^^^^^^^^^^^^^
182 |
183 |     result = subprocess.run(
    |

DTZ003 `datetime.datetime.utcnow()` used
   --> scripts/ingestion/scrape_news_articles.py:210:21
    |
208 |             True
209 |         """
210 |         timestamp = datetime.utcnow().isoformat()
    |                     ^^^^^^^^^^^^^^^^^
211 |         link_status: Optional[LinkStatus] = None
212 |         archive_url: Optional[str] = None
    |
help: Use `datetime.datetime.now(tz=...)` instead

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/metadata/build_chatbot_knowledge_index.py:24:14
   |
22 |     sha256 = hashlib.sha256()
23 |     try:
24 |         with open(file_path, "rb") as f:
   |              ^^^^
25 |             for chunk in iter(lambda: f.read(8192), b""):
26 |                 sha256.update(chunk)
   |
help: Replace with `Path.open()`

DTZ006 `datetime.datetime.fromtimestamp()` called without a `tz` argument
  --> scripts/metadata/build_chatbot_knowledge_index.py:88:33
   |
86 |                     "path": str(file),
87 |                     "size_bytes": file.stat().st_size,
88 |                     "modified": datetime.fromtimestamp(file.stat().st_mtime).isoformat(),
   |                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
89 |                 }
90 |             )
   |
help: Pass a `datetime.timezone` object to the `tz` parameter

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/metadata/build_chatbot_knowledge_index.py:122:14
    |
120 |     index_path = base_path / "data" / "metadata" / "master_document_index.json"
121 |     if index_path.exists():
122 |         with open(index_path) as f:
    |              ^^^^
123 |             return json.load(f)
124 |     return {}
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/metadata/build_chatbot_knowledge_index.py:131:14
    |
129 |     entity_path = base_path / "data" / "md" / "entities" / "ENTITIES_INDEX.json"
130 |     if entity_path.exists():
131 |         with open(entity_path) as f:
    |              ^^^^
132 |             return json.load(f)
133 |     return {}
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/metadata/build_chatbot_knowledge_index.py:140:14
    |
138 |     network_path = base_path / "data" / "metadata" / "entity_network.json"
139 |     if network_path.exists():
140 |         with open(network_path) as f:
    |              ^^^^
141 |             return json.load(f)
142 |     return {}
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/metadata/build_chatbot_knowledge_index.py:149:14
    |
147 |     email_stats_path = base_path / "data" / "canonical" / "emails" / "email_statistics.json"
148 |     if email_stats_path.exists():
149 |         with open(email_stats_path) as f:
    |              ^^^^
150 |             return json.load(f)
151 |     return {}
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/metadata/build_chatbot_knowledge_index.py:158:14
    |
156 |     download_log_path = base_path / "data" / "metadata" / "download_log.json"
157 |     if download_log_path.exists():
158 |         with open(download_log_path) as f:
    |              ^^^^
159 |             return json.load(f)
160 |     return []
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/metadata/build_chatbot_knowledge_index.py:184:14
    |
182 |     classifications_path = base_path / "data" / "metadata" / "document_classifications.json"
183 |     if classifications_path.exists():
184 |         with open(classifications_path) as f:
    |              ^^^^
185 |             classifications = json.load(f)
186 |         total_docs = master_index.get("unique_documents", 0)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/metadata/build_chatbot_knowledge_index.py:332:10
    |
330 |     # Write to file
331 |     output_path = base_path / "data" / "metadata" / "chatbot_knowledge_index.json"
332 |     with open(output_path, "w") as f:
    |          ^^^^
333 |         json.dump(index, f, indent=2)
    |
help: Replace with `Path.open()`

E402 Module level import not at top of file
  --> scripts/metadata/refresh_chatbot_index.py:23:1
   |
21 | sys.path.insert(0, str(scripts_dir))
22 |
23 | from metadata.build_chatbot_knowledge_index import build_knowledge_index
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

PLC0415 `import` should be at the top-level of a file
  --> scripts/metadata/refresh_chatbot_index.py:37:5
   |
35 |     # Write to file
36 |     output_path = base_path / "data" / "metadata" / "chatbot_knowledge_index.json"
37 |     import json
   |     ^^^^^^^^^^^
38 |
39 |     with open(output_path, "w") as f:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/metadata/refresh_chatbot_index.py:39:10
   |
37 |     import json
38 |
39 |     with open(output_path, "w") as f:
   |          ^^^^
40 |         json.dump(index, f, indent=2)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/migration/find_entity_aliases.py:12:10
   |
10 | def load_entity_mappings():
11 |     """Load entity ID mappings"""
12 |     with open("/Users/masa/Projects/epstein/data/migration/entity_id_mappings.json") as f:
   |          ^^^^
13 |         data = json.load(f)
14 |     return data["id_to_entity"]
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/find_entity_aliases.py:214:10
    |
213 |     output_path = "/Users/masa/Projects/epstein/data/migration/entity_network_aliases_analysis.json"
214 |     with open(output_path, "w") as f:
    |          ^^^^
215 |         json.dump(output, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/generate_entity_ids.py:278:14
    |
276 |         logging.info(f"Loading entities from {filepath}")
277 |
278 |         with open(filepath) as f:
    |              ^^^^
279 |             data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/generate_entity_ids.py:400:14
    |
398 |         }
399 |
400 |         with open(mappings_file, "w") as f:
    |              ^^^^
401 |             json.dump(mappings_data, f, indent=2)
402 |         logging.info(f"Saved ID mappings to {mappings_file}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/generate_entity_ids.py:409:18
    |
407 |             collision_report = self.generate_collision_report()
408 |
409 |             with open(collision_file, "w") as f:
    |                  ^^^^
410 |                 json.dump(collision_report, f, indent=2)
411 |             logging.info(
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/generate_entity_ids.py:424:14
    |
422 |         }
423 |
424 |         with open(stats_file, "w") as f:
    |              ^^^^
425 |             json.dump(stats_data, f, indent=2)
426 |         logging.info(f"Saved statistics to {stats_file}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_metadata.py:200:10
    |
198 | def load_id_mappings(mappings_file: Path) -> dict[str, str]:
199 |     """Load name-to-ID mappings."""
200 |     with open(mappings_file) as f:
    |          ^^^^
201 |         data = json.load(f)
202 |     return data["name_to_id"]
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (73 > 60)
   --> scripts/migration/migrate_entity_metadata.py:217:5
    |
217 | def main():
    |     ^^^^
218 |     """Main execution."""
219 |     parser = argparse.ArgumentParser(description="Migrate entity metadata files to ID-based schema")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_metadata.py:259:18
    |
257 |                 backups.append(create_backup(biographies_file))
258 |
259 |             with open(biographies_file) as f:
    |                  ^^^^
260 |                 bio_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_metadata.py:265:22
    |
264 |             if not args.dry_run:
265 |                 with open(biographies_file, "w") as f:
    |                      ^^^^
266 |                     json.dump(migrated_bio, f, indent=2)
267 |                 logging.info(f"Migrated: {biographies_file}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_metadata.py:274:18
    |
272 |                 backups.append(create_backup(tags_file))
273 |
274 |             with open(tags_file) as f:
    |                  ^^^^
275 |                 tags_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_metadata.py:280:22
    |
279 |             if not args.dry_run:
280 |                 with open(tags_file, "w") as f:
    |                      ^^^^
281 |                     json.dump(migrated_tags, f, indent=2)
282 |                 logging.info(f"Migrated: {tags_file}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_metadata.py:289:18
    |
287 |                 backups.append(create_backup(mappings_file_old))
288 |
289 |             with open(mappings_file_old) as f:
    |                  ^^^^
290 |                 old_mappings = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_metadata.py:295:22
    |
294 |             if not args.dry_run:
295 |                 with open(mappings_file_old, "w") as f:
    |                      ^^^^
296 |                     json.dump(migrated_mappings, f, indent=2)
297 |                 logging.info(f"Migrated: {mappings_file_old}")
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/migration/migrate_entity_network.py:352:9
    |
351 |         # Check 3: Valid node IDs
352 |         import re
    |         ^^^^^^^^^
353 |
354 |         id_pattern = re.compile(r"^[a-z0-9_]+$")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_network.py:418:10
    |
416 |     logging.info(f"Loading ID mappings from {mappings_file}")
417 |
418 |     with open(mappings_file) as f:
    |          ^^^^
419 |         data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_network.py:443:10
    |
441 |     logging.info(f"Loading alias mappings from {alias_file}")
442 |
443 |     with open(alias_file) as f:
    |          ^^^^
444 |         data = json.load(f)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (79 > 60)
   --> scripts/migration/migrate_entity_network.py:468:5
    |
468 | def main():
    |     ^^^^
469 |     """Main execution function."""
470 |     parser = argparse.ArgumentParser(description="Migrate entity_network.json to ID-based schema")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_network.py:534:14
    |
532 |         # Load original network
533 |         logging.info(f"Loading network from {network_file}")
534 |         with open(network_file) as f:
    |              ^^^^
535 |             original_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_network.py:580:18
    |
578 |             logging.info("DRY RUN: Skipping file write")
579 |         else:
580 |             with open(network_file, "w") as f:
    |                  ^^^^
581 |                 json.dump(migrated_data, f, indent=2)
582 |             logging.info(f"Saved migrated network to {network_file}")
    |
help: Replace with `Path.open()`

RUF002 Docstring contains ambiguous `×` (MULTIPLICATION SIGN). Did you mean `x` (LATIN SMALL LETTER X)?
   --> scripts/migration/migrate_entity_statistics.py:179:21
    |
178 |         Complexity:
179 |         - Time: O(n × m) where n=entities, m=avg fields per entity
    |                     ^
180 |         - Space: O(k) where k=error count
181 |         """
    |

PLC0415 `import` should be at the top-level of a file
   --> scripts/migration/migrate_entity_statistics.py:196:9
    |
195 |         # Check 2: All IDs are valid slugs
196 |         import re
    |         ^^^^^^^^^
197 |
198 |         id_pattern = re.compile(r"^[a-z0-9_]+$")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_statistics.py:309:10
    |
307 |     logging.info(f"Loading ID mappings from {mappings_file}")
308 |
309 |     with open(mappings_file) as f:
    |          ^^^^
310 |         data = json.load(f)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (65 > 60)
   --> scripts/migration/migrate_entity_statistics.py:335:5
    |
335 | def main():
    |     ^^^^
336 |     """
337 |     Main execution function.
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_statistics.py:404:14
    |
402 |         # Load original statistics
403 |         logging.info(f"Loading statistics from {statistics_file}")
404 |         with open(statistics_file) as f:
    |              ^^^^
405 |             original_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/migrate_entity_statistics.py:443:18
    |
441 |             logging.info("DRY RUN: Skipping file write")
442 |         else:
443 |             with open(statistics_file, "w") as f:
    |                  ^^^^
444 |                 json.dump(migrated_data, f, indent=2)
445 |             logging.info(f"Saved migrated statistics to {statistics_file}")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/validate_migration.py:118:18
    |
117 |         try:
118 |             with open(filepath) as f:
    |                  ^^^^
119 |                 data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/validate_migration.py:171:18
    |
170 |         try:
171 |             with open(filepath) as f:
    |                  ^^^^
172 |                 data = json.load(f)
    |
help: Replace with `Path.open()`

PLR0915 Too many statements (64 > 60)
   --> scripts/migration/validate_migration.py:360:5
    |
360 | def main():
    |     ^^^^
361 |     """Main validation execution."""
362 |     parser = argparse.ArgumentParser(description="Validate entity ID migration")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/validate_migration.py:394:18
    |
392 |     if biographies_file.exists():
393 |         try:
394 |             with open(biographies_file) as f:
    |                  ^^^^
395 |                 biographies = json.load(f)
396 |         except Exception as e:
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/validate_migration.py:402:18
    |
400 |     if tags_file.exists():
401 |         try:
402 |             with open(tags_file) as f:
    |                  ^^^^
403 |                 tags = json.load(f)
404 |         except Exception as e:
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/migration/validate_migration.py:451:14
    |
449 |     # Save report
450 |     if args.output:
451 |         with open(args.output, "w") as f:
    |              ^^^^
452 |             json.dump(report, f, indent=2)
453 |         print(f"\nReport saved: {args.output}")
    |
help: Replace with `Path.open()`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> scripts/rag/batch_embed_helper.py:59:9
   |
57 |           return collection
58 |       except Exception as e:
59 | /         raise RuntimeError(
60 | |             f"Collection '{COLLECTION_NAME}' not found. "
61 | |             f"Run build_vector_store.py first. Error: {e}"
62 | |         )
   | |_________^
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/batch_embed_helper.py:106:10
    |
104 |         }
105 |
106 |     with open(NEWS_INDEX_PATH) as f:
    |          ^^^^
107 |         news_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/batch_embed_helper.py:123:18
    |
121 |         last_updated = None
122 |         if PROGRESS_FILE.exists():
123 |             with open(PROGRESS_FILE) as f:
    |                  ^^^^
124 |                 progress = json.load(f)
125 |                 last_updated = progress.get("last_updated")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/batch_embed_helper.py:342:10
    |
340 |         return {"total_processed": 0, "last_updated": None, "processed_ids": []}
341 |
342 |     with open(PROGRESS_FILE) as f:
    |          ^^^^
343 |         return json.load(f)
    |
help: Replace with `Path.open()`

E722 Do not use bare `except`
  --> scripts/rag/build_vector_store.py:65:9
   |
63 |             print(f"✅ Found existing collection: {COLLECTION_NAME}")
64 |             print(f"   Current documents: {self.collection.count()}")
65 |         except:
   |         ^^^^^^
66 |             self.collection = self.client.create_collection(
67 |                 name=COLLECTION_NAME,
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/build_vector_store.py:87:18
   |
85 |         """Load entity index for entity mention detection."""
86 |         if ENTITY_INDEX_PATH.exists():
87 |             with open(ENTITY_INDEX_PATH) as f:
   |                  ^^^^
88 |                 data = json.load(f)
89 |                 print(f"✅ Loaded entity index: {len(data.get('entities', []))} entities")
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/build_vector_store.py:96:18
   |
94 |         """Load previously processed files for resume capability."""
95 |         if self.resume and PROGRESS_FILE.exists():
96 |             with open(PROGRESS_FILE) as f:
   |                  ^^^^
97 |                 progress = json.load(f)
98 |                 processed = set(progress.get("processed_files", []))
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/build_vector_store.py:106:14
    |
104 |         """Save progress for resume capability."""
105 |         PROGRESS_FILE.parent.mkdir(parents=True, exist_ok=True)
106 |         with open(PROGRESS_FILE, "w") as f:
    |              ^^^^
107 |             json.dump(
108 |                 {
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/build_vector_store.py:165:18
    |
163 |         """Read document and extract metadata."""
164 |         try:
165 |             with open(file_path, encoding="utf-8") as f:
    |                  ^^^^
166 |                 text = f.read()
    |
help: Replace with `Path.open()`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> scripts/rag/embed_news_articles.py:86:13
   |
84 |               print(f"   Total documents: {self.collection.count()}")
85 |           except Exception as e:
86 | /             raise RuntimeError(
87 | |                 f"Collection '{COLLECTION_NAME}' not found. "
88 | |                 f"Run build_vector_store.py first. Error: {e}"
89 | |             )
   | |_____________^
90 |
91 |           # Initialize embedding model (same as court docs)
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/embed_news_articles.py:103:18
    |
101 |         """Load previously processed article IDs for resume capability."""
102 |         if not self.force_reindex and PROGRESS_FILE.exists():
103 |             with open(PROGRESS_FILE) as f:
    |                  ^^^^
104 |                 progress = json.load(f)
105 |                 processed = set(progress.get("processed_article_ids", []))
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/embed_news_articles.py:112:14
    |
110 |     def _save_progress(self, processed_ids: set):
111 |         """Save progress for resume capability."""
112 |         with open(PROGRESS_FILE, "w") as f:
    |              ^^^^
113 |             json.dump(
114 |                 {
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/embed_news_articles.py:138:14
    |
136 |             )
137 |
138 |         with open(NEWS_INDEX_PATH, encoding="utf-8") as f:
    |              ^^^^
139 |             data = json.load(f)
    |
help: Replace with `Path.open()`

PLW2901 Outer `for` loop variable `article` overwritten by inner `for` loop target
   --> scripts/rag/embed_news_articles.py:319:25
    |
318 |                     # Mark as processed
319 |                     for article in articles_to_process[
    |                         ^^^^^^^
320 |                         len(self.processed_ids) : len(self.processed_ids) + len(batch_ids)
321 |                     ]:
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/kg_rag_integration.py:52:14
   |
51 |         # Load entity-document index
52 |         with open(ENTITY_DOC_INDEX_PATH) as f:
   |              ^^^^
53 |             self.entity_doc_index = json.load(f)
54 |             print("✅ Entity index loaded")
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/kg_rag_integration.py:57:14
   |
56 |         # Load entity network
57 |         with open(ENTITY_NETWORK_PATH) as f:
   |              ^^^^
58 |             self.entity_network = json.load(f)
59 |             print("✅ Entity network loaded")
   |
help: Replace with `Path.open()`

SIM102 Use a single `if` statement instead of nested `if` statements
   --> scripts/rag/kg_rag_integration.py:335:13
    |
334 |               # Filter by required entities
335 | /             if required_entities:
336 | |                 if not all(e in entity_mentions for e in required_entities):
    | |____________________________________________________________________________^
337 |                       continue
    |
help: Combine `if` statements using `and`

PLW2901 Outer `for` loop variable `i` overwritten by inner `for` loop target
   --> scripts/rag/kg_rag_integration.py:341:17
    |
339 |             # Calculate graph score (based on entity connections)
340 |             graph_score = 0
341 |             for i, entity1 in enumerate(entity_mentions):
    |                 ^
342 |                 for entity2 in entity_mentions[i + 1 :]:
343 |                     # Check if entities are connected in graph
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/link_entities_to_docs.py:45:14
   |
43 |     def _load_entity_index(self) -> dict:
44 |         """Load the master entity index."""
45 |         with open(ENTITY_INDEX_PATH) as f:
   |              ^^^^
46 |             data = json.load(f)
47 |             print(f"✅ Loaded entity index: {len(data.get('entities', []))} entities")
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/link_entities_to_docs.py:53:18
   |
51 |         """Load entity network for additional context."""
52 |         if ENTITY_NETWORK_PATH.exists():
53 |             with open(ENTITY_NETWORK_PATH) as f:
   |                  ^^^^
54 |                 data = json.load(f)
55 |                 print(
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/link_entities_to_docs.py:144:26
    |
142 |                 try:
143 |                     # Read document
144 |                     with open(file_path, encoding="utf-8") as f:
    |                          ^^^^
145 |                         text = f.read()
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/rag/link_entities_to_docs.py:224:14
    |
223 |         OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
224 |         with open(OUTPUT_PATH, "w") as f:
    |              ^^^^
225 |             json.dump(output_data, f, indent=2)
    |
help: Replace with `Path.open()`

E722 Do not use bare `except`
  --> scripts/rag/query_rag.py:48:9
   |
46 |             self.collection = self.client.get_collection(name=COLLECTION_NAME)
47 |             print(f"✅ Collection loaded: {self.collection.count()} documents")
48 |         except:
   |         ^^^^^^
49 |             print("❌ Error: Collection not found. Run build_vector_store.py first.")
50 |             sys.exit(1)
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/query_rag.py:66:18
   |
64 |         """Load entity-document index."""
65 |         if ENTITY_DOC_INDEX_PATH.exists():
66 |             with open(ENTITY_DOC_INDEX_PATH) as f:
   |                  ^^^^
67 |                 data = json.load(f)
68 |                 print(
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/rag/query_rag.py:77:18
   |
75 |         """Load entity network."""
76 |         if ENTITY_NETWORK_PATH.exists():
77 |             with open(ENTITY_NETWORK_PATH) as f:
   |                  ^^^^
78 |                 data = json.load(f)
79 |                 print(
   |
help: Replace with `Path.open()`

E722 Do not use bare `except`
   --> scripts/rag/query_rag.py:216:13
    |
214 |                         }
215 |                     )
216 |             except:
    |             ^^^^^^
217 |                 pass
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/reorganize_data.py:188:10
    |
186 |     index_path = DATA_DIR / "metadata" / "source_index.json"
187 |     index_path.parent.mkdir(exist_ok=True)
188 |     with open(index_path, "w") as f:
    |          ^^^^
189 |         json.dump(index, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/research/add_researched_entities.py:11:6
   |
10 | # Load existing data
11 | with open("/Users/masa/Projects/Epstein/data/metadata/enriched_entity_data.json") as f:
   |      ^^^^
12 |     data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/research/add_researched_entities.py:391:6
    |
390 | # Save
391 | with open("/Users/masa/Projects/Epstein/data/metadata/enriched_entity_data.json", "w") as f:
    |      ^^^^
392 |     json.dump(data, f, indent=2)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/research/basic_entity_whois.py:34:10
   |
32 | def load_json(filepath):
33 |     """Load JSON file"""
34 |     with open(filepath, encoding="utf-8") as f:
   |          ^^^^
35 |         return json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/research/basic_entity_whois.py:40:10
   |
38 | def save_json(filepath, data):
39 |     """Save JSON file with formatting"""
40 |     with open(filepath, "w", encoding="utf-8") as f:
   |          ^^^^
41 |         json.dump(data, f, indent=2, ensure_ascii=False)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (87 > 60)
   --> scripts/research/basic_entity_whois.py:196:5
    |
196 | def enrich_entities():
    |     ^^^^^^^^^^^^^^^
197 |     """Main function to enrich all entities with Wikipedia bios"""
198 |     print("=" * 80)
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/research/basic_entity_whois.py:353:10
    |
351 |     report_text = "\n".join(report_lines)
352 |
353 |     with open(REPORT_FILE, "w", encoding="utf-8") as f:
    |          ^^^^
354 |         f.write(report_text)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/research/enrich_entity_data.py:29:10
   |
27 | def load_priority_entities():
28 |     """Load the priority entity list"""
29 |     with open(PRIORITY_LIST) as f:
   |          ^^^^
30 |         data = json.load(f)
31 |     return data["entities"]
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/research/enrich_entity_data.py:36:10
   |
34 | def load_enriched_data():
35 |     """Load existing enriched entity data"""
36 |     with open(ENRICHED_DATA) as f:
   |          ^^^^
37 |         return json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/research/enrich_entity_data.py:42:10
   |
40 | def load_entity_index():
41 |     """Load the main entity index"""
42 |     with open(ENTITY_INDEX) as f:
   |          ^^^^
43 |         return json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/search/entity_search.py:26:14
   |
24 |         # Load semantic index (entity -> documents)
25 |         semantic_index_path = METADATA_DIR / "semantic_index.json"
26 |         with open(semantic_index_path) as f:
   |              ^^^^
27 |             self.semantic_data = json.load(f)
28 |             self.entity_to_docs = self.semantic_data.get("entity_to_documents", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/search/entity_search.py:32:14
   |
30 |         # Load classifications
31 |         classifications_path = METADATA_DIR / "document_classifications.json"
32 |         with open(classifications_path) as f:
   |              ^^^^
33 |             self.classifications = json.load(f).get("results", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/search/entity_search.py:37:14
   |
35 |         # Load entity network
36 |         network_path = METADATA_DIR / "entity_network.json"
37 |         with open(network_path) as f:
   |              ^^^^
38 |             self.network_data = json.load(f)
39 |             self.entity_nodes = {n["id"]: n for n in self.network_data["nodes"]}
   |
help: Replace with `Path.open()`

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
   --> scripts/utilities/convert_emails_to_markdown.py:88:17
    |
 87 |       # Document definitions based on Research agent's analysis
 88 |       DOCUMENTS = [
    |  _________________^
 89 | |         {
 90 | |             "id": 1,
 91 | |             "type": "Email",
 92 | |             "pages": range(1, 6),
 93 | |             "subject": "RE: Epstein",
 94 | |             "category": "emails",
 95 | |         },
 96 | |         {
 97 | |             "id": 2,
 98 | |             "type": "Email",
 99 | |             "pages": range(6, 9),
100 | |             "subject": "FW: Confidential",
101 | |             "category": "emails",
102 | |         },
103 | |         {
104 | |             "id": 3,
105 | |             "type": "Letter",
106 | |             "pages": range(9, 11),
107 | |             "subject": "Flight logs letter",
108 | |             "category": "legal",
109 | |         },
110 | |         {
111 | |             "id": 4,
112 | |             "type": "Invoice",
113 | |             "pages": range(11, 16),
114 | |             "subject": "Flight service invoices",
115 | |             "category": "records",
116 | |         },
117 | |         {
118 | |             "id": 5,
119 | |             "type": "Subpoena",
120 | |             "pages": range(16, 18),
121 | |             "subject": "Flight logs subpoena",
122 | |             "category": "legal",
123 | |         },
124 | |         {
125 | |             "id": 6,
126 | |             "type": "Report",
127 | |             "pages": range(18, 20),
128 | |             "subject": "DAVID Summary - Criminal database",
129 | |             "category": "records",
130 | |         },
131 | |         {
132 | |             "id": 7,
133 | |             "type": "Subpoena",
134 | |             "pages": [20],
135 | |             "subject": "Investigation assignment",
136 | |             "category": "legal",
137 | |         },
138 | |         {
139 | |             "id": 8,
140 | |             "type": "Note",
141 | |             "pages": [21],
142 | |             "subject": "Address location",
143 | |             "category": "notes",
144 | |         },
145 | |         {
146 | |             "id": 9,
147 | |             "type": "Report",
148 | |             "pages": range(22, 42),
149 | |             "subject": "FACTS Report - Background check",
150 | |             "category": "records",
151 | |         },
152 | |         {
153 | |             "id": 10,
154 | |             "type": "Letter",
155 | |             "pages": range(42, 46),
156 | |             "subject": "Legal correspondence",
157 | |             "category": "legal",
158 | |         },
159 | |         {
160 | |             "id": 11,
161 | |             "type": "Memo",
162 | |             "pages": range(46, 55),
163 | |             "subject": "State Attorney memo",
164 | |             "category": "legal",
165 | |         },
166 | |         {
167 | |             "id": 12,
168 | |             "type": "Directions",
169 | |             "pages": [55],
170 | |             "subject": "Driving directions",
171 | |             "category": "notes",
172 | |         },
173 | |         {
174 | |             "id": 13,
175 | |             "type": "Subpoena",
176 | |             "pages": [56],
177 | |             "subject": "Legal subpoena",
178 | |             "category": "legal",
179 | |         },
180 | |         {
181 | |             "id": 14,
182 | |             "type": "Memo",
183 | |             "pages": [57],
184 | |             "subject": "State Attorney document",
185 | |             "category": "legal",
186 | |         },
187 | |         {
188 | |             "id": 15,
189 | |             "type": "Subpoena",
190 | |             "pages": range(58, 60),
191 | |             "subject": "Legal subpoena",
192 | |             "category": "legal",
193 | |         },
194 | |         {
195 | |             "id": 16,
196 | |             "type": "Memo",
197 | |             "pages": range(60, 69),
198 | |             "subject": "Impeachment Material",
199 | |             "category": "legal",
200 | |         },
201 | |         {
202 | |             "id": 17,
203 | |             "type": "Email",
204 | |             "pages": range(69, 88),
205 | |             "subject": "RE: Meeting with Epstein's attorneys",
206 | |             "category": "emails",
207 | |         },
208 | |     ]
    | |_____^
209 |
210 |       def __init__(self, pages_dir: Path, notes_file: Path, output_dir: Path):
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utilities/convert_emails_to_markdown.py:218:14
    |
216 |     def _load_notes(self) -> list[dict]:
217 |         """Load annotation notes from JSON file."""
218 |         with open(self.notes_file) as f:
    |              ^^^^
219 |             data = json.load(f)
220 |             return data.get("results", [])
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utilities/convert_emails_to_markdown.py:227:14
    |
225 |         if not page_file.exists():
226 |             return ""
227 |         with open(page_file, encoding="utf-8") as f:
    |              ^^^^
228 |             return f.read()
    |
help: Replace with `Path.open()`

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> scripts/utilities/convert_emails_to_markdown.py:342:30
    |
340 |                 if match:
341 |                     try:
342 |                         dt = datetime.strptime(match.group(1), fmt)
    |                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
343 |                         date_str = dt.strftime("%Y-%m-%d")
344 |                         break
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

E722 Do not use bare `except`
   --> scripts/utilities/convert_emails_to_markdown.py:345:21
    |
343 |                         date_str = dt.strftime("%Y-%m-%d")
344 |                         break
345 |                     except:
    |                     ^^^^^^
346 |                         pass
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utilities/convert_emails_to_markdown.py:442:22
    |
440 |                 output_file.parent.mkdir(parents=True, exist_ok=True)
441 |
442 |                 with open(output_file, "w", encoding="utf-8") as f:
    |                      ^^^^
443 |                     f.write(content)
    |
help: Replace with `Path.open()`

PLC0206 Extracting value from dictionary without calling `.items()`
   --> scripts/utilities/convert_emails_to_markdown.py:489:9
    |
488 |       for filepath in sorted(stats["files_created"]):
489 | /         for category in categories:
490 | |             if f"/{category}/" in filepath:
491 | |                 categories[category].append(filepath)
492 | |                 break
    | |_____________________^
493 |
494 |       # Add each category
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utilities/convert_emails_to_markdown.py:525:10
    |
524 |     index_file = output_dir / "INDEX.md"
525 |     with open(index_file, "w", encoding="utf-8") as f:
    |          ^^^^
526 |         f.write("\n".join(index_lines))
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utilities/convert_emails_to_markdown.py:599:10
    |
598 |     stats_file = output_dir / "STATISTICS.md"
599 |     with open(stats_file, "w", encoding="utf-8") as f:
    |          ^^^^
600 |         f.write("\n".join(lines))
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/utils/build_entity_mappings.py:41:10
   |
39 |         Dictionary mapping variant names to canonical names
40 |     """
41 |     with open(entities_index_path) as f:
   |          ^^^^
42 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utils/build_entity_mappings.py:162:10
    |
160 |     # Save mappings
161 |     output_path.parent.mkdir(parents=True, exist_ok=True)
162 |     with open(output_path, "w") as f:
    |          ^^^^
163 |         json.dump(mappings, f, indent=2, sort_keys=True)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/utils/entity_filtering.py:63:18
   |
62 |         try:
63 |             with open(self.filter_list_path) as f:
   |                  ^^^^
64 |                 data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utils/entity_filtering.py:128:14
    |
126 |             return {}
127 |
128 |         with open(self.filter_list_path) as f:
    |              ^^^^
129 |             data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utils/entity_filtering.py:142:14
    |
140 |             return "Filter list not available"
141 |
142 |         with open(self.filter_list_path) as f:
    |              ^^^^
143 |             data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/utils/entity_normalization.py:39:14
   |
37 |             return
38 |
39 |         with open(self.mappings_path) as f:
   |              ^^^^
40 |             self.mappings = json.load(f)
   |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/utils/entity_normalization.py:120:9
    |
118 |             List of (canonical_name, variant_count) tuples
119 |         """
120 |         from collections import Counter
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
121 |
122 |         canonical_counts = Counter(self.mappings.values())
    |

PLW0603 Using the global statement to update `_normalizer_instance` is discouraged
   --> scripts/utils/entity_normalization.py:136:12
    |
134 |         EntityNormalizer instance
135 |     """
136 |     global _normalizer_instance
    |            ^^^^^^^^^^^^^^^^^^^^
137 |     if _normalizer_instance is None:
138 |         _normalizer_instance = EntityNormalizer()
    |

E402 Module level import not at top of file
  --> scripts/utils/verify_normalization.py:23:1
   |
21 | # Add utils to path
22 | sys.path.insert(0, str(Path(__file__).parent))
23 | from entity_normalization import EntityNormalizer
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/utils/verify_normalization.py:52:10
   |
51 |     network_path = METADATA_DIR / "entity_network.json"
52 |     with open(network_path) as f:
   |          ^^^^
53 |         network = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/utils/verify_normalization.py:113:10
    |
112 |     network_path = METADATA_DIR / "entity_network.json"
113 |     with open(network_path) as f:
    |          ^^^^
114 |         network = json.load(f)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/utils/verify_normalization.py:147:9
    |
145 |         server_path = PROJECT_ROOT / "server" / "services"
146 |         sys.path.insert(0, str(server_path))
147 |         from entity_disambiguation import get_disambiguator
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
148 |
149 |         disambiguator = get_disambiguator()
    |

E402 Module level import not at top of file
  --> scripts/validation/run_pydantic_audit.py:33:1
   |
31 | os.environ["USE_PYDANTIC"] = "true"
32 |
33 | from server.services.entity_service import EntityService
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/validation/run_pydantic_audit.py:127:10
    |
125 | """
126 |
127 |     with open(output_path, "w") as f:
    |          ^^^^
128 |         f.write(report)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/validation/run_pydantic_audit.py:133:5
    |
131 | def main():
132 |     """Run Pydantic data quality audit"""
133 |     import argparse
    |     ^^^^^^^^^^^^^^^
134 |
135 |     parser = argparse.ArgumentParser(description="Run Pydantic data quality audit")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/validation/run_pydantic_audit.py:187:10
    |
185 |     # Save errors to JSON for machine processing
186 |     errors_json_path = project_root / "pydantic_validation_errors.json"
187 |     with open(errors_json_path, "w") as f:
    |          ^^^^
188 |         json.dump(
189 |             {
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/validation/validate_entity_names.py:44:14
   |
42 |     def load_data(self) -> tuple[list[dict], list[dict]]:
43 |         """Load entities and flight data."""
44 |         with open(self.entities_file) as f:
   |              ^^^^
45 |             entities_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/validation/validate_entity_names.py:47:14
   |
45 |             entities_data = json.load(f)
46 |
47 |         with open(self.flights_file) as f:
   |              ^^^^
48 |             flights_data = json.load(f)
   |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/validation/validate_entity_names.py:261:5
    |
259 | def main():
260 |     """Main validation entry point."""
261 |     import argparse
    |     ^^^^^^^^^^^^^^^
262 |
263 |     parser = argparse.ArgumentParser(description="Validate entity name formatting")
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/verification/verify_all_fixes.py:25:10
   |
23 |     network_file = Path("/Users/masa/Projects/Epstein/data/metadata/entity_network.json")
24 |
25 |     with open(network_file) as f:
   |          ^^^^
26 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/verification/verify_all_fixes.py:53:10
   |
51 |     app_js = Path("/Users/masa/Projects/Epstein/server/web/app.js")
52 |
53 |     with open(app_js) as f:
   |          ^^^^
54 |         content = f.read()
   |
help: Replace with `Path.open()`

B007 Loop control variable `status` not used within loop body
  --> scripts/verification/verify_all_fixes.py:68:20
   |
66 |     if all_passed:
67 |         print("✅ PASSED: All progressive loading components found:")
68 |         for check, status in checks.items():
   |                    ^^^^^^
69 |             print(f"   ✓ {check}")
70 |         return True
   |
help: Rename unused `status` to `_status`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/verification/verify_all_fixes.py:86:10
   |
84 |     docs_js = Path("/Users/masa/Projects/Epstein/server/web/documents.js")
85 |
86 |     with open(docs_js) as f:
   |          ^^^^
87 |         content = f.read()
   |
help: Replace with `Path.open()`

B007 Loop control variable `status` not used within loop body
   --> scripts/verification/verify_all_fixes.py:99:20
    |
 97 |     if all_passed:
 98 |         print("✅ PASSED: All dropdown fix components found:")
 99 |         for check, status in checks.items():
    |                    ^^^^^^
100 |             print(f"   ✓ {check}")
101 |         return True
    |
help: Rename unused `status` to `_status`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/verification/verify_all_fixes.py:117:10
    |
115 |     index_html = Path("/Users/masa/Projects/Epstein/server/web/index.html")
116 |
117 |     with open(index_html) as f:
    |          ^^^^
118 |         content = f.read()
    |
help: Replace with `Path.open()`

B007 Loop control variable `status` not used within loop body
   --> scripts/verification/verify_all_fixes.py:135:20
    |
133 |     if all_passed:
134 |         print("✅ PASSED: All legend components found:")
135 |         for check, status in checks.items():
    |                    ^^^^^^
136 |             print(f"   ✓ {check}")
137 |         return True
    |
help: Rename unused `status` to `_status`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/verification/verify_all_fixes.py:153:10
    |
151 |     app_js = Path("/Users/masa/Projects/Epstein/server/web/app.js")
152 |
153 |     with open(app_js) as f:
    |          ^^^^
154 |         content = f.read()
    |
help: Replace with `Path.open()`

B007 Loop control variable `status` not used within loop body
   --> scripts/verification/verify_all_fixes.py:169:20
    |
167 |     if all_passed:
168 |         print("✅ PASSED: All edge styling components found:")
169 |         for check, status in checks.items():
    |                    ^^^^^^
170 |             print(f"   ✓ {check}")
171 |         return True
    |
help: Rename unused `status` to `_status`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/verification/verify_task_completion.py:26:10
   |
24 |     # Check ENTITIES_INDEX.json
25 |     entities_file = BASE_DIR / "data/md/entities/ENTITIES_INDEX.json"
26 |     with open(entities_file) as f:
   |          ^^^^
27 |         data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> scripts/verification/verify_task_completion.py:87:10
   |
85 |     # Check HTML file
86 |     html_file = BASE_DIR / "server/web/index.html"
87 |     with open(html_file) as f:
   |          ^^^^
88 |         html_content = f.read()
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> scripts/verification/verify_task_completion.py:111:10
    |
109 |     # Check JavaScript file
110 |     js_file = BASE_DIR / "server/web/app.js"
111 |     with open(js_file) as f:
    |          ^^^^
112 |         js_content = f.read()
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> scripts/verification/verify_task_completion.py:132:5
    |
130 |     # Check syntax
131 |     print("\n🔍 Syntax Validation")
132 |     import subprocess
    |     ^^^^^^^^^^^^^^^^^
133 |
134 |     try:
    |

PLW0603 Using the global statement to update `entity_service` is discouraged
  --> server/api_routes.py:43:12
   |
41 |     Call this from app.py startup event
42 |     """
43 |     global entity_service, flight_service, document_service, network_service
   |            ^^^^^^^^^^^^^^
44 |
45 |     entity_service = EntityService(data_path)
   |

PLW0603 Using the global statement to update `flight_service` is discouraged
  --> server/api_routes.py:43:28
   |
41 |     Call this from app.py startup event
42 |     """
43 |     global entity_service, flight_service, document_service, network_service
   |                            ^^^^^^^^^^^^^^
44 |
45 |     entity_service = EntityService(data_path)
   |

PLW0603 Using the global statement to update `document_service` is discouraged
  --> server/api_routes.py:43:44
   |
41 |     Call this from app.py startup event
42 |     """
43 |     global entity_service, flight_service, document_service, network_service
   |                                            ^^^^^^^^^^^^^^^^
44 |
45 |     entity_service = EntityService(data_path)
   |

PLW0603 Using the global statement to update `network_service` is discouraged
  --> server/api_routes.py:43:62
   |
41 |     Call this from app.py startup event
42 |     """
43 |     global entity_service, flight_service, document_service, network_service
   |                                                              ^^^^^^^^^^^^^^^
44 |
45 |     entity_service = EntityService(data_path)
   |

E402 Module level import not at top of file
  --> server/app.py:36:1
   |
35 | # Import audit logger (after Path is available)
36 | import sys
   | ^^^^^^^^^^
   |

E402 Module level import not at top of file
  --> server/app.py:41:1
   |
39 | SERVER_DIR = Path(__file__).parent
40 | sys.path.insert(0, str(SERVER_DIR))
41 | from services.audit_logger import AuditLogger, LoginEvent
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
42 | from services.file_watcher import FileWatcherService
   |

E402 Module level import not at top of file
  --> server/app.py:42:1
   |
40 | sys.path.insert(0, str(SERVER_DIR))
41 | from services.audit_logger import AuditLogger, LoginEvent
42 | from services.file_watcher import FileWatcherService
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

PLW0603 Using the global statement to update `openrouter_client` is discouraged
  --> server/app.py:67:12
   |
65 | def get_openrouter_client():
66 |     """Initialize OpenRouter client (lazy loading)"""
67 |     global openrouter_client
   |            ^^^^^^^^^^^^^^^^^
68 |     if openrouter_client is None:
69 |         api_key = os.getenv("OPENROUTER_API_KEY")
   |

E402 Module level import not at top of file
  --> server/app.py:87:1
   |
86 | # Load credentials from file
87 | import os
   | ^^^^^^^^^
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:107:14
    |
106 |     if CREDENTIALS_FILE.exists():
107 |         with open(CREDENTIALS_FILE) as f:
    |              ^^^^
108 |             for line in f:
109 |                 line = line.strip()
    |
help: Replace with `Path.open()`

PLW2901 `for` loop variable `line` overwritten by assignment target
   --> server/app.py:109:17
    |
107 |         with open(CREDENTIALS_FILE) as f:
108 |             for line in f:
109 |                 line = line.strip()
    |                 ^^^^
110 |                 if line and not line.startswith("#") and ":" in line:
111 |                     username, password = line.split(":", 1)
    |

ARG001 Unused function argument: `request`
   --> server/app.py:217:5
    |
215 | # Flexible authentication (supports both session cookie and HTTP Basic Auth)
216 | async def get_current_user(
217 |     request: Request,
    |     ^^^^^^^
218 |     credentials: Optional[HTTPBasicCredentials] = Depends(HTTPBasic(auto_error=False)),
219 | ) -> str:
    |

ARG001 Unused function argument: `credentials`
   --> server/app.py:218:5
    |
216 | async def get_current_user(
217 |     request: Request,
218 |     credentials: Optional[HTTPBasicCredentials] = Depends(HTTPBasic(auto_error=False)),
    |     ^^^^^^^^^^^
219 | ) -> str:
220 |     """
    |

PLW0602 Using global for `name_to_id` but no assignment is done
   --> server/app.py:290:12
    |
288 | def build_name_mappings():
289 |     """Build reverse mappings from names to entity IDs for backward compatibility"""
290 |     global name_to_id, id_to_name
    |            ^^^^^^^^^^
291 |
292 |     for entity_id, entity_data in entity_stats.items():
    |

PLW0602 Using global for `id_to_name` but no assignment is done
   --> server/app.py:290:24
    |
288 | def build_name_mappings():
289 |     """Build reverse mappings from names to entity IDs for backward compatibility"""
290 |     global name_to_id, id_to_name
    |                        ^^^^^^^^^^
291 |
292 |     for entity_id, entity_data in entity_stats.items():
    |

PLR0915 Too many statements (74 > 60)
   --> server/app.py:311:5
    |
311 | def load_data():
    |     ^^^^^^^^^
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |

PLW0603 Using the global statement to update `entity_stats` is discouraged
   --> server/app.py:313:12
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |            ^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `entity_stats` is discouraged
   --> server/app.py:313:12
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |            ^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `entity_stats` is discouraged
   --> server/app.py:313:12
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |            ^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `network_data` is discouraged
   --> server/app.py:313:26
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                          ^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `network_data` is discouraged
   --> server/app.py:313:26
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                          ^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `network_data` is discouraged
   --> server/app.py:313:26
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                          ^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `semantic_index` is discouraged
   --> server/app.py:313:40
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                        ^^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `semantic_index` is discouraged
   --> server/app.py:313:40
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                        ^^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `semantic_index` is discouraged
   --> server/app.py:313:40
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                        ^^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `classifications` is discouraged
   --> server/app.py:313:56
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                                        ^^^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `classifications` is discouraged
   --> server/app.py:313:56
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                                        ^^^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `classifications` is discouraged
   --> server/app.py:313:56
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                                        ^^^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `timeline_data` is discouraged
   --> server/app.py:313:73
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                                                         ^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `timeline_data` is discouraged
   --> server/app.py:313:73
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                                                         ^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0603 Using the global statement to update `timeline_data` is discouraged
   --> server/app.py:313:73
    |
311 | def load_data():
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
    |                                                                         ^^^^^^^^^^^^^
314 |     global name_to_id, id_to_name
    |

PLW0602 Using global for `name_to_id` but no assignment is done
   --> server/app.py:314:12
    |
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
314 |     global name_to_id, id_to_name
    |            ^^^^^^^^^^
315 |
316 |     print("Loading data...")
    |

PLW0602 Using global for `id_to_name` but no assignment is done
   --> server/app.py:314:24
    |
312 |     """Load all JSON data into memory with error handling"""
313 |     global entity_stats, network_data, semantic_index, classifications, timeline_data
314 |     global name_to_id, id_to_name
    |                        ^^^^^^^^^^
315 |
316 |     print("Loading data...")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:322:18
    |
320 |     if stats_path.exists():
321 |         try:
322 |             with open(stats_path) as f:
    |                  ^^^^
323 |                 data = json.load(f)
324 |                 # Fix: entity_statistics.json has structure: {statistics: {entity_name: {...}}}
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:342:18
    |
340 |     if network_path.exists():
341 |         try:
342 |             with open(network_path) as f:
    |                  ^^^^
343 |                 network_data = json.load(f)
344 |                 print(f"  ✓ Loaded {len(network_data.get('nodes', []))} network nodes")
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:356:18
    |
354 |     if semantic_path.exists():
355 |         try:
356 |             with open(semantic_path) as f:
    |                  ^^^^
357 |                 data = json.load(f)
358 |                 semantic_index = data.get("entity_to_documents", {})
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:371:18
    |
369 |     if class_path.exists():
370 |         try:
371 |             with open(class_path) as f:
    |                  ^^^^
372 |                 data = json.load(f)
373 |                 classifications = data.get("results", {})
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:386:18
    |
384 |     if timeline_path.exists():
385 |         try:
386 |             with open(timeline_path) as f:
    |                  ^^^^
387 |                 timeline_data = json.load(f)
388 |                 print("  ✓ Loaded timeline data")
    |
help: Replace with `Path.open()`

ARG001 Unused function argument: `admin_user`
   --> server/app.py:647:5
    |
645 |     limit: int = Query(100, le=1000),
646 |     offset: int = Query(0, ge=0),
647 |     admin_user: str = Depends(get_current_user),
    |     ^^^^^^^^^^
648 | ):
649 |     """Get login audit logs (admin only)
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/app.py:667:9
    |
665 |           return {"total": len(logs), "limit": limit, "offset": offset, "logs": logs}
666 |       except Exception as e:
667 | /         raise HTTPException(
668 | |             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
669 | |             detail=f"Failed to retrieve audit logs: {e!s}",
670 | |         )
    | |_________^
    |

ARG001 Unused function argument: `admin_user`
   --> server/app.py:678:5
    |
676 |     limit: int = Query(100, le=1000),
677 |     offset: int = Query(0, ge=0),
678 |     admin_user: str = Depends(get_current_user),
    |     ^^^^^^^^^^
679 | ):
680 |     """Get security events for admin dashboard
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/app.py:698:9
    |
696 |           return {"total": len(events), "limit": limit, "offset": offset, "events": events}
697 |       except Exception as e:
698 | /         raise HTTPException(
699 | |             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
700 | |             detail=f"Failed to retrieve security events: {e!s}",
701 | |         )
    | |_________^
    |

ARG001 Unused function argument: `admin_user`
   --> server/app.py:705:32
    |
704 | @app.get("/api/admin/login-statistics")
705 | async def get_login_statistics(admin_user: str = Depends(get_current_user)):
    |                                ^^^^^^^^^^
706 |     """Get aggregate login statistics for admin dashboard
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/app.py:715:9
    |
713 |           return stats
714 |       except Exception as e:
715 | /         raise HTTPException(
716 | |             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
717 | |             detail=f"Failed to retrieve statistics: {e!s}",
718 | |         )
    | |_________^
    |

ARG001 Unused function argument: `admin_user`
   --> server/app.py:723:43
    |
721 | @app.post("/api/admin/anonymize-logs")
722 | async def anonymize_old_logs(
723 |     days: int = Query(90, ge=30, le=365), admin_user: str = Depends(get_current_user)
    |                                           ^^^^^^^^^^
724 | ):
725 |     """Anonymize audit logs older than specified days (GDPR compliance)
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/app.py:738:9
    |
736 |           return {"success": True, "anonymized_records": count, "days_threshold": days}
737 |       except Exception as e:
738 | /         raise HTTPException(
739 | |             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
740 | |             detail=f"Failed to anonymize logs: {e!s}",
741 | |         )
    | |_________^
    |

ARG001 Unused function argument: `username`
   --> server/app.py:754:21
    |
753 | @app.get("/api/about")
754 | async def get_about(username: str = Depends(get_current_user)):
    |                     ^^^^^^^^
755 |     """Get ABOUT.md content for home page
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:780:14
    |
779 |         # Read markdown content
780 |         with open(about_path, encoding="utf-8") as f:
    |              ^^^^
781 |             content = f.read()
    |
help: Replace with `Path.open()`

DTZ006 `datetime.datetime.fromtimestamp()` called without a `tz` argument
   --> server/app.py:785:22
    |
783 |         # Get file metadata
784 |         stat = about_path.stat()
785 |         updated_at = datetime.fromtimestamp(stat.st_mtime).isoformat()
    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
786 |
787 |         return {"content": content, "updated_at": updated_at, "file_size": stat.st_size}
    |
help: Pass a `datetime.timezone` object to the `tz` parameter

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/app.py:793:9
    |
791 |     except Exception as e:
792 |         logger.error(f"Error reading ABOUT.md: {e}")
793 |         raise HTTPException(status_code=500, detail=f"Failed to read ABOUT.md: {e!s}")
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

ARG001 Unused function argument: `username`
   --> server/app.py:798:50
    |
796 | @app.get("/api/updates")
797 | async def get_updates(
798 |     limit: int = Query(default=10, ge=1, le=50), username: str = Depends(get_current_user)
    |                                                  ^^^^^^^^
799 | ):
800 |     """Get latest git commits for home page updates feed
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/app.py:849:9
    |
847 |     except subprocess.TimeoutExpired:
848 |         logger.error("Git log command timed out")
849 |         raise HTTPException(status_code=500, detail="Git command timed out")
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
850 |     except Exception as e:
851 |         logger.error(f"Error fetching git updates: {e}")
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/app.py:852:9
    |
850 |     except Exception as e:
851 |         logger.error(f"Error fetching git updates: {e}")
852 |         raise HTTPException(status_code=500, detail=f"Error fetching updates: {e!s}")
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

ARG001 Unused function argument: `username`
   --> server/app.py:856:21
    |
855 | @app.get("/api/stats")
856 | async def get_stats(username: str = Depends(get_current_user)):
    |                     ^^^^^^^^
857 |     """Get overall statistics
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:873:18
    |
871 |     if source_index_path.exists():
872 |         try:
873 |             with open(source_index_path) as f:
    |                  ^^^^
874 |                 sources_data = json.load(f)
875 |                 # source_index.json has structure: {sources: {key: {description: ...}}}
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:896:18
    |
894 |     if unified_index_path.exists():
895 |         try:
896 |             with open(unified_index_path) as f:
    |                  ^^^^
897 |                 unified_data = json.load(f)
898 |                 total_documents = unified_data.get("total_documents", len(classifications))
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:911:22
    |
909 |         if master_index_path.exists():
910 |             try:
911 |                 with open(master_index_path) as f:
    |                      ^^^^
912 |                     index_data = json.load(f)
913 |                     total_documents = index_data.get("unique_documents", len(classifications))
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:922:18
    |
920 |     if flight_data_path.exists():
921 |         try:
922 |             with open(flight_data_path) as f:
    |                  ^^^^
923 |                 flight_data = json.load(f)
924 |                 flight_count = len(flight_data.get("flights", []))
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:933:18
    |
931 |     if news_index_path.exists():
932 |         try:
933 |             with open(news_index_path) as f:
    |                  ^^^^
934 |                 news_data = json.load(f)
935 |                 news_articles_count = len(news_data.get("articles", []))
    |
help: Replace with `Path.open()`

ARG001 Unused function argument: `username`
   --> server/app.py:956:29
    |
955 | @app.get("/api/sources/index")
956 | async def get_sources_index(username: str = Depends(get_current_user)):
    |                             ^^^^^^^^
957 |     """Get master document index with deduplication statistics.
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/app.py:987:14
    |
986 |         # Load master index (on-demand, not cached due to size)
987 |         with open(index_path) as f:
    |              ^^^^
988 |             index_data = json.load(f)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:1071:9
     |
1069 |     except Exception as e:
1070 |         print(f"Error loading sources index: {e}")
1071 |         import traceback
     |         ^^^^^^^^^^^^^^^^
1072 |
1073 |         traceback.print_exc()
     |

PLR0915 Too many statements (73 > 60)
    --> server/app.py:1084:11
     |
1083 | @app.get("/api/ingestion/status")
1084 | async def get_ingestion_status(username: str = Depends(get_current_user)):
     |           ^^^^^^^^^^^^^^^^^^^^
1085 |     """Get ingestion progress status
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1084:32
     |
1083 | @app.get("/api/ingestion/status")
1084 | async def get_ingestion_status(username: str = Depends(get_current_user)):
     |                                ^^^^^^^^
1085 |     """Get ingestion progress status
     |

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:1100:18
     |
1098 |     if merged_index_path.exists():
1099 |         try:
1100 |             with open(merged_index_path) as f:
     |                  ^^^^
1101 |                 entity_data = json.load(f)
1102 |                 entities_merged = entity_data.get("duplicates_merged", 0)
     |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:1132:18
     |
1130 |     if progress_path.exists():
1131 |         try:
1132 |             with open(progress_path) as f:
     |                  ^^^^
1133 |                 progress_data = json.load(f)
1134 |                 last_updated = progress_data.get("last_updated")
     |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:1143:18
     |
1141 |     if download_log_path.exists():
1142 |         try:
1143 |             with open(download_log_path) as f:
     |                  ^^^^
1144 |                 log_content = f.read()
1145 |                 # Parse the last complete summary if it exists
     |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:1147:21
     |
1145 |                 # Parse the last complete summary if it exists
1146 |                 if "Download Complete" in log_content:
1147 |                     import re
     |                     ^^^^^^^^^
1148 |
1149 |                     success_match = re.search(r"Successfully downloaded: (\d+)", log_content)
     |

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:1168:13
     |
1166 |     if dedup_db_path.exists():
1167 |         try:
1168 |             import sqlite3
     |             ^^^^^^^^^^^^^^
1169 |
1170 |             conn = sqlite3.connect(str(dedup_db_path))
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1222:33
     |
1221 | @app.get("/api/chatbot/knowledge")
1222 | async def get_chatbot_knowledge(username: str = Depends(get_current_user)):
     |                                 ^^^^^^^^
1223 |     """Get comprehensive knowledge index for chatbot.
     |

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:1251:14
     |
1250 |     try:
1251 |         with open(knowledge_path) as f:
     |              ^^^^
1252 |             knowledge = json.load(f)
     |
help: Replace with `Path.open()`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
    --> server/app.py:1256:9
     |
1254 |         return JSONResponse(content=knowledge)
1255 |     except Exception as e:
1256 |         raise HTTPException(status_code=500, detail=f"Failed to load knowledge index: {e!s}")
     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1260:34
     |
1259 | @app.get("/api/entity-biographies")
1260 | async def get_entity_biographies(username: str = Depends(get_current_user)):
     |                                  ^^^^^^^^
1261 |     """Get entity biography summaries
     |

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:1279:14
     |
1277 |             return {}
1278 |
1279 |         with open(bio_path) as f:
     |              ^^^^
1280 |             data = json.load(f)
1281 |             # Extract entities dictionary from JSON structure
     |
help: Replace with `Path.open()`

ARG001 Unused function argument: `username`
    --> server/app.py:1291:27
     |
1290 | @app.get("/api/entity-tags")
1291 | async def get_entity_tags(username: str = Depends(get_current_user)):
     |                           ^^^^^^^^
1292 |     """Get entity tags and categorizations
     |

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:1310:14
     |
1308 |             return {}
1309 |
1310 |         with open(tags_path) as f:
     |              ^^^^
1311 |             data = json.load(f)
1312 |             # Extract entities dictionary from JSON structure
     |
help: Replace with `Path.open()`

ARG001 Unused function argument: `username`
    --> server/app.py:1328:5
     |
1326 |     filter_billionaires: bool = Query(False),
1327 |     filter_connected: bool = Query(False),
1328 |     username: str = Depends(get_current_user),
     |     ^^^^^^^^
1329 | ):
1330 |     """Get list of entities with optional filtering and sorting
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1360:41
     |
1359 | @app.get("/api/v2/entities/{entity_id}")
1360 | async def get_entity_v2(entity_id: str, username: str = Depends(get_current_user)):
     |                                         ^^^^^^^^
1361 |     """Get entity by ID (v2 - recommended)
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1384:39
     |
1383 | @app.get("/api/entities/{name_or_id}")
1384 | async def get_entity(name_or_id: str, username: str = Depends(get_current_user)):
     |                                       ^^^^^^^^
1385 |     """Get entity by name or ID (v1 - backward compatible)
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1427:48
     |
1426 | @app.get("/api/entities/resolve/{name}")
1427 | async def resolve_entity_name_to_id(name: str, username: str = Depends(get_current_user)):
     |                                                ^^^^^^^^
1428 |     """Resolve entity name to ID
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1464:35
     |
1462 | @app.post("/api/entities/batch/resolve")
1463 | async def batch_resolve_names(
1464 |     request: BatchResolveRequest, username: str = Depends(get_current_user)
     |                                   ^^^^^^^^
1465 | ):
1466 |     """Batch resolve names to IDs
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1510:5
     |
1508 |     max_nodes: int = Query(500, le=1000),
1509 |     deduplicate: bool = Query(True),
1510 |     username: str = Depends(get_current_user),
     |     ^^^^^^^^
1511 | ):
1512 |     """Get network graph data with optional deduplication
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1583:5
     |
1581 |     type: Optional[str] = Query(None),
1582 |     limit: int = Query(50, le=500),
1583 |     username: str = Depends(get_current_user),
     |     ^^^^^^^^
1584 | ):
1585 |     """Search for entities or documents
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1615:5
     |
1613 |     end_date: Optional[str] = None,
1614 |     limit: int = Query(1000, le=5000),
1615 |     username: str = Depends(get_current_user),
     |     ^^^^^^^^
1616 | ):
1617 |     """Get timeline events"""
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1680:38
     |
1679 | @app.post("/api/chat")
1680 | async def chat(message: ChatMessage, username: str = Depends(get_current_user)):
     |                                      ^^^^^^^^
1681 |     """Chat with GPT-4.5 assistant about the archive with RAG-powered document retrieval
     |

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:1693:13
     |
1691 |         # Import RAG dependencies
1692 |         try:
1693 |             import chromadb
     |             ^^^^^^^^^^^^^^^
1694 |             from chromadb.config import Settings
1695 |             from sentence_transformers import SentenceTransformer
     |

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:1694:13
     |
1692 |         try:
1693 |             import chromadb
1694 |             from chromadb.config import Settings
     |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
1695 |             from sentence_transformers import SentenceTransformer
1696 |         except ImportError:
     |

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:1695:13
     |
1693 |             import chromadb
1694 |             from chromadb.config import Settings
1695 |             from sentence_transformers import SentenceTransformer
     |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
1696 |         except ImportError:
1697 |             # Fallback to basic search if ChromaDB not available
     |

N806 Variable `VECTOR_STORE_DIR` in function should be lowercase
    --> server/app.py:1705:13
     |
1703 |         # Initialize ChromaDB and embedding model
1704 |         try:
1705 |             VECTOR_STORE_DIR = PROJECT_ROOT / "data/vector_store/chroma"
     |             ^^^^^^^^^^^^^^^^
1706 |             COLLECTION_NAME = "epstein_documents"
     |

N806 Variable `COLLECTION_NAME` in function should be lowercase
    --> server/app.py:1706:13
     |
1704 |         try:
1705 |             VECTOR_STORE_DIR = PROJECT_ROOT / "data/vector_store/chroma"
1706 |             COLLECTION_NAME = "epstein_documents"
     |             ^^^^^^^^^^^^^^^
1707 |
1708 |             chroma_client = chromadb.PersistentClient(
     |

E402 Module level import not at top of file
    --> server/app.py:1841:1
     |
1839 | # Initialize suggestion service
1840 | # Add utils path for entity filtering
1841 | import sys
     | ^^^^^^^^^^
1842 |
1843 | from models.suggested_source import (
     |

E402 Module level import not at top of file
    --> server/app.py:1843:1
     |
1841 |   import sys
1842 |
1843 | / from models.suggested_source import (
1844 | |     SourcePriority,
1845 | |     SourceStatus,
1846 | |     SuggestedSourceCreate,
1847 | |     SuggestedSourceUpdate,
1848 | | )
     | |_^
1849 |
1850 |   # Initialize entity disambiguation service
     |

E402 Module level import not at top of file
    --> server/app.py:1851:1
     |
1850 | # Initialize entity disambiguation service
1851 | from services.entity_disambiguation import get_disambiguator
     | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
1852 |
1853 | # Initialize entity enrichment service
     |

E402 Module level import not at top of file
    --> server/app.py:1854:1
     |
1853 | # Initialize entity enrichment service
1854 | from services.entity_enrichment import EntityEnrichmentService, format_for_ui
     | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
1855 | from services.suggestion_service import SuggestionService
     |

E402 Module level import not at top of file
    --> server/app.py:1855:1
     |
1853 | # Initialize entity enrichment service
1854 | from services.entity_enrichment import EntityEnrichmentService, format_for_ui
1855 | from services.suggestion_service import SuggestionService
     | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     |

E402 Module level import not at top of file
    --> server/app.py:1859:1
     |
1858 | sys.path.insert(0, str(PROJECT_ROOT / "scripts/utils"))
1859 | from entity_filtering import EntityFilter
     | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     |

F811 Redefinition of unused `startup_event` from line 469
    --> server/app.py:1878:11
     |
1876 | # Start file watcher on app startup
1877 | @app.on_event("startup")
1878 | async def startup_event():
     |           ^^^^^^^^^^^^^ `startup_event` redefined here
1879 |     """Initialize services on server startup"""
1880 |     if ENABLE_HOT_RELOAD:
     |
    ::: server/app.py:469:11
     |
 468 | @app.on_event("startup")
 469 | async def startup_event():
     |           ------------- previous definition of `startup_event` here
 470 |     """Load data on startup"""
 471 |     load_data()
     |
help: Remove definition: `startup_event`

ARG001 Unused function argument: `username`
    --> server/app.py:1940:5
     |
1938 |     limit: int = Query(100, le=500),
1939 |     offset: int = Query(0, ge=0),
1940 |     username: str = Depends(get_current_user),
     |     ^^^^^^^^
1941 | ):
1942 |     """Get list of source suggestions with filtering
     |

ARG001 Unused function argument: `username`
    --> server/app.py:1961:46
     |
1960 | @app.get("/api/suggestions/{suggestion_id}")
1961 | async def get_suggestion(suggestion_id: str, username: str = Depends(get_current_user)):
     |                                              ^^^^^^^^
1962 |     """Get single suggestion by ID
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2009:49
     |
2008 | @app.delete("/api/suggestions/{suggestion_id}")
2009 | async def delete_suggestion(suggestion_id: str, username: str = Depends(get_current_user)):
     |                                                 ^^^^^^^^
2010 |     """Delete suggestion by ID (admin only)
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2030:37
     |
2029 | @app.get("/api/suggestions/stats/summary")
2030 | async def get_suggestion_statistics(username: str = Depends(get_current_user)):
     |                                     ^^^^^^^^
2031 |     """Get suggestion statistics for admin dashboard
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2047:57
     |
2045 | @app.get("/api/entities/{entity_id}/enrich")
2046 | async def enrich_entity(
2047 |     entity_id: str, force_refresh: bool = Query(False), username: str = Depends(get_current_user)
     |                                                         ^^^^^^^^
2048 | ):
2049 |     """Trigger web search enrichment for an entity.
     |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
    --> server/app.py:2135:9
     |
2134 |     except Exception as e:
2135 |         raise HTTPException(status_code=500, detail=f"Error enriching entity: {e!s}")
     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2139:42
     |
2138 | @app.get("/api/entities/{entity_id}/enrichment")
2139 | async def get_enrichment(entity_id: str, username: str = Depends(get_current_user)):
     |                                          ^^^^^^^^
2140 |     """Get cached enrichment data for an entity.
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2197:5
     |
2195 |     entity_ids: list[str],
2196 |     max_concurrent: int = Query(3, ge=1, le=5),
2197 |     username: str = Depends(get_current_user),
     |     ^^^^^^^^
2198 | ):
2199 |     """Enrich multiple entities in a single request.
     |

PLW2901 `for` loop variable `entity_id` overwritten by assignment target
    --> server/app.py:2237:13
     |
2235 |             if not matching:
2236 |                 raise HTTPException(status_code=404, detail=f"Entity '{entity_id}' not found")
2237 |             entity_id, entity_data = matching[0]
     |             ^^^^^^^^^
2238 |
2239 |         entities.append({"id": entity_id, "name": entity_data.get("name", entity_id)})
     |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
    --> server/app.py:2251:9
     |
2250 |     except Exception as e:
2251 |         raise HTTPException(status_code=500, detail=f"Error during batch enrichment: {e!s}")
     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2255:37
     |
2254 | @app.get("/api/enrichment/stats")
2255 | async def get_enrichment_statistics(username: str = Depends(get_current_user)):
     |                                     ^^^^^^^^
2256 |     """Get enrichment cache statistics.
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2333:27
     |
2332 | @app.get("/api/flights/all")
2333 | async def get_all_flights(username: str = Depends(get_current_user)):
     |                           ^^^^^^^^
2334 |     """Get all 1,167 flights grouped by route for map visualization.
     |

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:2349:14
     |
2347 |             return {"routes": [], "total_flights": 0, "error": "Flight data not found"}
2348 |
2349 |         with open(flight_data_path) as f:
     |              ^^^^
2350 |             flight_data = json.load(f)
     |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:2357:14
     |
2355 |             return {"routes": [], "total_flights": 0, "error": "Location database not found"}
2356 |
2357 |         with open(locations_path) as f:
     |              ^^^^
2358 |             locations_db = json.load(f)
2359 |             airports = locations_db.get("airports", {})
     |
help: Replace with `Path.open()`

B007 Loop control variable `route_key` not used within loop body
    --> server/app.py:2414:13
     |
2412 |         # Convert route_map to array with frequency
2413 |         routes = []
2414 |         for route_key, route_data in route_map.items():
     |             ^^^^^^^^^
2415 |             routes.append(
2416 |                 {
     |
help: Rename unused `route_key` to `_route_key`

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:2447:9
     |
2446 |     except Exception as e:
2447 |         import traceback
     |         ^^^^^^^^^^^^^^^^
2448 |
2449 |         traceback.print_exc()
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2466:5
     |
2464 |     limit: int = Query(20, ge=1, le=100, description="Results per page"),
2465 |     offset: int = Query(0, ge=0, description="Pagination offset"),
2466 |     username: str = Depends(get_current_user),
     |     ^^^^^^^^
2467 | ):
2468 |     """Search documents with filters.
     |

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:2481:14
     |
2479 |             return {"documents": [], "total": 0, "error": "Document index not found"}
2480 |
2481 |         with open(doc_index_path) as f:
     |              ^^^^
2482 |             doc_data = json.load(f)
     |
help: Replace with `Path.open()`

ARG001 Unused function argument: `username`
    --> server/app.py:2562:37
     |
2561 | @app.get("/api/documents/{doc_id}")
2562 | async def get_document(doc_id: str, username: str = Depends(get_current_user)):
     |                                     ^^^^^^^^
2563 |     """Get full document content by ID.
     |

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:2575:14
     |
2573 |             raise HTTPException(status_code=404, detail="Document index not found")
2574 |
2575 |         with open(doc_index_path) as f:
     |              ^^^^
2576 |             doc_data = json.load(f)
     |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
    --> server/app.py:2593:26
     |
2591 |             if md_path.exists() and md_path.suffix == ".md":
2592 |                 try:
2593 |                     with open(md_path) as f:
     |                          ^^^^
2594 |                         content = f.read()
2595 |                 except Exception as e:
     |
help: Replace with `Path.open()`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
    --> server/app.py:2604:9
     |
2602 |     except Exception as e:
2603 |         logger.error(f"Error fetching document {doc_id}: {e}")
2604 |         raise HTTPException(status_code=500, detail=str(e))
     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2609:50
     |
2607 | @app.get("/api/git/recent-commits")
2608 | async def get_recent_commits(
2609 |     limit: int = Query(default=10, ge=1, le=50), username: str = Depends(get_current_user)
     |                                                  ^^^^^^^^
2610 | ):
2611 |     """Get recent git commits with semantic commit parsing.
     |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
    --> server/app.py:2694:9
     |
2693 |     except subprocess.TimeoutExpired:
2694 |         raise HTTPException(status_code=500, detail="Git command timed out")
     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2695 |     except Exception as e:
2696 |         raise HTTPException(status_code=500, detail=f"Error fetching commits: {e!s}")
     |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
    --> server/app.py:2696:9
     |
2694 |         raise HTTPException(status_code=500, detail="Git command timed out")
2695 |     except Exception as e:
2696 |         raise HTTPException(status_code=500, detail=f"Error fetching commits: {e!s}")
     |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
     |

ARG001 Unused function argument: `username`
    --> server/app.py:2705:41
     |
2704 | @app.get("/api/sse/updates")
2705 | async def sse_updates(request: Request, username: str = Depends(get_current_user)):
     |                                         ^^^^^^^^
2706 |     """
2707 |     Server-Sent Events endpoint for hot-reload functionality
     |

E402 Module level import not at top of file
    --> server/app.py:2808:1
     |
2807 | # Import and register new API routes
2808 | import api_routes
     | ^^^^^^^^^^^^^^^^^
     |

ARG001 Unused function argument: `full_path`
    --> server/app.py:2949:21
     |
2947 | # Catch-all for React Router (SPA routing) - handles /news, /entities, etc.
2948 | @app.get("/{full_path:path}")
2949 | async def serve_spa(full_path: str):
     |                     ^^^^^^^^^
2950 |     """Serve React SPA for all non-API, non-static routes"""
2951 |     # This will handle routes like /news, /entities, /documents, etc.
     |

PLC0415 `import` should be at the top-level of a file
    --> server/app.py:2965:5
     |
2963 | def main():
2964 |     """Run server"""
2965 |     import sys
     |     ^^^^^^^^^^
2966 |
2967 |     port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000
     |

PTH103 `os.makedirs()` should be replaced by `Path.mkdir(parents=True)`
   --> server/create_favicon.py:136:5
    |
134 |     """Generate all required favicon sizes"""
135 |
136 |     os.makedirs(output_dir, exist_ok=True)
    |     ^^^^^^^^^^^
137 |
138 |     # Size specifications
    |
help: Replace with `Path(...).mkdir(parents=True)`

PTH118 `os.path.join()` should be replaced by `Path` with `/` operator
   --> server/create_favicon.py:156:20
    |
154 |         print(f"  Creating {filename} ({size}x{size})")
155 |         img = create_island_plane_favicon(size)
156 |         filepath = os.path.join(output_dir, filename)
    |                    ^^^^^^^^^^^^
157 |         img.save(filepath, "PNG")
    |

PTH118 `os.path.join()` should be replaced by `Path` with `/` operator
   --> server/create_favicon.py:165:16
    |
163 |     # Create multi-resolution favicon.ico
164 |     print("  Creating favicon.ico (multi-resolution)")
165 |     ico_path = os.path.join(output_dir, "favicon.ico")
    |                ^^^^^^^^^^^^
166 |
167 |     # Sort by size and extract images
    |

PTH118 `os.path.join()` should be replaced by `Path` with `/` operator
   --> server/create_favicon.py:202:21
    |
200 | """
201 |
202 |     manifest_path = os.path.join(output_dir, "site.webmanifest")
    |                     ^^^^^^^^^^^^
203 |     with open(manifest_path, "w") as f:
204 |         f.write(manifest_content)
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/create_favicon.py:203:10
    |
202 |     manifest_path = os.path.join(output_dir, "site.webmanifest")
203 |     with open(manifest_path, "w") as f:
    |          ^^^^
204 |         f.write(manifest_content)
    |
help: Replace with `Path.open()`

PTH118 `os.path.join()` should be replaced by `Path` with `/` operator
   --> server/create_favicon.py:212:18
    |
210 | if __name__ == "__main__":
211 |     # Output directory
212 |     output_dir = os.path.join(os.path.dirname(__file__), "web")
    |                  ^^^^^^^^^^^^
213 |
214 |     print("=" * 60)
    |

PTH120 `os.path.dirname()` should be replaced by `Path.parent`
   --> server/create_favicon.py:212:31
    |
210 | if __name__ == "__main__":
211 |     # Output directory
212 |     output_dir = os.path.join(os.path.dirname(__file__), "web")
    |                               ^^^^^^^^^^^^^^^
213 |
214 |     print("=" * 60)
    |
help: Replace with `Path(...).parent`

PLC0415 `import` should be at the top-level of a file
   --> server/models/entity.py:174:9
    |
172 |         if v is None:
173 |             return None
174 |         import re
    |         ^^^^^^^^^
175 |
176 |         return re.sub(r"\s+", " ", v.strip())
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/models/entity.py:299:13
    |
297 |             return v
298 |         except ValueError:
299 |             raise ValueError(f"Invalid timestamp format: {v}. Expected ISO format.")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
300 |
301 |     model_config = ConfigDict(
    |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/flight.py:160:18
    |
158 |         # Parse to validate it's a real date and normalize
159 |         try:
160 |             dt = datetime.strptime(v, "%m/%d/%Y")
    |                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
161 |             # Return normalized zero-padded format
162 |             return dt.strftime("%m/%d/%Y")
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/models/flight.py:164:13
    |
162 |             return dt.strftime("%m/%d/%Y")
163 |         except ValueError as e:
164 |             raise ValueError(f"Invalid date value: {v} - {e!s}")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
165 |
166 |         return v
    |

PLC0415 `import` should be at the top-level of a file
   --> server/models/network.py:336:9
    |
334 |             }
335 |         """
336 |         from collections import defaultdict
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
337 |
338 |         adj: dict[str, list[str]] = defaultdict(list)
    |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/news_article.py:132:13
    |
130 |         """Ensure date is in YYYY-MM-DD format"""
131 |         try:
132 |             datetime.strptime(v, "%Y-%m-%d")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
133 |             return v
134 |         except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/models/news_article.py:135:13
    |
133 |             return v
134 |         except ValueError:
135 |             raise ValueError("Date must be in YYYY-MM-DD format")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
136 |
137 |     class Config:
    |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
   --> server/models/news_article.py:138:25
    |
137 |     class Config:
138 |         json_encoders = {datetime: lambda v: v.isoformat() if v else None}
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
139 |         use_enum_values = True
    |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
   --> server/models/news_article.py:160:25
    |
159 |     class Config:
160 |         json_encoders = {datetime: lambda v: v.isoformat() if v else None}
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/news_article.py:205:13
    |
203 |             return v
204 |         try:
205 |             datetime.strptime(v, "%Y-%m-%d")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
206 |             return v
207 |         except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/models/news_article.py:208:13
    |
206 |             return v
207 |         except ValueError:
208 |             raise ValueError("Date must be in YYYY-MM-DD format")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/news_article.py:240:13
    |
238 |         """Ensure date is in YYYY-MM-DD format"""
239 |         try:
240 |             datetime.strptime(v, "%Y-%m-%d")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
241 |             return v
242 |         except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/models/news_article.py:243:13
    |
241 |             return v
242 |         except ValueError:
243 |             raise ValueError("Date must be in YYYY-MM-DD format")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
   --> server/models/suggested_source.py:127:25
    |
126 |     class Config:
127 |         json_encoders = {datetime: lambda v: v.isoformat() if v else None}
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
128 |         use_enum_values = True
    |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:166:13
    |
164 |         # Try full date (YYYY-MM-DD)
165 |         try:
166 |             datetime.strptime(v, "%Y-%m-%d")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
167 |             return v
168 |         except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:173:13
    |
171 |         # Try year-month (YYYY-MM)
172 |         try:
173 |             datetime.strptime(v, "%Y-%m")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
174 |             return v
175 |         except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:180:13
    |
178 |         # Try year only (YYYY)
179 |         try:
180 |             datetime.strptime(v, "%Y")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^
181 |             return v
182 |         except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

PLW2901 `for` loop variable `item` overwritten by assignment target
   --> server/models/timeline.py:236:13
    |
234 |         unique = []
235 |         for item in v:
236 |             item = item.strip()
    |             ^^^^
237 |             if item and item not in seen:
238 |                 unique.append(item)
    |

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:306:24
    |
304 |             # Full date (YYYY-MM-DD)
305 |             try:
306 |                 return datetime.strptime(date_str, "%Y-%m-%d")
    |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
307 |             except ValueError:
308 |                 pass
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:312:22
    |
310 |             # Year-month (YYYY-MM) - sort to middle of month
311 |             try:
312 |                 dt = datetime.strptime(date_str, "%Y-%m")
    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
313 |                 return dt.replace(day=15)
314 |             except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:319:22
    |
317 |             # Year only (YYYY) - sort to middle of year
318 |             try:
319 |                 dt = datetime.strptime(date_str, "%Y")
    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
320 |                 return dt.replace(month=7, day=1)
321 |             except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

DTZ001 `datetime.datetime()` called without a `tzinfo` argument
   --> server/models/timeline.py:325:20
    |
324 |             # Fallback: treat as very old date
325 |             return datetime(1900, 1, 1)
    |                    ^^^^^^^^^^^^^^^^^^^^
326 |
327 |         return sorted(v, key=lambda event: parse_date_for_sorting(event.date))
    |
help: Pass a `datetime.timezone` object to the `tzinfo` parameter

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:414:13
    |
412 |         try:
413 |             # Try full date
414 |             datetime.strptime(v, "%Y-%m-%d")
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
415 |             return v
416 |         except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:419:17
    |
417 |             try:
418 |                 # Try year-month
419 |                 datetime.strptime(v, "%Y-%m")
    |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
420 |                 return v
421 |             except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

DTZ007 Naive datetime constructed using `datetime.datetime.strptime()` without %z
   --> server/models/timeline.py:424:21
    |
422 |                 try:
423 |                     # Try year only
424 |                     datetime.strptime(v, "%Y")
    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
425 |                     return v
426 |                 except ValueError:
    |
help: Call `.replace(tzinfo=<timezone>)` or `.astimezone()` to convert to an aware datetime

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/models/timeline.py:427:21
    |
425 |                       return v
426 |                   except ValueError:
427 | /                     raise ValueError(
428 | |                         f"Date must be in ISO format (YYYY-MM-DD, YYYY-MM, or YYYY), got: {v}"
429 | |                     )
    | |_____________________^
430 |
431 |       model_config = ConfigDict(
    |

PLW2901 `for` loop variable `tag` overwritten by assignment target
   --> server/models/validators.py:169:9
    |
168 |     for tag in tags:
169 |         tag = tag.strip().lower()
    |         ^^^
170 |         if not tag:
171 |             raise ValueError("Empty tags not allowed")
    |

PLW0603 Using the global statement to update `openrouter_client` is discouraged
  --> server/routes/chat_enhanced.py:32:12
   |
30 | def get_openrouter_client():
31 |     """Initialize OpenRouter client (lazy loading)"""
32 |     global openrouter_client
   |            ^^^^^^^^^^^^^^^^^
33 |     if openrouter_client is None:
34 |         api_key = os.getenv("OPENROUTER_API_KEY")
   |

PLC0415 `import` should be at the top-level of a file
  --> server/routes/chat_enhanced.py:44:5
   |
42 | # Import get_current_user at runtime to avoid circular imports
43 | def get_auth_dependency():
44 |     from app import get_current_user
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
45 |
46 |     return get_current_user
   |

PLW0603 Using the global statement to update `_entity_stats` is discouraged
  --> server/routes/chat_enhanced.py:83:12
   |
81 | def load_entity_stats() -> dict:
82 |     """Load entity statistics (cached)"""
83 |     global _entity_stats
   |            ^^^^^^^^^^^^^
84 |     if _entity_stats is None:
85 |         stats_path = METADATA_DIR / "entity_statistics.json"
   |

PLW0603 Using the global statement to update `_entity_stats` is discouraged
  --> server/routes/chat_enhanced.py:83:12
   |
81 | def load_entity_stats() -> dict:
82 |     """Load entity statistics (cached)"""
83 |     global _entity_stats
   |            ^^^^^^^^^^^^^
84 |     if _entity_stats is None:
85 |         stats_path = METADATA_DIR / "entity_statistics.json"
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/chat_enhanced.py:87:18
   |
85 |         stats_path = METADATA_DIR / "entity_statistics.json"
86 |         if stats_path.exists():
87 |             with open(stats_path) as f:
   |                  ^^^^
88 |                 data = json.load(f)
89 |                 _entity_stats = data.get("statistics", {})
   |
help: Replace with `Path.open()`

PLW0603 Using the global statement to update `_network_data` is discouraged
  --> server/routes/chat_enhanced.py:97:12
   |
95 | def load_network_data() -> dict:
96 |     """Load network data (cached)"""
97 |     global _network_data
   |            ^^^^^^^^^^^^^
98 |     if _network_data is None:
99 |         network_path = METADATA_DIR / "entity_network.json"
   |

PLW0603 Using the global statement to update `_network_data` is discouraged
  --> server/routes/chat_enhanced.py:97:12
   |
95 | def load_network_data() -> dict:
96 |     """Load network data (cached)"""
97 |     global _network_data
   |            ^^^^^^^^^^^^^
98 |     if _network_data is None:
99 |         network_path = METADATA_DIR / "entity_network.json"
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/chat_enhanced.py:101:18
    |
 99 |         network_path = METADATA_DIR / "entity_network.json"
100 |         if network_path.exists():
101 |             with open(network_path) as f:
    |                  ^^^^
102 |                 _network_data = json.load(f)
103 |         else:
    |
help: Replace with `Path.open()`

PLW0603 Using the global statement to update `_classifications` is discouraged
   --> server/routes/chat_enhanced.py:110:12
    |
108 | def load_classifications() -> dict:
109 |     """Load document classifications (cached)"""
110 |     global _classifications
    |            ^^^^^^^^^^^^^^^^
111 |     if _classifications is None:
112 |         class_path = METADATA_DIR / "document_classifications.json"
    |

PLW0603 Using the global statement to update `_classifications` is discouraged
   --> server/routes/chat_enhanced.py:110:12
    |
108 | def load_classifications() -> dict:
109 |     """Load document classifications (cached)"""
110 |     global _classifications
    |            ^^^^^^^^^^^^^^^^
111 |     if _classifications is None:
112 |         class_path = METADATA_DIR / "document_classifications.json"
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/chat_enhanced.py:114:18
    |
112 |         class_path = METADATA_DIR / "document_classifications.json"
113 |         if class_path.exists():
114 |             with open(class_path) as f:
    |                  ^^^^
115 |                 data = json.load(f)
116 |                 _classifications = data.get("results", {})
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/chat_enhanced.py:128:18
    |
126 |     if unified_index_path.exists():
127 |         try:
128 |             with open(unified_index_path) as f:
    |                  ^^^^
129 |                 unified_data = json.load(f)
130 |                 total = unified_data.get("total_documents", total)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/chat_enhanced.py:141:18
    |
139 |     if flight_data_path.exists():
140 |         try:
141 |             with open(flight_data_path) as f:
    |                  ^^^^
142 |                 flight_data = json.load(f)
143 |                 return len(flight_data.get("flights", []))
    |
help: Replace with `Path.open()`

ARG001 Unused function argument: `username`
   --> server/routes/chat_enhanced.py:388:47
    |
387 | @router.post("/enhanced", response_model=ChatResponse)
388 | async def chat_enhanced(request: ChatRequest, username: str = Depends(get_auth_dependency())):
    |                                               ^^^^^^^^
389 |     """
390 |     Enhanced AI chatbot with complete site awareness
    |

PLC0415 `import` should be at the top-level of a file
  --> server/routes/flights.py:23:5
   |
21 | # Import get_current_user at runtime to avoid circular imports
22 | def get_auth_dependency():
23 |     from app import get_current_user
   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
24 |
25 |     return get_current_user
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/flights.py:45:14
   |
43 |             return {"routes": [], "total_flights": 0, "error": "Flight data not found"}
44 |
45 |         with open(flight_data_path) as f:
   |              ^^^^
46 |             flight_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/flights.py:53:14
   |
51 |             return {"routes": [], "total_flights": 0, "error": "Location database not found"}
52 |
53 |         with open(locations_path) as f:
   |              ^^^^
54 |             locations_db = json.load(f)
55 |             airports = locations_db.get("airports", {})
   |
help: Replace with `Path.open()`

B007 Loop control variable `route_key` not used within loop body
   --> server/routes/flights.py:110:13
    |
108 |         # Convert route_map to array with frequency
109 |         routes = []
110 |         for route_key, route_data in route_map.items():
    |             ^^^^^^^^^
111 |             routes.append(
112 |                 {
    |
help: Rename unused `route_key` to `_route_key`

PLC0415 `import` should be at the top-level of a file
   --> server/routes/flights.py:140:9
    |
139 |     except Exception as e:
140 |         import traceback
    |         ^^^^^^^^^^^^^^^^
141 |
142 |         traceback.print_exc()
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/flights.py:183:14
    |
181 |             }
182 |
183 |         with open(flight_data_path) as f:
    |              ^^^^
184 |             flight_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/flights.py:196:14
    |
194 |             }
195 |
196 |         with open(locations_path) as f:
    |              ^^^^
197 |             locations_db = json.load(f)
198 |             airports = locations_db.get("airports", {})
    |
help: Replace with `Path.open()`

PLR1704 Redefining argument with the local name `passenger`
   --> server/routes/flights.py:272:17
    |
270 |             unique_routes.add(route_key)
271 |
272 |             for passenger in flight["passengers"]:
    |                 ^^^^^^^^^
273 |                 passenger_counts[passenger] = passenger_counts.get(passenger, 0) + 1
    |

PLC0415 `import` should be at the top-level of a file
   --> server/routes/flights.py:303:9
    |
302 |     except Exception as e:
303 |         import traceback
    |         ^^^^^^^^^^^^^^^^
304 |
305 |         traceback.print_exc()
    |

PLW0603 Using the global statement to update `_news_service` is discouraged
  --> server/routes/news.py:46:12
   |
44 | def get_news_service() -> NewsService:
45 |     """Get news service instance (lazy loading)."""
46 |     global _news_service
   |            ^^^^^^^^^^^^^
47 |
48 |     if _news_service is None:
   |

PLW0603 Using the global statement to update `_entity_service` is discouraged
  --> server/routes/news.py:56:12
   |
54 | def get_entity_service() -> EntityService:
55 |     """Get entity service instance (lazy loading)."""
56 |     global _entity_service
   |            ^^^^^^^^^^^^^^^
57 |
58 |     if _entity_service is None:
   |

PLW0603 Using the global statement to update `_search_service` is discouraged
  --> server/routes/news.py:66:12
   |
64 | def get_search_service() -> NewsSemanticSearch:
65 |     """Get semantic search service instance (lazy loading)."""
66 |     global _search_service
   |            ^^^^^^^^^^^^^^^
67 |
68 |     if _search_service is None:
   |

PLC0415 `import` should be at the top-level of a file
   --> server/routes/news.py:150:13
    |
148 |         entity_query = entity
149 |         if entity:
150 |             import logging
    |             ^^^^^^^^^^^^^^
151 |
152 |             logger = logging.getLogger(__name__)
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:228:9
    |
227 |     except Exception as e:
228 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:257:9
    |
255 |         raise
256 |     except Exception as e:
257 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:307:9
    |
306 |     except ValueError as e:
307 |         raise HTTPException(status_code=400, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
308 |     except Exception as e:
309 |         raise HTTPException(status_code=500, detail=str(e))
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:309:9
    |
307 |         raise HTTPException(status_code=400, detail=str(e))
308 |     except Exception as e:
309 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:397:9
    |
396 |     except Exception as e:
397 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:450:9
    |
449 |     except Exception as e:
450 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:481:9
    |
480 |     except Exception as e:
481 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:516:9
    |
515 |     except Exception as e:
516 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/news.py:551:9
    |
550 |     except Exception as e:
551 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

PLW0603 Using the global statement to update `_chroma_client` is discouraged
  --> server/routes/rag.py:41:12
   |
39 | def get_chroma_collection():
40 |     """Get or create ChromaDB collection (lazy loading)."""
41 |     global _chroma_client, _collection
   |            ^^^^^^^^^^^^^^
42 |
43 |     if _collection is None:
   |

PLW0603 Using the global statement to update `_collection` is discouraged
  --> server/routes/rag.py:41:28
   |
39 | def get_chroma_collection():
40 |     """Get or create ChromaDB collection (lazy loading)."""
41 |     global _chroma_client, _collection
   |                            ^^^^^^^^^^^
42 |
43 |     if _collection is None:
   |

E722 Do not use bare `except`
  --> server/routes/rag.py:49:9
   |
47 |         try:
48 |             _collection = _chroma_client.get_collection(name=COLLECTION_NAME)
49 |         except:
   |         ^^^^^^
50 |             raise HTTPException(
51 |                 status_code=503,
   |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> server/routes/rag.py:50:13
   |
48 |               _collection = _chroma_client.get_collection(name=COLLECTION_NAME)
49 |           except:
50 | /             raise HTTPException(
51 | |                 status_code=503,
52 | |                 detail="Vector store not initialized. Run build_vector_store.py first.",
53 | |             )
   | |_____________^
54 |
55 |       return _collection
   |

PLW0603 Using the global statement to update `_embedding_model` is discouraged
  --> server/routes/rag.py:60:12
   |
58 | def get_embedding_model():
59 |     """Get embedding model (lazy loading)."""
60 |     global _embedding_model
   |            ^^^^^^^^^^^^^^^^
61 |
62 |     if _embedding_model is None:
   |

PLW0603 Using the global statement to update `_entity_doc_index` is discouraged
  --> server/routes/rag.py:70:12
   |
68 | def get_entity_doc_index():
69 |     """Get entity-document index (lazy loading)."""
70 |     global _entity_doc_index
   |            ^^^^^^^^^^^^^^^^^
71 |
72 |     if _entity_doc_index is None:
   |

PLW0603 Using the global statement to update `_entity_doc_index` is discouraged
  --> server/routes/rag.py:70:12
   |
68 | def get_entity_doc_index():
69 |     """Get entity-document index (lazy loading)."""
70 |     global _entity_doc_index
   |            ^^^^^^^^^^^^^^^^^
71 |
72 |     if _entity_doc_index is None:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/rag.py:74:18
   |
72 |     if _entity_doc_index is None:
73 |         if ENTITY_DOC_INDEX_PATH.exists():
74 |             with open(ENTITY_DOC_INDEX_PATH) as f:
   |                  ^^^^
75 |                 _entity_doc_index = json.load(f)
76 |         else:
   |
help: Replace with `Path.open()`

PLW0603 Using the global statement to update `_entity_network` is discouraged
  --> server/routes/rag.py:84:12
   |
82 | def get_entity_network():
83 |     """Get entity network (lazy loading)."""
84 |     global _entity_network
   |            ^^^^^^^^^^^^^^^
85 |
86 |     if _entity_network is None:
   |

PLW0603 Using the global statement to update `_entity_network` is discouraged
  --> server/routes/rag.py:84:12
   |
82 | def get_entity_network():
83 |     """Get entity network (lazy loading)."""
84 |     global _entity_network
   |            ^^^^^^^^^^^^^^^
85 |
86 |     if _entity_network is None:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/rag.py:88:18
   |
86 |     if _entity_network is None:
87 |         if ENTITY_NETWORK_PATH.exists():
88 |             with open(ENTITY_NETWORK_PATH) as f:
   |                  ^^^^
89 |                 _entity_network = json.load(f)
90 |         else:
   |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> server/routes/rag.py:160:5
    |
158 |         doc_type: Filter by document type (e.g., "news_article")
159 |     """
160 |     import time
    |     ^^^^^^^^^^^
161 |
162 |     start_time = time.time()
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:221:9
    |
220 |     except Exception as e:
221 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

PLC0415 `import` should be at the top-level of a file
   --> server/routes/rag.py:250:13
    |
248 |         else:
249 |             # Try alias resolution via ENTITIES_INDEX.json
250 |             from pathlib import Path
    |             ^^^^^^^^^^^^^^^^^^^^^^^^
251 |
252 |             project_root = Path(__file__).parent.parent.parent
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/rag.py:257:22
    |
255 |             canonical_name = None
256 |             if entities_index_path.exists():
257 |                 with open(entities_index_path) as f:
    |                      ^^^^
258 |                     data = json.load(f)
259 |                     entities = data.get("entities", [])
    |
help: Replace with `Path.open()`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:332:9
    |
330 |         raise
331 |     except Exception as e:
332 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:402:9
    |
400 |         raise
401 |     except Exception as e:
402 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:446:9
    |
445 |     except Exception as e:
446 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:500:9
    |
498 |         raise
499 |     except Exception as e:
500 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

PLC0415 `import` should be at the top-level of a file
   --> server/routes/rag.py:552:5
    |
550 |     metadata and use $gte/$lte operators.
551 |     """
552 |     import time
    |     ^^^^^^^^^^^
553 |
554 |     start_time = time.time()
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:631:9
    |
630 |     except Exception as e:
631 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:721:9
    |
719 |         raise
720 |     except Exception as e:
721 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/rag.py:755:9
    |
754 |     except Exception as e:
755 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

PLW0603 Using the global statement to update `_chroma_client` is discouraged
  --> server/routes/search.py:45:12
   |
43 | def get_chroma_collection():
44 |     """Get or create ChromaDB collection (lazy loading)."""
45 |     global _chroma_client, _collection
   |            ^^^^^^^^^^^^^^
46 |
47 |     if _collection is None:
   |

PLW0603 Using the global statement to update `_collection` is discouraged
  --> server/routes/search.py:45:28
   |
43 | def get_chroma_collection():
44 |     """Get or create ChromaDB collection (lazy loading)."""
45 |     global _chroma_client, _collection
   |                            ^^^^^^^^^^^
46 |
47 |     if _collection is None:
   |

E722 Do not use bare `except`
  --> server/routes/search.py:53:9
   |
51 |         try:
52 |             _collection = _chroma_client.get_collection(name=COLLECTION_NAME)
53 |         except:
   |         ^^^^^^
54 |             raise HTTPException(
55 |                 status_code=503,
   |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> server/routes/search.py:54:13
   |
52 |               _collection = _chroma_client.get_collection(name=COLLECTION_NAME)
53 |           except:
54 | /             raise HTTPException(
55 | |                 status_code=503,
56 | |                 detail="Vector store not initialized. Run build_vector_store.py first.",
57 | |             )
   | |_____________^
58 |
59 |       return _collection
   |

PLW0603 Using the global statement to update `_embedding_model` is discouraged
  --> server/routes/search.py:64:12
   |
62 | def get_embedding_model():
63 |     """Get embedding model (lazy loading)."""
64 |     global _embedding_model
   |            ^^^^^^^^^^^^^^^^
65 |
66 |     if _embedding_model is None:
   |

PLW0603 Using the global statement to update `_entity_index` is discouraged
  --> server/routes/search.py:74:12
   |
72 | def get_entity_index():
73 |     """Get entity index (lazy loading)."""
74 |     global _entity_index
   |            ^^^^^^^^^^^^^
75 |
76 |     if _entity_index is None:
   |

PLW0603 Using the global statement to update `_entity_index` is discouraged
  --> server/routes/search.py:74:12
   |
72 | def get_entity_index():
73 |     """Get entity index (lazy loading)."""
74 |     global _entity_index
   |            ^^^^^^^^^^^^^
75 |
76 |     if _entity_index is None:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/search.py:78:18
   |
76 |     if _entity_index is None:
77 |         if ENTITY_INDEX_PATH.exists():
78 |             with open(ENTITY_INDEX_PATH) as f:
   |                  ^^^^
79 |                 _entity_index = json.load(f)
80 |         else:
   |
help: Replace with `Path.open()`

PLW0603 Using the global statement to update `_search_analytics` is discouraged
  --> server/routes/search.py:88:12
   |
86 | def load_search_analytics():
87 |     """Get or initialize search analytics (lazy loading)."""
88 |     global _search_analytics
   |            ^^^^^^^^^^^^^^^^^
89 |
90 |     if _search_analytics is None:
   |

PLW0603 Using the global statement to update `_search_analytics` is discouraged
  --> server/routes/search.py:88:12
   |
86 | def load_search_analytics():
87 |     """Get or initialize search analytics (lazy loading)."""
88 |     global _search_analytics
   |            ^^^^^^^^^^^^^^^^^
89 |
90 |     if _search_analytics is None:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/search.py:92:18
   |
90 |     if _search_analytics is None:
91 |         if SEARCH_ANALYTICS_PATH.exists():
92 |             with open(SEARCH_ANALYTICS_PATH) as f:
   |                  ^^^^
93 |                 _search_analytics = json.load(f)
94 |         else:
   |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/search.py:99:33
    |
 97 |                 "popular_queries": {},
 98 |                 "recent_searches": [],
 99 |                 "last_updated": datetime.utcnow().isoformat(),
    |                                 ^^^^^^^^^^^^^^^^^
100 |             }
    |
help: Use `datetime.datetime.now(tz=...)` instead

PLW0602 Using global for `_search_analytics` but no assignment is done
   --> server/routes/search.py:107:12
    |
105 | def save_search_analytics():
106 |     """Save search analytics to disk."""
107 |     global _search_analytics
    |            ^^^^^^^^^^^^^^^^^
108 |
109 |     if _search_analytics:
    |

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/search.py:111:45
    |
109 |     if _search_analytics:
110 |         SEARCH_ANALYTICS_PATH.parent.mkdir(parents=True, exist_ok=True)
111 |         _search_analytics["last_updated"] = datetime.utcnow().isoformat()
    |                                             ^^^^^^^^^^^^^^^^^
112 |
113 |         with open(SEARCH_ANALYTICS_PATH, "w") as f:
    |
help: Use `datetime.datetime.now(tz=...)` instead

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/search.py:113:14
    |
111 |         _search_analytics["last_updated"] = datetime.utcnow().isoformat()
112 |
113 |         with open(SEARCH_ANALYTICS_PATH, "w") as f:
    |              ^^^^
114 |             json.dump(_search_analytics, f, indent=2)
    |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/search.py:288:46
    |
286 |         analytics["popular_queries"][query] = analytics["popular_queries"].get(query, 0) + 1
287 |         analytics["recent_searches"].insert(
288 |             0, {"query": query, "timestamp": datetime.utcnow().isoformat(), "fields": fields}
    |                                              ^^^^^^^^^^^^^^^^^
289 |         )
290 |         analytics["recent_searches"] = analytics["recent_searches"][:100]  # Keep last 100
    |
help: Use `datetime.datetime.now(tz=...)` instead

B007 Loop control variable `result` not used within loop body
   --> server/routes/search.py:307:17
    |
305 |             all_results.extend(entity_results)
306 |
307 |             for result in entity_results:
    |                 ^^^^^^
308 |                 facets["types"]["entity"] = facets["types"].get("entity", 0) + 1
    |
help: Rename unused `result` to `_result`

B007 Loop control variable `result` not used within loop body
   --> server/routes/search.py:333:17
    |
331 |             all_results.extend(news_results)
332 |
333 |             for result in news_results:
    |                 ^^^^^^
334 |                 facets["types"]["news"] = facets["types"].get("news", 0) + 1
    |
help: Rename unused `result` to `_result`

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/search.py:357:9
    |
356 |     except Exception as e:
357 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

ARG001 Unused function argument: `boolean_terms`
   --> server/routes/search.py:361:17
    |
360 | async def search_entities(
361 |     query: str, boolean_terms: dict[str, list[str]], fuzzy: bool, min_similarity: float
    |                 ^^^^^^^^^^^^^
362 | ) -> list[SearchResult]:
363 |     """Search entities with fuzzy matching."""
    |

ARG001 Unused function argument: `boolean_terms`
   --> server/routes/search.py:417:5
    |
415 | async def search_documents(
416 |     query: str,
417 |     boolean_terms: dict[str, list[str]],
    |     ^^^^^^^^^^^^^
418 |     min_similarity: float,
419 |     doc_type: Optional[str],
    |

ARG001 Unused function argument: `date_start`
   --> server/routes/search.py:421:5
    |
419 |     doc_type: Optional[str],
420 |     source: Optional[str],
421 |     date_start: Optional[str],
    |     ^^^^^^^^^^
422 |     date_end: Optional[str],
423 | ) -> list[SearchResult]:
    |

ARG001 Unused function argument: `date_end`
   --> server/routes/search.py:422:5
    |
420 |     source: Optional[str],
421 |     date_start: Optional[str],
422 |     date_end: Optional[str],
    |     ^^^^^^^^
423 | ) -> list[SearchResult]:
424 |     """Search documents via vector store."""
    |

ARG001 Unused function argument: `boolean_terms`
   --> server/routes/search.py:475:5
    |
473 | async def search_news(
474 |     query: str,
475 |     boolean_terms: dict[str, list[str]],
    |     ^^^^^^^^^^^^^
476 |     min_similarity: float,
477 |     date_start: Optional[str],
    |

ARG001 Unused function argument: `date_start`
   --> server/routes/search.py:477:5
    |
475 |     boolean_terms: dict[str, list[str]],
476 |     min_similarity: float,
477 |     date_start: Optional[str],
    |     ^^^^^^^^^^
478 |     date_end: Optional[str],
479 | ) -> list[SearchResult]:
    |

ARG001 Unused function argument: `date_end`
   --> server/routes/search.py:478:5
    |
476 |     min_similarity: float,
477 |     date_start: Optional[str],
478 |     date_end: Optional[str],
    |     ^^^^^^^^
479 | ) -> list[SearchResult]:
480 |     """Search news articles."""
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/search.py:631:9
    |
630 |     except Exception as e:
631 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/search.py:670:9
    |
669 |     except Exception as e:
670 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

PLW0602 Using global for `_search_analytics` but no assignment is done
   --> server/routes/search.py:685:16
    |
683 |     """
684 |     try:
685 |         global _search_analytics
    |                ^^^^^^^^^^^^^^^^^
686 |
687 |         analytics = load_search_analytics()
    |

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/search.py:689:37
    |
687 |         analytics = load_search_analytics()
688 |         analytics["recent_searches"] = []
689 |         analytics["last_updated"] = datetime.utcnow().isoformat()
    |                                     ^^^^^^^^^^^^^^^^^
690 |
691 |         save_search_analytics()
    |
help: Use `datetime.datetime.now(tz=...)` instead

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/search.py:696:9
    |
695 |     except Exception as e:
696 |         raise HTTPException(status_code=500, detail=str(e))
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

PLW0603 Using the global statement to update `_entity_stats` is discouraged
  --> server/routes/stats.py:42:12
   |
40 | def _load_entity_stats() -> dict:
41 |     """Load entity statistics from JSON file."""
42 |     global _entity_stats
   |            ^^^^^^^^^^^^^
43 |
44 |     if _entity_stats is None:
   |

PLW0603 Using the global statement to update `_entity_stats` is discouraged
  --> server/routes/stats.py:42:12
   |
40 | def _load_entity_stats() -> dict:
41 |     """Load entity statistics from JSON file."""
42 |     global _entity_stats
   |            ^^^^^^^^^^^^^
43 |
44 |     if _entity_stats is None:
   |

PLW0603 Using the global statement to update `_entity_stats` is discouraged
  --> server/routes/stats.py:42:12
   |
40 | def _load_entity_stats() -> dict:
41 |     """Load entity statistics from JSON file."""
42 |     global _entity_stats
   |            ^^^^^^^^^^^^^
43 |
44 |     if _entity_stats is None:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/stats.py:48:22
   |
46 |         if stats_path.exists():
47 |             try:
48 |                 with open(stats_path) as f:
   |                      ^^^^
49 |                     data = json.load(f)
50 |                     _entity_stats = data.get("statistics", {})
   |
help: Replace with `Path.open()`

PLW0603 Using the global statement to update `_network_data` is discouraged
  --> server/routes/stats.py:62:12
   |
60 | def _load_network_data() -> dict:
61 |     """Load network graph data from JSON file."""
62 |     global _network_data
   |            ^^^^^^^^^^^^^
63 |
64 |     if _network_data is None:
   |

PLW0603 Using the global statement to update `_network_data` is discouraged
  --> server/routes/stats.py:62:12
   |
60 | def _load_network_data() -> dict:
61 |     """Load network graph data from JSON file."""
62 |     global _network_data
   |            ^^^^^^^^^^^^^
63 |
64 |     if _network_data is None:
   |

PLW0603 Using the global statement to update `_network_data` is discouraged
  --> server/routes/stats.py:62:12
   |
60 | def _load_network_data() -> dict:
61 |     """Load network graph data from JSON file."""
62 |     global _network_data
   |            ^^^^^^^^^^^^^
63 |
64 |     if _network_data is None:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/stats.py:68:22
   |
66 |         if network_path.exists():
67 |             try:
68 |                 with open(network_path) as f:
   |                      ^^^^
69 |                     _network_data = json.load(f)
70 |             except Exception as e:
   |
help: Replace with `Path.open()`

PLW0603 Using the global statement to update `_timeline_data` is discouraged
  --> server/routes/stats.py:81:12
   |
79 | def _load_timeline_data() -> dict:
80 |     """Load timeline data from JSON file."""
81 |     global _timeline_data
   |            ^^^^^^^^^^^^^^
82 |
83 |     if _timeline_data is None:
   |

PLW0603 Using the global statement to update `_timeline_data` is discouraged
  --> server/routes/stats.py:81:12
   |
79 | def _load_timeline_data() -> dict:
80 |     """Load timeline data from JSON file."""
81 |     global _timeline_data
   |            ^^^^^^^^^^^^^^
82 |
83 |     if _timeline_data is None:
   |

PLW0603 Using the global statement to update `_timeline_data` is discouraged
  --> server/routes/stats.py:81:12
   |
79 | def _load_timeline_data() -> dict:
80 |     """Load timeline data from JSON file."""
81 |     global _timeline_data
   |            ^^^^^^^^^^^^^^
82 |
83 |     if _timeline_data is None:
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/routes/stats.py:87:22
   |
85 |         if timeline_path.exists():
86 |             try:
87 |                 with open(timeline_path) as f:
   |                      ^^^^
88 |                     _timeline_data = json.load(f)
89 |             except Exception as e:
   |
help: Replace with `Path.open()`

PLW0603 Using the global statement to update `_classifications` is discouraged
   --> server/routes/stats.py:100:12
    |
 98 | def _load_classifications() -> dict:
 99 |     """Load document classifications from JSON file."""
100 |     global _classifications
    |            ^^^^^^^^^^^^^^^^
101 |
102 |     if _classifications is None:
    |

PLW0603 Using the global statement to update `_classifications` is discouraged
   --> server/routes/stats.py:100:12
    |
 98 | def _load_classifications() -> dict:
 99 |     """Load document classifications from JSON file."""
100 |     global _classifications
    |            ^^^^^^^^^^^^^^^^
101 |
102 |     if _classifications is None:
    |

PLW0603 Using the global statement to update `_classifications` is discouraged
   --> server/routes/stats.py:100:12
    |
 98 | def _load_classifications() -> dict:
 99 |     """Load document classifications from JSON file."""
100 |     global _classifications
    |            ^^^^^^^^^^^^^^^^
101 |
102 |     if _classifications is None:
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/stats.py:106:22
    |
104 |         if class_path.exists():
105 |             try:
106 |                 with open(class_path) as f:
    |                      ^^^^
107 |                     data = json.load(f)
108 |                     _classifications = data.get("results", {})
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/stats.py:126:18
    |
124 |         unified_index_path = METADATA_DIR / "all_documents_index.json"
125 |         if unified_index_path.exists():
126 |             with open(unified_index_path) as f:
    |                  ^^^^
127 |                 unified_data = json.load(f)
128 |                 return {
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/stats.py:139:18
    |
137 |         master_index_path = METADATA_DIR / "master_document_index.json"
138 |         if master_index_path.exists():
139 |             with open(master_index_path) as f:
    |                  ^^^^
140 |                 index_data = json.load(f)
141 |                 return {
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/stats.py:214:14
    |
212 |             return None
213 |
214 |         with open(flight_data_path) as f:
    |              ^^^^
215 |             flight_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/routes/stats.py:256:14
    |
254 |             return None
255 |
256 |         with open(news_index_path) as f:
    |              ^^^^
257 |             news_data = json.load(f)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> server/routes/stats.py:312:9
    |
310 |     try:
311 |         # Try to import ChromaDB and get collection stats
312 |         from chromadb import PersistentClient
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
313 |         from chromadb.config import Settings
    |

PLC0415 `import` should be at the top-level of a file
   --> server/routes/stats.py:313:9
    |
311 |         # Try to import ChromaDB and get collection stats
312 |         from chromadb import PersistentClient
313 |         from chromadb.config import Settings
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
314 |
315 |         vector_store_dir = PROJECT_ROOT / "data/vector_store/chroma"
    |

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/stats.py:411:22
    |
409 |     return {
410 |         "status": status,
411 |         "timestamp": datetime.utcnow().isoformat() + "Z",
    |                      ^^^^^^^^^^^^^^^^^
412 |         "data": data,
413 |         "errors": errors if errors else None,
    |
help: Use `datetime.datetime.now(tz=...)` instead

ARG001 Unused function argument: `detailed`
   --> server/routes/stats.py:420:5
    |
418 | async def get_unified_stats(
419 |     use_cache: bool = Query(True, description="Use cached data"),
420 |     detailed: bool = Query(False, description="Include detailed breakdowns (future)"),
    |     ^^^^^^^^
421 |     sections: Optional[str] = Query(None, description="Comma-separated sections to include"),
422 | ):
    |

PLW0603 Using the global statement to update `_stats_cache` is discouraged
   --> server/routes/stats.py:474:12
    |
472 |         GET /api/v2/stats?sections=documents,news,timeline
473 |     """
474 |     global _stats_cache, _cache_timestamp
    |            ^^^^^^^^^^^^
475 |
476 |     try:
    |

PLW0603 Using the global statement to update `_cache_timestamp` is discouraged
   --> server/routes/stats.py:474:26
    |
472 |         GET /api/v2/stats?sections=documents,news,timeline
473 |     """
474 |     global _stats_cache, _cache_timestamp
    |                          ^^^^^^^^^^^^^^^^
475 |
476 |     try:
    |

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/stats.py:479:19
    |
477 |         # Check cache
478 |         if use_cache and _stats_cache and _cache_timestamp:
479 |             age = datetime.utcnow() - _cache_timestamp
    |                   ^^^^^^^^^^^^^^^^^
480 |             if age < timedelta(seconds=CACHE_TTL_SECONDS):
481 |                 result = _stats_cache.copy()
    |
help: Use `datetime.datetime.now(tz=...)` instead

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/stats.py:499:28
    |
497 |         # Update cache
498 |         _stats_cache = result.copy()
499 |         _cache_timestamp = datetime.utcnow()
    |                            ^^^^^^^^^^^^^^^^^
500 |
501 |         # Add cache metadata
    |
help: Use `datetime.datetime.now(tz=...)` instead

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
   --> server/routes/stats.py:522:9
    |
520 |     except Exception as e:
521 |         logger.error(f"Error fetching unified stats: {e}")
522 |         raise HTTPException(status_code=500, detail=f"Error fetching statistics: {e!s}")
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

PLW0603 Using the global statement to update `_stats_cache` is discouraged
   --> server/routes/stats.py:537:12
    |
535 |         POST /api/v2/stats/cache/clear
536 |     """
537 |     global _stats_cache, _cache_timestamp
    |            ^^^^^^^^^^^^
538 |
539 |     _stats_cache = None
    |

PLW0603 Using the global statement to update `_cache_timestamp` is discouraged
   --> server/routes/stats.py:537:26
    |
535 |         POST /api/v2/stats/cache/clear
536 |     """
537 |     global _stats_cache, _cache_timestamp
    |                          ^^^^^^^^^^^^^^^^
538 |
539 |     _stats_cache = None
    |

DTZ003 `datetime.datetime.utcnow()` used
   --> server/routes/stats.py:545:29
    |
543 |         "status": "success",
544 |         "message": "Statistics cache cleared",
545 |         "cache_cleared_at": datetime.utcnow().isoformat() + "Z",
    |                             ^^^^^^^^^^^^^^^^^
546 |     }
    |
help: Use `datetime.datetime.now(tz=...)` instead

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/scripts/enrichment/automated_entity_enrichment.py:60:22
   |
58 |       """
59 |
60 |       TIER_1_DOMAINS = [
   |  ______________________^
61 | |         "courtlistener.com",
62 | |         "pacer.gov",
63 | |         "supremecourt.gov",
64 | |         "justice.gov",
65 | |         "uscourts.gov",
66 | |     ]
   | |_____^
67 |
68 |       TIER_2_DOMAINS = [
   |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/scripts/enrichment/automated_entity_enrichment.py:68:22
   |
66 |       ]
67 |
68 |       TIER_2_DOMAINS = [
   |  ______________________^
69 | |         "nytimes.com",
70 | |         "washingtonpost.com",
71 | |         "theguardian.com",
72 | |         "reuters.com",
73 | |         "apnews.com",
74 | |         "bbc.com",
75 | |         "bbc.co.uk",
76 | |         "npr.org",
77 | |         "wsj.com",
78 | |         "ft.com",
79 | |         "propublica.org",
80 | |         "miamiherald.com",
81 | |     ]
   | |_____^
82 |
83 |       TIER_3_DOMAINS = ["wikipedia.org", "britannica.com", "documentcloud.org", "archive.org"]
   |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/scripts/enrichment/automated_entity_enrichment.py:83:22
   |
81 |     ]
82 |
83 |     TIER_3_DOMAINS = ["wikipedia.org", "britannica.com", "documentcloud.org", "archive.org"]
   |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
84 |
85 |     TIER_4_DOMAINS = ["forbes.com", "bloomberg.com", "cnn.com", "vanityfair.com"]
   |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/scripts/enrichment/automated_entity_enrichment.py:85:22
   |
83 |     TIER_3_DOMAINS = ["wikipedia.org", "britannica.com", "documentcloud.org", "archive.org"]
84 |
85 |     TIER_4_DOMAINS = ["forbes.com", "bloomberg.com", "cnn.com", "vanityfair.com"]
   |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
86 |
87 |     @classmethod
   |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/scripts/enrichment/automated_entity_enrichment.py:157:18
    |
155 |         """Load existing enriched entity data"""
156 |         if self.output_path.exists():
157 |             with open(self.output_path) as f:
    |                  ^^^^
158 |                 return json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/scripts/enrichment/automated_entity_enrichment.py:180:14
    |
178 |         )
179 |
180 |         with open(self.output_path, "w") as f:
    |              ^^^^
181 |             json.dump(self.existing_data, f, indent=2, default=str)
    |
help: Replace with `Path.open()`

ARG002 Unused method argument: `query`
   --> server/scripts/enrichment/automated_entity_enrichment.py:323:33
    |
321 |         ]
322 |
323 |     async def _web_search(self, query: str) -> list[dict]:
    |                                 ^^^^^
324 |         """
325 |         Placeholder for WebSearch MCP integration
    |

ARG002 Unused method argument: `entity_name`
   --> server/scripts/enrichment/automated_entity_enrichment.py:336:44
    |
334 |         return []
335 |
336 |     def _extract_facts(self, snippet: str, entity_name: str) -> dict[str, list[str]]:
    |                                            ^^^^^^^^^^^
337 |         """Extract biographical and Epstein-related facts from snippet"""
338 |         biographical = []
    |

PLC0415 `import` should be at the top-level of a file
   --> server/scripts/enrichment/automated_entity_enrichment.py:575:5
    |
573 | async def main():
574 |     """Main execution function"""
575 |     import argparse
    |     ^^^^^^^^^^^^^^^
576 |
577 |     parser = argparse.ArgumentParser(description="Automated entity enrichment with content agent")
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/scripts/enrichment/automated_entity_enrichment.py:597:10
    |
595 |         "/Users/masa/Projects/epstein/data/metadata/priority_entities_for_research.json"
596 |     )
597 |     with open(priority_path) as f:
    |          ^^^^
598 |         priority_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/scripts/enrichment/qa_validation.py:56:14
   |
54 |     def _load_data(self) -> dict:
55 |         """Load enriched entity data"""
56 |         with open(self.data_path) as f:
   |              ^^^^
57 |             return json.load(f)
   |
help: Replace with `Path.open()`

F821 Undefined name `datetime`
   --> server/scripts/enrichment/qa_validation.py:375:32
    |
373 |         """Export issues to JSON for review"""
374 |         issues_data = {
375 |             "validation_date": datetime.now().isoformat(),
    |                                ^^^^^^^^
376 |             "total_issues": len(self.issues),
377 |             "by_severity": {
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/scripts/enrichment/qa_validation.py:396:14
    |
394 |         }
395 |
396 |         with open(output_path, "w") as f:
    |              ^^^^
397 |             json.dump(issues_data, f, indent=2)
    |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> server/scripts/enrichment/qa_validation.py:404:5
    |
402 | def main():
403 |     """Run QA validation"""
404 |     import argparse
    |     ^^^^^^^^^^^^^^^
405 |
406 |     parser = argparse.ArgumentParser(description="QA validation for enriched entity data")
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/scripts/enrichment/websearch_integration.py:64:18
   |
63 |         try:
64 |             with open(cache_path) as f:
   |                  ^^^^
65 |                 data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/scripts/enrichment/websearch_integration.py:88:14
   |
86 |         }
87 |
88 |         with open(cache_path, "w") as f:
   |              ^^^^
89 |             json.dump(data, f, indent=2)
   |
help: Replace with `Path.open()`

PLR0915 Too many statements (65 > 60)
   --> server/services/audit_logger.py:248:9
    |
247 |     @staticmethod
248 |     def parse_user_agent(user_agent: str) -> dict[str, str]:
    |         ^^^^^^^^^^^^^^^^
249 |         """Parse User-Agent string to extract browser and OS information.
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/document_service.py:46:18
   |
44 |         doc_index_path = self.metadata_dir / "all_documents_index.json"
45 |         if doc_index_path.exists():
46 |             with open(doc_index_path) as f:
   |                  ^^^^
47 |                 doc_data = json.load(f)
48 |                 self.documents = doc_data.get("documents", [])
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/document_service.py:53:18
   |
51 |         class_path = self.metadata_dir / "document_classifications.json"
52 |         if class_path.exists():
53 |             with open(class_path) as f:
   |                  ^^^^
54 |                 data = json.load(f)
55 |                 self.classifications = data.get("results", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/document_service.py:60:18
   |
58 |         semantic_path = self.metadata_dir / "semantic_index.json"
59 |         if semantic_path.exists():
60 |             with open(semantic_path) as f:
   |                  ^^^^
61 |                 data = json.load(f)
62 |                 self.semantic_index = data.get("entity_to_documents", {})
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/document_service.py:192:26
    |
190 |             if md_path.exists() and md_path.suffix == ".md":
191 |                 try:
192 |                     with open(md_path) as f:
    |                          ^^^^
193 |                         content = f.read()
194 |                 except Exception:
    |
help: Replace with `Path.open()`

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/services/entity_disambiguation.py:29:38
   |
28 |     # Entity aliases loaded from JSON file
29 |     ENTITY_ALIASES: dict[str, str] = {}
   |                                      ^^
30 |
31 |     # Reverse mapping: canonical -> all known variations
   |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/services/entity_disambiguation.py:32:52
   |
31 |     # Reverse mapping: canonical -> all known variations
32 |     CANONICAL_TO_VARIATIONS: dict[str, set[str]] = {}
   |                                                    ^^
33 |
34 |     def __init__(self, mappings_path: Optional[Path] = None):
   |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/entity_disambiguation.py:73:18
   |
72 |         try:
73 |             with open(mappings_path) as f:
   |                  ^^^^
74 |                 data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_disambiguation.py:112:18
    |
111 |         try:
112 |             with open(entities_index_path) as f:
    |                  ^^^^
113 |                 data = json.load(f)
114 |                 entities = data.get("entities", [])
    |
help: Replace with `Path.open()`

ARG002 Unused method argument: `node_mapping`
   --> server/services/entity_disambiguation.py:292:52
    |
290 |         return list(canonical_nodes.values())
291 |
292 |     def deduplicate_edges(self, edges: list[dict], node_mapping: dict[str, str]) -> list[dict]:
    |                                                    ^^^^^^^^^^^^
293 |         """Deduplicate network edges after node merging
    |

PLW0603 Using the global statement to update `_disambiguator_instance` is discouraged
   --> server/services/entity_disambiguation.py:361:12
    |
359 | def get_disambiguator() -> EntityDisambiguation:
360 |     """Get global EntityDisambiguation instance (lazy singleton)"""
361 |     global _disambiguator_instance
    |            ^^^^^^^^^^^^^^^^^^^^^^^
362 |     if _disambiguator_instance is None:
363 |         _disambiguator_instance = EntityDisambiguation()
    |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/services/entity_enrichment.py:65:25
   |
64 |     class Config:
65 |         json_encoders = {datetime: lambda dt: dt.isoformat(), HttpUrl: str}
   |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |

ARG002 Unused method argument: `v`
  --> server/services/entity_enrichment.py:91:29
   |
90 |     @validator("total_sources", always=True)
91 |     def count_sources(self, v, values):
   |                             ^
92 |         return len(values.get("sources", []))
   |

ARG002 Unused method argument: `v`
  --> server/services/entity_enrichment.py:95:35
   |
94 |     @validator("average_confidence", always=True)
95 |     def calc_avg_confidence(self, v, values):
   |                                   ^
96 |         sources = values.get("sources", [])
97 |         if not sources:
   |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
   --> server/services/entity_enrichment.py:102:25
    |
101 |     class Config:
102 |         json_encoders = {datetime: lambda dt: dt.isoformat()}
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
   --> server/services/entity_enrichment.py:127:21
    |
126 |       # Domain patterns with confidence scores
127 |       DOMAIN_SCORES = {
    |  _____________________^
128 | |         # Court records and official documents (highest trust)
129 | |         r"courtlistener\.com": 1.0,
130 | |         r"supremecourt\.gov": 1.0,
131 | |         r"pacer\.gov": 1.0,
132 | |         r"documentcloud\.org": 0.95,
133 | |         # Wikipedia and academic sources
134 | |         r"wikipedia\.org": 0.9,
135 | |         r"britannica\.com": 0.9,
136 | |         r"scholar\.google\.com": 0.9,
137 | |         r"jstor\.org": 0.9,
138 | |         r"\.edu($|/)": 0.85,
139 | |         # Major news outlets (high trust)
140 | |         r"nytimes\.com": 0.85,
141 | |         r"washingtonpost\.com": 0.85,
142 | |         r"theguardian\.com": 0.85,
143 | |         r"reuters\.com": 0.85,
144 | |         r"apnews\.com": 0.85,
145 | |         r"bbc\.(com|co\.uk)": 0.85,
146 | |         r"npr\.org": 0.8,
147 | |         r"wsj\.com": 0.8,
148 | |         r"ft\.com": 0.8,
149 | |         # Mid-tier news outlets
150 | |         r"cnn\.com": 0.7,
151 | |         r"forbes\.com": 0.7,
152 | |         r"bloomberg\.com": 0.75,
153 | |         r"vanityfair\.com": 0.65,
154 | |         r"newyorker\.com": 0.75,
155 | |         # Archive and research sites
156 | |         r"archive\.org": 0.8,
157 | |         r"archive\.is": 0.7,
158 | |         # Social media (low trust for facts)
159 | |         r"twitter\.com": 0.3,
160 | |         r"x\.com": 0.3,
161 | |         r"facebook\.com": 0.3,
162 | |         r"reddit\.com": 0.35,
163 | |         # Blogs and unknown sources (lowest)
164 | |         r"blogspot\.com": 0.2,
165 | |         r"wordpress\.com": 0.2,
166 | |         r"medium\.com": 0.4,
167 | |     }
    | |_____^
168 |
169 |       DEFAULT_SCORE = 0.5  # Unknown sources get medium confidence
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/entity_enrichment.py:471:21
    |
469 |                 if url.startswith("//duckduckgo.com/l/"):
470 |                     # Extract actual URL from redirect
471 |                     import urllib.parse
    |                     ^^^^^^^^^^^^^^^^^^^
472 |
473 |                     parsed = urllib.parse.parse_qs(url.split("?")[1] if "?" in url else "")
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/entity_enrichment.py:506:13
    |
504 |         except Exception as e:
505 |             print(f"Error parsing search results for '{query}': {e}")
506 |             import traceback
    |             ^^^^^^^^^^^^^^^^
507 |
508 |             traceback.print_exc()
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_enrichment.py:572:18
    |
571 |         try:
572 |             with open(self.storage_path) as f:
    |                  ^^^^
573 |                 data = json.load(f)
574 |                 return {
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_enrichment.py:585:18
    |
583 |         """Save enrichments to disk"""
584 |         try:
585 |             with open(self.storage_path, "w") as f:
    |                  ^^^^
586 |                 json.dump(
587 |                     {
    |
help: Replace with `Path.open()`

DTZ003 `datetime.datetime.utcnow()` used
   --> server/services/entity_enrichment.py:600:15
    |
598 |     def _is_cache_valid(self, enrichment: EntityEnrichment) -> bool:
599 |         """Check if cached enrichment is still valid (within TTL)"""
600 |         age = datetime.utcnow() - enrichment.last_updated
    |               ^^^^^^^^^^^^^^^^^
601 |         return age < timedelta(days=self.CACHE_TTL_DAYS)
    |
help: Use `datetime.datetime.now(tz=...)` instead

ARG002 Unused method argument: `entity_name`
   --> server/services/entity_enrichment.py:603:52
    |
601 |         return age < timedelta(days=self.CACHE_TTL_DAYS)
602 |
603 |     async def get_enrichment(self, entity_id: str, entity_name: str) -> Optional[EntityEnrichment]:
    |                                                    ^^^^^^^^^^^
604 |         """Get cached enrichment if valid, otherwise return None"""
605 |         if entity_id in self.cache:
    |

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_service.py:104:18
    |
102 |         stats_path = self.metadata_dir / "entity_statistics.json"
103 |         if stats_path.exists():
104 |             with open(stats_path) as f:
    |                  ^^^^
105 |                 data = json.load(f)
106 |                 self.entity_stats = data.get("statistics", {})
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_service.py:118:18
    |
116 |         bio_path = self.metadata_dir / "entity_biographies.json"
117 |         if bio_path.exists():
118 |             with open(bio_path) as f:
    |                  ^^^^
119 |                 data = json.load(f)
120 |                 self.entity_bios = data.get("entities", {})
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_service.py:129:18
    |
127 |         tags_path = self.metadata_dir / "entity_tags.json"
128 |         if tags_path.exists():
129 |             with open(tags_path) as f:
    |                  ^^^^
130 |                 data = json.load(f)
131 |                 self.entity_tags = data.get("entities", {})
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_service.py:140:18
    |
138 |         network_path = self.metadata_dir / "entity_network.json"
139 |         if network_path.exists():
140 |             with open(network_path) as f:
    |                  ^^^^
141 |                 self.network_data = json.load(f)
    |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
   --> server/services/entity_service.py:150:18
    |
148 |         semantic_path = self.metadata_dir / "semantic_index.json"
149 |         if semantic_path.exists():
150 |             with open(semantic_path) as f:
    |                  ^^^^
151 |                 data = json.load(f)
152 |                 self.semantic_index = data.get("entity_to_documents", {})
    |
help: Replace with `Path.open()`

RUF012 Mutable class attributes should be annotated with `typing.ClassVar`
  --> server/services/file_watcher.py:31:17
   |
30 |       # Map filenames to event types
31 |       EVENT_MAP = {
   |  _________________^
32 | |         "entity_network.json": "entity_network_updated",
33 | |         "timeline_events.json": "timeline_updated",
34 | |         "master_document_index.json": "entities_updated",
35 | |         "unified_document_index.json": "documents_updated",
36 | |         "cases_index.json": "cases_updated",
37 | |         "victims_index.json": "victims_updated",
38 | |         "entity_name_mappings.json": "entity_mappings_updated",
39 | |         "entity_filter_list.json": "entity_filter_updated",
40 | |     }
   | |_____^
41 |
42 |       def __init__(self, enable_hot_reload: bool = True):
   |

PTH119 `os.path.basename()` should be replaced by `Path.name`
  --> server/services/file_watcher.py:72:20
   |
70 |             return
71 |
72 |         filename = os.path.basename(event.src_path)
   |                    ^^^^^^^^^^^^^^^^
73 |
74 |         # Check if this file is in our watch list
   |
help: Replace with `Path(...).name`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/flight_service.py:45:18
   |
43 |         flight_data_path = self.md_dir / "entities/flight_logs_by_flight.json"
44 |         if flight_data_path.exists():
45 |             with open(flight_data_path) as f:
   |                  ^^^^
46 |                 self.flight_data = json.load(f)
   |
help: Replace with `Path.open()`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/flight_service.py:51:18
   |
49 |         locations_path = self.metadata_dir / "flight_locations.json"
50 |         if locations_path.exists():
51 |             with open(locations_path) as f:
   |                  ^^^^
52 |                 self.locations_db = json.load(f)
   |
help: Replace with `Path.open()`

B007 Loop control variable `route_key` not used within loop body
   --> server/services/flight_service.py:228:13
    |
226 |         # Convert route_map to array with frequency
227 |         routes = []
228 |         for route_key, route_data in route_map.items():
    |             ^^^^^^^^^
229 |             routes.append(
230 |                 {
    |
help: Rename unused `route_key` to `_route_key`

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/network_service.py:57:18
   |
55 |         network_path = self.metadata_dir / "entity_network.json"
56 |         if network_path.exists():
57 |             with open(network_path) as f:
   |                  ^^^^
58 |                 self.network_data = json.load(f)
   |
help: Replace with `Path.open()`

PLC0415 `import` should be at the top-level of a file
   --> server/services/network_service.py:196:9
    |
195 |         # BFS to find shortest path
196 |         from collections import deque
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
197 |
198 |         queue = deque([(node_a["id"], [node_a["id"]])])
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/network_service.py:269:9
    |
268 |         # BFS to find nodes within max_hops
269 |         from collections import deque
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
270 |
271 |         visited = {entity_node["id"]: 0}  # node_id -> hop distance
    |

PTH123 `open()` should be replaced by `Path.open()`
  --> server/services/news_search_service.py:62:22
   |
60 |         if self._news_index_cache is None:
61 |             if NEWS_INDEX_PATH.exists():
62 |                 with open(NEWS_INDEX_PATH, encoding="utf-8") as f:
   |                      ^^^^
63 |                     self._news_index_cache = json.load(f)
64 |             else:
   |
help: Replace with `Path.open()`

E402 Module level import not at top of file
  --> server/services/news_service.py:32:1
   |
30 |   sys.path.insert(0, str(SERVER_DIR))
31 |
32 | / from models.news_article import (
33 | |     ArchiveStatus,
34 | |     NewsArticle,
35 | |     NewsArticleCreate,
36 | |     NewsArticleMetadata,
37 | |     NewsArticlesIndex,
38 | | )
   | |_^
   |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> server/services/news_service.py:83:13
   |
81 |                 return self._index
82 |         except Exception as e:
83 |             raise RuntimeError(f"Failed to load news index: {e}")
   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
84 |
85 |     def save_news_index(self, index: NewsArticlesIndex) -> None:
   |

PLC0415 `import` should be at the top-level of a file
   --> server/services/news_service.py:486:13
    |
484 |         try:
485 |             # Import batch_embed_helper here to avoid circular dependency
486 |             import sys
    |             ^^^^^^^^^^
487 |             from pathlib import Path
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/news_service.py:487:13
    |
485 |             # Import batch_embed_helper here to avoid circular dependency
486 |             import sys
487 |             from pathlib import Path
    |             ^^^^^^^^^^^^^^^^^^^^^^^^
488 |
489 |             scripts_dir = Path(__file__).parent.parent.parent / "scripts" / "rag"
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/news_service.py:492:13
    |
490 |             sys.path.insert(0, str(scripts_dir))
491 |
492 |             from batch_embed_helper import batch_embed_articles
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
493 |
494 |             # Embed single article
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/news_service.py:533:13
    |
531 |         try:
532 |             # Import batch_embed_helper
533 |             import sys
    |             ^^^^^^^^^^
534 |             from pathlib import Path
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/news_service.py:534:13
    |
532 |             # Import batch_embed_helper
533 |             import sys
534 |             from pathlib import Path
    |             ^^^^^^^^^^^^^^^^^^^^^^^^
535 |
536 |             scripts_dir = Path(__file__).parent.parent.parent / "scripts" / "rag"
    |

PLC0415 `import` should be at the top-level of a file
   --> server/services/news_service.py:539:13
    |
537 |             sys.path.insert(0, str(scripts_dir))
538 |
539 |             from batch_embed_helper import batch_embed_articles
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
540 |
541 |             # Load all articles
    |

PLC0415 `import` should be at the top-level of a file
   --> server/tests/test_phase2_models.py:110:9
    |
108 |     def test_document_reference_from_document(self):
109 |         """Test creating DocumentReference from Document."""
110 |         from models.document import DocumentReference
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
111 |
112 |         doc = Document(
    |

PLC0415 `import` should be at the top-level of a file
   --> server/tests/test_phase2_models.py:159:9
    |
157 |     def test_document_metadata_optional(self):
158 |         """Test document metadata is optional."""
159 |         from models.document import DocumentMetadata
    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
160 |
161 |         doc = Document(id="doc_1", metadata=DocumentMetadata(file_size=1024, pages=10))
    |

Found 753 errors.

Black Formatting Issues:
All done! ✨ 🍰 ✨
156 files would be left unchanged.

isort Import Sorting Issues:
ERROR: /Users/masa/Projects/epstein/scripts/expand_news_database.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/reorganize_data.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/research/basic_entity_whois.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/research/add_researched_entities.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/research/enrich_entity_data.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/classification/classify_all_documents.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/classification/classify_emails.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/database/init_audit_db.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/content_extractor.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/scrape_news_articles.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/link_verifier.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/ingest_news_batch.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/__init__.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/populate_news_database.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/entity_extractor.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/import_manual_news.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/ingest_seed_articles.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/ingestion/credibility_scorer.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/canonicalization/canonicalize_emails.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/canonicalization/canonicalize.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/canonicalization/initialize_deduplication.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/canonicalization/query_deduplication.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/canonicalization/process_bulk_emails.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/verify_entity_filtering.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/rebuild_document_stats.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/analyze_giuffre_maxwell_pdfs.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/final_entity_cleanup_complete.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/web_relationship_finder.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/fix_entity_data_quality.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/entity_statistics.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/rebuild_flight_network.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/timeline_builder.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/build_unified_index.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/enrich_entity_relationships.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/mistral_entity_disambiguator.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/build_knowledge_graph.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/entity_disambiguator.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/entity_network.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/analysis/batch_entity_disambiguation.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/generate_entity_mappings.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/rebuild_all_documents_index.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/merge_epstein_duplicates.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/fix_flight_counts.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/normalize_entity_names.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/fix_biography_names_v3.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/normalize_raw_flight_logs.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/categorize_documents.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/validate_categorization.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/rebuild_entity_statistics.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/validate_entity_sync.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/merge_categorizations.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/data_quality/restore_entity_bios.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/utils/verify_normalization.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/rag/kg_rag_integration.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/rag/batch_embed_helper.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/rag/embed_news_articles.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/rag/query_rag.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/rag/link_entities_to_docs.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/rag/build_vector_store.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/search/entity_search.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/verification/verify_task_completion.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/download/download_all_sources.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/download/check_courtlistener_progress.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/download/download_courtlistener.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/download/download_case_files.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/indexing/generate_summary_report.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/indexing/build_unified_index.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/import/import_huggingface_emails.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/import/import_huggingface_documents.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/extraction/check_ocr_status.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/extraction/extract_emails.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/extraction/ocr_house_oversight.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/metadata/refresh_chatbot_index.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/scripts/validation/run_pydantic_audit.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/demo_fixes.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/api_routes.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/app.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/tests/test_phase2_models.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/models/__init__.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/scripts/enrichment/automated_entity_enrichment.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/routes/chat_enhanced.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/routes/flights.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/routes/rag.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/routes/stats.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/routes/search.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/routes/news.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/news_service.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/__init__.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/entity_service.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/suggestion_service.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/entity_enrichment.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/file_watcher.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/network_service.py Imports are incorrectly sorted and/or formatted.
ERROR: /Users/masa/Projects/epstein/server/services/news_search_service.py Imports are incorrectly sorted and/or formatted.
Skipped 4 files

mypy Type Checking:
scripts/DEPRECATED/research/test_whois.py:9: error: Library stubs not installed for "requests"  [import-untyped]
scripts/DEPRECATED/research/test_whois.py:9: note: Hint: "python3 -m pip install types-requests"
scripts/DEPRECATED/research/test_whois.py:9: note: (or run "mypy --install-types" to install all missing stub packages)
scripts/DEPRECATED/research/test_whois.py:9: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports
scripts/canonicalization/canonicalize.py:373: error: Library stubs not installed for "yaml"  [import-untyped]
scripts/canonicalization/canonicalize.py:373: note: Hint: "python3 -m pip install types-PyYAML"
scripts/classification/document_classifier.py: error: Source file found twice under different module names: "document_classifier" and "classification.document_classifier"
scripts/classification/document_classifier.py: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules for more info
scripts/classification/document_classifier.py: note: Common resolutions include: a) adding `__init__.py` somewhere, b) using `--explicit-package-bases` or adjusting MYPYPATH
Found 3 errors in 3 files (errors prevented further checking)

